
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Elementos de procesamiento de lenguajes naturales, o El Hacedor &#8212; Dantis Elementorum</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="_static/logo.png"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Elementos terminológicos de matemáticas computacionales" href="definiciones.html" />
    <link rel="prev" title="Elementos de redes neuronales" href="redes.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Dantis Elementorum</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Intro
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Elementorum Physicæ Mathematicæ
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="programacion.html">
   Elementos de programación
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="redes.html">
   Elementos de redes neuronales
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Elementos de procesamiento de lenguajes naturales, o El Hacedor
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="definiciones.html">
   Elementos terminológicos de matemáticas computacionales
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Elementorum Iurisprudentiæ
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="iurisprudentiae.html">
   Elementos de leyes, o Nova methodus discendæ iurisprudentiæ
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Elementorum Poeticæ
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="borges.html">
   Elementos de literatura borgesiana
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="afro.html">
   Elementos de música afroamericana
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Exagium
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="tzompantli.html">
   Breve ensayo sobre el tzompantli
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="castillos.html">
   La importancia de ser olvidado
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/DanteNoguez/Ensayos/main?urlpath=tree/ensayos/hacedor.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/DanteNoguez/Ensayos"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/DanteNoguez/Ensayos/issues/new?title=Issue%20on%20page%20%2Fhacedor.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/hacedor.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bigrama">
   Bigrama
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#red-neuronal">
   Red neuronal
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Elementos de procesamiento de lenguajes naturales, o El Hacedor</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bigrama">
   Bigrama
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#red-neuronal">
   Red neuronal
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="elementos-de-procesamiento-de-lenguajes-naturales-o-el-hacedor">
<h1>Elementos de procesamiento de lenguajes naturales, o El Hacedor<a class="headerlink" href="#elementos-de-procesamiento-de-lenguajes-naturales-o-el-hacedor" title="Permalink to this headline">#</a></h1>
<p>Este texto está basado en <a class="reference external" href="https://github.com/karpathy/makemore">makemore</a> de Andrej Karpathy. A continuación, crearemos un modelo de lenguaje basado en bigramas. Con base en 21,209 nombres argentinos, nuestro modelo aprenderá a formular nuevos nombres basándose en probabilidades asignadas a cada carácter con base en el anterior.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>El conjunto de datos original se puede encontrar <a class="reference external" href="https://www.kaggle.com/datasets/akielbowicz/nombres-de-personas-fsicas-de-argentina">aquí</a>. El código que utilicé para limpiar los datos está comentado en la primera celda; el archivo generado con dicho código se encuentra <a class="reference external" href="https://github.com/DanteNoguez/CalculusRatiocinator/blob/main/data/nombres.txt">aquí</a>.</p>
</aside>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="c1">#nombres = pd.read_csv(&#39;historico-nombres.csv&#39;).iloc[0:200000]</span>
<span class="c1">#regex = &quot;[^a-z]&quot;</span>
<span class="c1">#nombres = nombres[&#39;nombre&#39;].str.lower()</span>
<span class="c1">#filtro = nombres.str.contains(&quot;[^a-z]&quot;)</span>
<span class="c1">#nombres = nombres[~filtro].astype(&#39;str&#39;)</span>

<span class="c1">#nombres.to_csv(r&#39;nombres.txt&#39;, header=None, index=None, mode=&#39;a&#39;)</span>
<span class="c1">#nombres.head(10)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>!wget https://github.com/DanteNoguez/CalculusRatiocinator/raw/main/data/nombres.txt
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--2022-10-18 03:00:02--  https://github.com/DanteNoguez/CalculusRatiocinator/raw/main/data/nombres.txt
Resolving github.com (github.com)... 140.82.121.3
Connecting to github.com (github.com)|140.82.121.3|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://raw.githubusercontent.com/DanteNoguez/CalculusRatiocinator/main/data/nombres.txt [following]
--2022-10-18 03:00:02--  https://raw.githubusercontent.com/DanteNoguez/CalculusRatiocinator/main/data/nombres.txt
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 165469 (162K) [text/plain]
Saving to: ‘nombres.txt’

nombres.txt         100%[===================&gt;] 161.59K  --.-KB/s    in 0.01s   

2022-10-18 03:00:03 (14.6 MB/s) - ‘nombres.txt’ saved [165469/165469]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">palabras</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;nombres.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">palabras</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;maria&#39;,
 &#39;rosa&#39;,
 &#39;jose&#39;,
 &#39;carmen&#39;,
 &#39;ana&#39;,
 &#39;juana&#39;,
 &#39;antonio&#39;,
 &#39;elena&#39;,
 &#39;teresa&#39;,
 &#39;angela&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">palabras</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>21029
</pre></div>
</div>
</div>
</div>
<section id="bigrama">
<h2>Bigrama<a class="headerlink" href="#bigrama" title="Permalink to this headline">#</a></h2>
<p>Primero, formaremos bigramas (pares) de caracteres por cada nombre que hay en nuestro conjunto de datos. Al final e inicio de cada nombre, agregaremos un <code class="docutils literal notranslate"><span class="pre">.</span></code> para indicar el inicio y fin de dicho nombre:</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Cada <em>token</em> es una unidad indivisible de texto, aunque el diseño o especificación de <em>tokens</em> en un modelo es una decisión personal (técnica, para ser más precisos). Por ejemplo, podemos crear un conjunto de datos enfocado en palabras, de manera que «mesa» sea un <em>token</em>; pero también podemos considerar a cada letra del alfabeto como un <em>token</em>.</p>
</aside>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">palabras</span><span class="p">[:</span><span class="mi">3</span><span class="p">]:</span> <span class="c1"># vemos los primeros tres nombres</span>
  <span class="n">cs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> 
  <span class="k">for</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cs</span><span class="p">,</span> <span class="n">cs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span> <span class="c1"># iteramos sobre cada caracter para crear bigramas</span>
    <span class="n">bigrama</span> <span class="o">=</span> <span class="p">(</span><span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">)</span>
    <span class="n">b</span><span class="p">[</span><span class="n">bigrama</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">bigrama</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1"># hacemos un conteo de bigramas</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>. m
m a
a r
r i
i a
a .
. r
r o
o s
s a
a .
. j
j o
o s
s e
e .
</pre></div>
</div>
</div>
</div>
<p>El conteo de bigramas luce así:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">b</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{(&#39;.&#39;, &#39;m&#39;): 1,
 (&#39;m&#39;, &#39;a&#39;): 1,
 (&#39;a&#39;, &#39;r&#39;): 1,
 (&#39;r&#39;, &#39;i&#39;): 1,
 (&#39;i&#39;, &#39;a&#39;): 1,
 (&#39;a&#39;, &#39;.&#39;): 2,
 (&#39;.&#39;, &#39;r&#39;): 1,
 (&#39;r&#39;, &#39;o&#39;): 1,
 (&#39;o&#39;, &#39;s&#39;): 2,
 (&#39;s&#39;, &#39;a&#39;): 1,
 (&#39;.&#39;, &#39;j&#39;): 1,
 (&#39;j&#39;, &#39;o&#39;): 1,
 (&#39;s&#39;, &#39;e&#39;): 1,
 (&#39;e&#39;, &#39;.&#39;): 1}
</pre></div>
</div>
</div>
</div>
<p>Ahora, crearemos una lista de caracteres únicos (nuestro vocabulario) para luego asignarles un índice en un diccionario de Python. A este proceso de mapear o relacionar cada letra de nuestro vocabulario con un número se le denomina «incrustación» (<em>embedding</em>), mientras que el diccionario de Python resultante es una «tabla de consulta» (<em>lookup table</em>), debido a que en ella podemos buscar la letra que corresponde a un número y viceversa.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">caracs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">palabras</span><span class="p">))))</span> <span class="c1"># lista de caracteres únicos</span>

<span class="n">paf</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="p">:</span><span class="n">f</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">f</span><span class="p">,</span><span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">caracs</span><span class="p">)}</span> <span class="c1"># mapeamos letras a números de principio a fin</span>
<span class="n">paf</span><span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># agregamos nuestro token «.»</span>
<span class="n">fap</span> <span class="o">=</span> <span class="p">{</span><span class="n">f</span><span class="p">:</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span><span class="p">,</span><span class="n">f</span> <span class="ow">in</span> <span class="n">paf</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span> <span class="c1"># invertimos el orden para que sea apropiado</span>
<span class="n">fap</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{1: &#39;a&#39;,
 2: &#39;b&#39;,
 3: &#39;c&#39;,
 4: &#39;d&#39;,
 5: &#39;e&#39;,
 6: &#39;f&#39;,
 7: &#39;g&#39;,
 8: &#39;h&#39;,
 9: &#39;i&#39;,
 10: &#39;j&#39;,
 11: &#39;k&#39;,
 12: &#39;l&#39;,
 13: &#39;m&#39;,
 14: &#39;n&#39;,
 15: &#39;o&#39;,
 16: &#39;p&#39;,
 17: &#39;q&#39;,
 18: &#39;r&#39;,
 19: &#39;s&#39;,
 20: &#39;t&#39;,
 21: &#39;u&#39;,
 22: &#39;v&#39;,
 23: &#39;w&#39;,
 24: &#39;x&#39;,
 25: &#39;y&#39;,
 26: &#39;z&#39;,
 0: &#39;.&#39;}
</pre></div>
</div>
</div>
</div>
<p>Ahora, construiremos una matriz —vía PyTorch— con el conteo de todos los bigramas de nuestro conjunto de datos. Con esta matriz, podremos familiarizarnos más visualmente con lo que hemos estado preparando. Las dimensiones de la matriz serán 27x27 porque tenemos 27 elementos en nuestro vocabulario y queremos emparejarlos (hacer bigramas) con cada uno de los otros elementos del mismo:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">N</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">27</span><span class="p">,</span><span class="mi">27</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">palabras</span><span class="p">:</span>
  <span class="n">cs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cs</span><span class="p">,</span> <span class="n">cs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c2</span><span class="p">]</span>
    <span class="n">N</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span> <span class="n">ix2</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">27</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">27</span><span class="p">):</span>
    <span class="n">cts</span> <span class="o">=</span> <span class="n">fap</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">fap</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">cts</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">N</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/hacedor_16_0.png" src="_images/hacedor_16_0.png" />
</div>
</div>
<p>Hemos contado la ocurrencia de cada bigrama en el documento de nombres. Ahora, podemos utilizar este conteo como una distribución de probabilidades acerca de cuál letra debe ser consecutiva con otra. Ejemplifiquemos con una fila:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([   0., 2611.,  909., 1465., 1038., 2205.,  941.,  924.,  668.,  726.,
         522.,   88., 1230., 1228.,  913.,  584.,  774.,   41., 1014., 1248.,
         578.,  154.,  548.,  169.,    0.,  218.,  233.])
</pre></div>
</div>
</div>
</div>
<p>Obtendremos las probabilidades de cada valor al dividir cada uno por la sumatoria de los demás. Con este truco, todos los valores sumados entre sí nos darán 1:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">N</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">p</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.0000, 0.1242, 0.0432, 0.0697, 0.0494, 0.1049, 0.0447, 0.0439, 0.0318,
        0.0345, 0.0248, 0.0042, 0.0585, 0.0584, 0.0434, 0.0278, 0.0368, 0.0019,
        0.0482, 0.0593, 0.0275, 0.0073, 0.0261, 0.0080, 0.0000, 0.0104, 0.0111])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">p</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1.0000)
</pre></div>
</div>
</div>
</div>
<p>Ahora utilizaremos <code class="docutils literal notranslate"><span class="pre">torch.multinomial</span></code> para generar números enteros con base en las probabilidades de la distribución que creamos. Primero veamos un ejemplo:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="c1">#creamos tres valores aleatorios</span>
<span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="c1"># ahora, creamos una distribución de probabilidades con base en ellos</span>
<span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.0043, 0.4267, 0.4462])
tensor([0.0049, 0.4865, 0.5086])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># ahora tomamos muestras de números enteros con base en la distribución</span>
<span class="c1"># Notemos que los números generados reflejan la distribución de probabilidades anteriores</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([1, 1, 1, 1, 1, 1, 1, 2, 1, 2])
</pre></div>
</div>
</div>
</div>
<p>Podemos ejemplificar lo mismo con la primera fila de nuestra matriz:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">N</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([22,  7, 19,  8, 13])
</pre></div>
</div>
</div>
</div>
<p>Pero el resultado obtenido es el índice. Utilicemos nuestra tabla de consulta para obtener la letra correspondiente:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">ejemplo</span> <span class="o">=</span> <span class="n">fap</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
<span class="n">ejemplo</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;e&#39;
</pre></div>
</div>
</div>
</div>
<p>Ahora haremos lo mismo con todos los bigramas:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">P</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="c1"># agregamos 1 al conteo para que el logaritmo no tenga problemas eventualmente (smoothing)</span>
<span class="n">P</span> <span class="o">/=</span> <span class="n">P</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># el 1 indica que la sumatoria se hace en la dimensión 1 (i. e., las columnas colapsan para sumarse)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">P</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">P</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor(1.0000), torch.Size([27, 27]))
</pre></div>
</div>
</div>
</div>
<p>Las probabilidades de nuestra primera fila lucen así:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">P</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([4.7492e-05, 1.2405e-01, 4.3218e-02, 6.9624e-02, 4.9345e-02, 1.0477e-01,
        4.4738e-02, 4.3930e-02, 3.1772e-02, 3.4527e-02, 2.4839e-02, 4.2268e-03,
        5.8463e-02, 5.8368e-02, 4.3408e-02, 2.7783e-02, 3.6807e-02, 1.9947e-03,
        4.8205e-02, 5.9318e-02, 2.7498e-02, 7.3613e-03, 2.6073e-02, 8.0737e-03,
        4.7492e-05, 1.0401e-02, 1.1113e-02])
</pre></div>
</div>
</div>
</div>
<p>Ahora que ya tenemos una probabilidad asignada a cada bigrama, podemos comenzar a predecir el carácter que debe acompañar a su precedente con base en nuestra matriz de probabilidades. Experimentemos con cinco palabras:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
  <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">ix</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fap</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">break</span>
    
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>lio.
jutes.
mblinano.
sho.
mito.
</pre></div>
</div>
</div>
</div>
<p>Aunque quizá no elijamos ninguno de estos nombres para uso personal, podemos ver que el modelo funciona y ha generado palabras que de alguna forma reflejan la estructura del español. Incluso hemos conseguido la palabra «mito».</p>
<p>También podemos observar las probabilidades asignadas a cada bigrama:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">palabras</span><span class="p">[:</span><span class="mi">3</span><span class="p">]:</span>
  <span class="n">cs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cs</span><span class="p">,</span> <span class="n">cs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c2</span><span class="p">]</span> 
    <span class="n">prob</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span> <span class="n">ix2</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">c1</span><span class="si">}{</span><span class="n">c2</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">prob</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.m: 0.0584
ma: 0.3492
ar: 0.0863
ri: 0.2349
ia: 0.1907
a.: 0.4568
.r: 0.0482
ro: 0.1212
os: 0.0515
sa: 0.1840
a.: 0.4568
.j: 0.0248
jo: 0.2045
os: 0.0515
se: 0.1236
e.: 0.0674
</pre></div>
</div>
</div>
</div>
<p>Dado que altas probabilidades en nuestros bigramas indican buen «aprendizaje», en el sentido de que nuestro modelo no es completamente aleatorio, sino que concede importancia a bigramas apropiadamente, podemos medir la «precisión» o capacidad de nuestro modelo mediante la función de verosimilitud (<em>likelihood</em>), que es el resultado de multiplicar todas las probabilidades entre sí. Si el número es alto, eso indicaría que nuestro modelo funciona bien; si es bajo, eso indicaría que no tiene suficiente información para predecir caracteres.</p>
<p>Por conveniencia, esta estimación utiliza el logaritmo natural de las probabilidades: sumar los logaritmos de las probabilidades es equivalente a multiplicar las probabilidades (es decir, podemos emplear cualquiera de las dos formas para estimar la verosimilitud). Esto es particularmente útil porque nuestras probabilidades están dadas en números decimales, de manera que multiplicarlas entre sí nos daría un número pequeño y poco intuitivo.</p>
<p>El logaritmo natural de una serie de números presenta como valor máximo al 0, pero como valor mínimo al infinito:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log
  &quot;&quot;&quot;Entry point for launching an IPython kernel.
</pre></div>
</div>
<img alt="_images/hacedor_40_1.png" src="_images/hacedor_40_1.png" />
</div>
</div>
<p>Pero, dado que quisiéramos números positivos para hacerlo más intuitivo, podemos volver positivo este número al multiplicarlo por -1:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logprob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
<span class="n">nlog</span> <span class="o">=</span> <span class="o">-</span><span class="n">logprob</span>
<span class="sa">f</span><span class="s1">&#39;Logaritmo natural de la probabilidad: </span><span class="si">{</span><span class="n">logprob</span><span class="si">}</span><span class="s1"> | Logaritmo natural negativo: </span><span class="si">{</span><span class="n">nlog</span><span class="si">}</span><span class="s1">&#39;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Logaritmo natural de la probabilidad: -2.6970839500427246 | Logaritmo natural negativo: 2.6970839500427246&#39;
</pre></div>
</div>
</div>
</div>
<p>Y el logaritmo negativo de la verosimilitud (<em>negative log likelihood</em>) es la suma de todos los logaritmos negativos. Nuestra función de pérdida entonces podría ser el logaritmo negativo de la verosimilitud (<code class="docutils literal notranslate"><span class="pre">nll</span></code>), normalizada para obtener el promedio. Mientras esta función de pérdida sea menor, nuestro modelo será mejor:</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Matemáticamente, podemos formular esto como <span class="math notranslate nohighlight">\(-\log\left(p(X\mid\boldsymbol{\theta})\right) = -\log(p(x_1\mid\boldsymbol{\theta})) - \log(p(x_2\mid\boldsymbol{\theta})) \cdots - \log(p(x_n\mid\boldsymbol{\theta})) = -\sum_i \log(p(x_i \mid \theta)).\)</span> Para saber más, véase <a class="reference external" href="https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html">este capítulo</a>.</p>
</aside>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">log_likelihood</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">n</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">palabras</span><span class="p">:</span>
  <span class="n">cs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cs</span><span class="p">,</span> <span class="n">cs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c2</span><span class="p">]</span>
    <span class="n">prob</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span><span class="n">ix2</span><span class="p">]</span>
    <span class="n">logprob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
    <span class="n">log_likelihood</span> <span class="o">+=</span> <span class="n">logprob</span>
    <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">nll</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_likelihood</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Logaritmo negativo de verosimilitud: </span><span class="si">{</span><span class="n">nll</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Logaritmo negativo de verosimilitud promedio: </span><span class="si">{</span><span class="n">nll</span><span class="o">/</span><span class="n">n</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Logaritmo negativo de verosimilitud: 375152.34375
Logaritmo negativo de verosimilitud promedio: 2.2672061920166016
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">log_likelihood</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">n</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="k">for</span> <span class="n">bi</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;dante&#39;</span><span class="p">]:</span>
  <span class="n">cs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">bi</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cs</span><span class="p">,</span> <span class="n">cs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c2</span><span class="p">]</span>
    <span class="n">prob</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span><span class="n">ix2</span><span class="p">]</span>
    <span class="n">logprob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
    <span class="n">log_likelihood</span> <span class="o">+=</span> <span class="n">logprob</span>
    <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">c1</span><span class="si">}{</span><span class="n">c2</span><span class="si">}</span><span class="s1"> | prob: </span><span class="si">{</span><span class="n">prob</span><span class="si">:</span><span class="s1">.7f</span><span class="si">}</span><span class="s1"> | logaritmo de la verosimilitud: </span><span class="si">{</span><span class="n">log_likelihood</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">nll</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_likelihood</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;logaritmo negativo de la verosimilitud: </span><span class="si">{</span><span class="n">nll</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;promedio del logaritmo negativo: </span><span class="si">{</span><span class="n">nll</span><span class="o">/</span><span class="n">n</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.d | prob: 0.0493446 | logaritmo de la verosimilitud: -3.0089
da | prob: 0.3054336 | logaritmo de la verosimilitud: -4.1949
an | prob: 0.1229762 | logaritmo de la verosimilitud: -6.2907
nt | prob: 0.0515025 | logaritmo de la verosimilitud: -9.2568
te | prob: 0.1527994 | logaritmo de la verosimilitud: -11.1355
e. | prob: 0.0674018 | logaritmo de la verosimilitud: -13.8326
logaritmo negativo de la verosimilitud: 13.832551956176758
promedio del logaritmo negativo: 2.3054254055023193
</pre></div>
</div>
</div>
</div>
</section>
<section id="red-neuronal">
<h2>Red neuronal<a class="headerlink" href="#red-neuronal" title="Permalink to this headline">#</a></h2>
<p>Ahora que tenemos una función de pérdida, podemos adaptar nuestro modelo a una red neuronal y optimizarlo. Crearemos bigramas de la misma manera, pero ahora crearemos un vector <span class="math notranslate nohighlight">\(x\)</span> con el primer elemento del bigrama y otro <span class="math notranslate nohighlight">\(y\)</span> con el segundo. Las <span class="math notranslate nohighlight">\(x\)</span> serán entonces nuestras entradas y las <span class="math notranslate nohighlight">\(y\)</span> nuestros objetivos:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Juntaremos los bigramas para el set de entrenamiento (inputs x, objetivos y)</span>
<span class="c1"># Primero un ejemplo:</span>

<span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">palabras</span><span class="p">[:</span><span class="mi">1</span><span class="p">]:</span>
  <span class="n">cs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cs</span><span class="p">,</span> <span class="n">cs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c2</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">)</span>
    <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix1</span><span class="p">)</span>
    <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix2</span><span class="p">)</span>

<span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
<span class="n">xs</span><span class="p">,</span> <span class="n">ys</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>. m
m a
a r
r i
i a
a .
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([ 0, 13,  1, 18,  9,  1]), tensor([13,  1, 18,  9,  1,  0]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Ahora todas las palabras</span>

<span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">palabras</span><span class="p">:</span>
  <span class="n">cs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cs</span><span class="p">,</span> <span class="n">cs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c2</span><span class="p">]</span>
    <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix1</span><span class="p">)</span>
    <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix2</span><span class="p">)</span>

<span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span> <span class="c1"># Pasamos cada bigrama a tensores x (inputs), y (predicción deseada)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
<span class="n">xs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 0, 13,  1,  ..., 12, 12,  1])
</pre></div>
</div>
</div>
</div>
<p>Para pasar esta información a una red neuronal, primero la codificaremos (haremos un <em>encoding</em>) en vectores vía <em>one-hot encoding</em>, ya que este formato es más conveniente para una red neuronal. Esto significa que nuestros vectores tendrán 27 elementos, y todos serán de valor 0 salvo aquel que ocupe el lugar del carácter correspondiente, el cual será 1.</p>
<p>Visualicemos, por ejemplo, el vector correspondiente a la letra «a», que se encuentra en la posición 1 de nuestro vocabulario (el punto <code class="docutils literal notranslate"><span class="pre">.</span></code> ocupa la posición 0):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="c1"># Primero veamos un ejemplo:</span>
<span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">6</span><span class="p">],</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">27</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">xenc</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])
</pre></div>
</div>
</div>
</div>
<p>Podemos crear una visualización más gráfica de 6 vectores codificados. Como digo, la posición del 1 en cada vector indica el índice de la letra a la que corresponde:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">xenc</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.image.AxesImage at 0x7f1da4dac410&gt;
</pre></div>
</div>
<img alt="_images/hacedor_52_1.png" src="_images/hacedor_52_1.png" />
</div>
</div>
<p>Ahora haremos lo mismo con todos los datos:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">27</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">xenc</span><span class="p">,</span> <span class="n">xenc</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[1., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 1., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 1., 0.,  ..., 0., 0., 0.]]), torch.Size([165469, 27]))
</pre></div>
</div>
</div>
</div>
<p>Ahora crearemos una capa de neuronas, asignando pesos aleatorios a nuestro modelo para que se multipliquen con las entradas y se optimicen mediante la propagación hacia atrás. La elección de las dimensiones de la matriz de pesos depende de nuestro criterio, aunque generalmente a más pesos, mejor desempeño. P</p>
<p>Primero, procuremos entender cómo funcionará la multiplicación de nuestros vectores con la matriz de pesos. Ejemplifiquemos con los primeros tres vectores (o sea, los primeros tres caracteres) de nuestra incrustación:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">27</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">xenc</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([3, 27]), torch.Size([27, 4]))
</pre></div>
</div>
</div>
</div>
<p>Nuestra matriz de pesos luce así:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">w</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.7493, -1.7217, -1.9662,  0.3226],
        [ 0.9598, -0.0769,  0.5544,  0.1202],
        [-0.7308,  1.0370,  0.2202,  0.4571],
        [ 0.7234,  0.6443,  0.5270,  0.9999],
        [ 2.1325, -1.2494, -0.4937, -0.0239],
        [-0.0110, -0.2827, -0.1645, -0.9663],
        [-0.2962, -0.1961,  1.0553,  1.9316],
        [ 0.0340, -1.6144,  0.0173,  0.6747],
        [ 0.6830,  0.5947,  0.8399,  0.3414],
        [-0.8134,  0.2195, -1.3937,  0.1083],
        [ 0.0147,  0.3515,  1.1222,  0.3504],
        [-0.3418, -0.5324,  0.7876,  0.0495],
        [ 1.3837,  1.4100, -0.6970,  0.2293],
        [ 0.1839,  0.5087, -0.9664,  0.4561],
        [-0.6544, -1.0187, -1.3252, -0.0363],
        [ 1.5762,  0.6825, -0.1226, -0.5034],
        [-1.4061, -0.2015,  1.5431, -2.3322],
        [ 1.7501, -0.7095,  0.5137, -1.4174],
        [-0.4061, -0.2551,  1.2554, -0.0525],
        [ 0.6981,  0.5954,  1.1393,  0.6059],
        [-1.4104, -0.8889, -1.2340, -0.1644],
        [-0.5133, -0.4082, -1.3096, -0.9333],
        [ 0.5903,  1.3089,  0.1734,  0.9023],
        [ 1.4701, -1.4298, -0.7756,  0.7064],
        [ 0.0126,  1.4738,  0.4572, -1.3859],
        [-0.7019, -0.3149, -0.2051,  0.2662],
        [ 0.0047,  1.0465,  1.3855,  1.1045]])
</pre></div>
</div>
</div>
</div>
<p>Nuestra matriz de vectores:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xenc</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.]])
</pre></div>
</div>
</div>
</div>
<p>Si multiplicamos ambas, obtenemos:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ejemplo</span> <span class="o">=</span> <span class="n">xenc</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span> <span class="o">@</span> <span class="n">w</span>
<span class="n">ejemplo</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.7493, -1.7217, -1.9662,  0.3226],
        [ 0.1839,  0.5087, -0.9664,  0.4561],
        [ 0.9598, -0.0769,  0.5544,  0.1202]])
</pre></div>
</div>
</div>
</div>
<p>El resultado de la multiplicación es una matriz con dimensiones 3x4. Para entender cómo se generó esta matriz, podemos tomar el primer vector de <code class="docutils literal notranslate"><span class="pre">xenc</span></code> y multiplicar cada uno de sus elementos por la primera columna de <code class="docutils literal notranslate"><span class="pre">w</span></code>. El primer vector luce así:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xenc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])
</pre></div>
</div>
</div>
</div>
<p>De manera que al multiplicarlo por la primera columna, elemento por elemento (es decir, realizando una multiplicación Hadamard, denotada comúnmente por el signo <span class="math notranslate nohighlight">\(\odot\)</span>), obtenemos:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xenc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">w</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-0.7493,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,
         0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,
        -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,
         0.0000, -0.0000,  0.0000])
</pre></div>
</div>
</div>
</div>
<p>Y la sumatoria de este vector claramente resulta:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">xenc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">w</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(-0.7493)
</pre></div>
</div>
</div>
</div>
<p>Que podemos observar en el primer valor de nuestra multiplicación de <code class="docutils literal notranslate"><span class="pre">xenc</span></code> con <code class="docutils literal notranslate"><span class="pre">w</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ejemplo</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.7493, -1.7217, -1.9662,  0.3226],
        [ 0.1839,  0.5087, -0.9664,  0.4561],
        [ 0.9598, -0.0769,  0.5544,  0.1202]])
</pre></div>
</div>
</div>
</div>
<p>Exactamente lo mismo, aunque de manera más eficiente, sucede cuando multiplicamos ambas matrices. En síntesis: al multiplicar nuestra matriz <code class="docutils literal notranslate"><span class="pre">w</span></code> por la matriz <code class="docutils literal notranslate"><span class="pre">xenc</span></code>, cada columna de pesos evalúa cada vector de <code class="docutils literal notranslate"><span class="pre">xenc</span></code>. Es decir, obtenemos una matriz de dimensiones 3x4 donde cada fila corresponde a cada vector (<em>i. e.</em>, cada carácter), pero esta fila tiene 4 valores correspondientes a la evaluación de cada vector por cada una de las columnas de la matriz <code class="docutils literal notranslate"><span class="pre">w</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#FORWARD PASS</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">27</span><span class="p">,</span> <span class="mi">27</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># Creamos weights aleatorios</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span> <span class="c1">#multiplicamos valores de x por w para obtener log-counts</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="c1"># exponenciamos para obtener valores mayores a 0, equivalentes a matriz N</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># normalizar los counts para obtener probabilidades</span>
<span class="n">probs</span> <span class="c1"># los últimos dos pasos son equivalentes a la función softmax</span>
<span class="n">logits</span><span class="o">.</span><span class="n">shape</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([165469, 27])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([165469, 27])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.0185, 0.0231, 0.0733, 0.0296, 0.0494, 0.0180, 0.0120, 0.0146, 0.0462,
        0.0402, 0.0079, 0.0088, 0.1057, 0.0884, 0.1183, 0.0207, 0.0331, 0.0831,
        0.0233, 0.0198, 0.0227, 0.0167, 0.0247, 0.0255, 0.0203, 0.0050, 0.0509],
       grad_fn=&lt;SelectBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nlls</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">ys</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;------&#39;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;bigrama ejemplo </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">fap</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="si">}{</span><span class="n">fap</span><span class="p">[</span><span class="n">y</span><span class="p">]</span><span class="si">}</span><span class="s1">, índices </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;input: </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;probabilidad del output, calculado por la red neuronal: </span><span class="si">{</span><span class="n">probs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;output correcto: </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">probs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;probabilidad asignada por la red al caracter correcto: </span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="n">logp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;log likelihood&#39;</span><span class="p">,</span> <span class="n">logp</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
  <span class="n">nll</span> <span class="o">=</span> <span class="o">-</span><span class="n">logp</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;negative log likelihood:&#39;</span><span class="p">,</span> <span class="n">nll</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
  <span class="n">nlls</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">nll</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-----&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;promedio de la nll, i. e. loss = </span><span class="si">{</span><span class="n">nlls</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>------
bigrama ejemplo 1: .m, índices 0,13
input: 0
probabilidad del output, calculado por la red neuronal: tensor([0.0185, 0.0231, 0.0733, 0.0296, 0.0494, 0.0180, 0.0120, 0.0146, 0.0462,
        0.0402, 0.0079, 0.0088, 0.1057, 0.0884, 0.1183, 0.0207, 0.0331, 0.0831,
        0.0233, 0.0198, 0.0227, 0.0167, 0.0247, 0.0255, 0.0203, 0.0050, 0.0509],
       grad_fn=&lt;SelectBackward0&gt;)
output correcto: 13
probabilidad asignada por la red al caracter correcto: 0.08839690685272217
log likelihood -2.4259183406829834
negative log likelihood: 2.4259183406829834
------
bigrama ejemplo 2: ma, índices 13,1
input: 13
probabilidad del output, calculado por la red neuronal: tensor([0.0101, 0.0230, 0.0574, 0.1025, 0.0128, 0.0272, 0.0830, 0.0031, 0.0193,
        0.0152, 0.0138, 0.0172, 0.0107, 0.0226, 0.0062, 0.0115, 0.0066, 0.0401,
        0.1341, 0.0745, 0.0124, 0.0121, 0.0635, 0.0064, 0.1110, 0.0103, 0.0934],
       grad_fn=&lt;SelectBackward0&gt;)
output correcto: 1
probabilidad asignada por la red al caracter correcto: 0.02303677797317505
log likelihood -3.770663261413574
negative log likelihood: 3.770663261413574
------
bigrama ejemplo 3: ar, índices 1,18
input: 1
probabilidad del output, calculado por la red neuronal: tensor([0.0020, 0.0371, 0.0136, 0.0462, 0.0118, 0.0143, 0.0620, 0.0552, 0.0064,
        0.1602, 0.0033, 0.0068, 0.0097, 0.1578, 0.0140, 0.1031, 0.0157, 0.0304,
        0.0065, 0.0056, 0.0288, 0.0345, 0.0556, 0.0660, 0.0103, 0.0338, 0.0094],
       grad_fn=&lt;SelectBackward0&gt;)
output correcto: 18
probabilidad asignada por la red al caracter correcto: 0.006546150892972946
log likelihood -5.028878211975098
negative log likelihood: 5.028878211975098
------
bigrama ejemplo 4: ri, índices 18,9
input: 18
probabilidad del output, calculado por la red neuronal: tensor([0.0201, 0.0250, 0.0671, 0.0216, 0.0542, 0.0093, 0.0164, 0.0783, 0.1021,
        0.0121, 0.0179, 0.0253, 0.0679, 0.0837, 0.1422, 0.0155, 0.0132, 0.0042,
        0.0315, 0.0173, 0.0807, 0.0070, 0.0116, 0.0336, 0.0226, 0.0103, 0.0092],
       grad_fn=&lt;SelectBackward0&gt;)
output correcto: 9
probabilidad asignada por la red al caracter correcto: 0.012082945555448532
log likelihood -4.415960311889648
negative log likelihood: 4.415960311889648
------
bigrama ejemplo 5: ia, índices 9,1
input: 9
probabilidad del output, calculado por la red neuronal: tensor([0.0289, 0.0632, 0.0248, 0.0174, 0.0440, 0.0109, 0.1917, 0.0348, 0.0201,
        0.0157, 0.0042, 0.1255, 0.0281, 0.0208, 0.0036, 0.0411, 0.0056, 0.0291,
        0.0451, 0.0214, 0.0144, 0.0381, 0.0193, 0.0171, 0.0582, 0.0345, 0.0425],
       grad_fn=&lt;SelectBackward0&gt;)
output correcto: 1
probabilidad asignada por la red al caracter correcto: 0.06315254420042038
log likelihood -2.762202024459839
negative log likelihood: 2.762202024459839
------
bigrama ejemplo 6: a., índices 1,0
input: 1
probabilidad del output, calculado por la red neuronal: tensor([0.0020, 0.0371, 0.0136, 0.0462, 0.0118, 0.0143, 0.0620, 0.0552, 0.0064,
        0.1602, 0.0033, 0.0068, 0.0097, 0.1578, 0.0140, 0.1031, 0.0157, 0.0304,
        0.0065, 0.0056, 0.0288, 0.0345, 0.0556, 0.0660, 0.0103, 0.0338, 0.0094],
       grad_fn=&lt;SelectBackward0&gt;)
output correcto: 0
probabilidad asignada por la red al caracter correcto: 0.001964769558981061
log likelihood -6.232380390167236
negative log likelihood: 6.232380390167236
-----
promedio de la nll, i. e. loss = 4.1060004234313965
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 0, 13,  1,  ..., 12, 12,  1])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ys</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([13,  1, 18,  ..., 12,  1,  0])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">13</span><span class="p">],</span> <span class="n">probs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">probs</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">18</span><span class="p">],</span> <span class="n">probs</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">9</span><span class="p">],</span> <span class="n">probs</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">probs</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor(0.0884, grad_fn=&lt;SelectBackward0&gt;),
 tensor(0.0230, grad_fn=&lt;SelectBackward0&gt;),
 tensor(0.0065, grad_fn=&lt;SelectBackward0&gt;),
 tensor(0.0121, grad_fn=&lt;SelectBackward0&gt;),
 tensor(0.0632, grad_fn=&lt;SelectBackward0&gt;),
 tensor(0.0020, grad_fn=&lt;SelectBackward0&gt;))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">W</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(706.2115, grad_fn=&lt;SumBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># AHORA TODO JUNTO:</span>

<span class="n">num</span> <span class="o">=</span> <span class="n">xs</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>

<span class="c1"># FORWARD PASS</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
  <span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">27</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="c1"># Ahora hacemos un one-hot encoding</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span> <span class="c1">#multiplicamos valores de x por w para obtener log-counts</span>
  <span class="n">counts</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="c1"># exponenciamos para obtener valores mayores a 0, equivalentes a matriz N</span>
  <span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># normalizar los counts para obtener probabilidades</span>
  <span class="n">probs</span> <span class="c1"># los últimos dos pasos son equivalentes a la función softmax</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num</span><span class="p">),</span> <span class="n">ys</span><span class="p">]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.01</span><span class="o">*</span><span class="p">(</span><span class="n">W</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
  
  <span class="c1"># BACKWARD PASS</span>
  <span class="n">W</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

  <span class="c1"># UPDATE</span>
  <span class="n">W</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="mi">50</span> <span class="o">*</span> <span class="n">W</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3.905247926712036
3.2726011276245117
2.951911687850952
2.8040242195129395
2.7105953693389893
2.648975372314453
2.603548526763916
2.568493127822876
2.5400938987731934
2.516664743423462
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
  <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">ix</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">ix</span><span class="p">]),</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">27</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fap</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">break</span>

  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ma.
meio.
erndo.
rmero.
a.
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="redes.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Elementos de redes neuronales</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="definiciones.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Elementos terminológicos de matemáticas computacionales</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Dante Noguez<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>