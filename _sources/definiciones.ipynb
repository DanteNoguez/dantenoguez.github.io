{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Álgebra lineal"
      ],
      "metadata": {
        "id": "AGCfmhzSyaVJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kaIuGpK6Yy-w"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La notación convencional de un **vector** es con una variable minúscula y en negritas. Por ejemplo, el vector $x \\in \\mathbb{R}^{n \\times 1}$. Para referirmos a una de sus filas (elementos, ejemplos, dimensiones), utilizamos la variable en itálicas y el superíndice: $x^{(3)}$. La notación en Python es muy similar, salvo por la indexación en 0:"
      ],
      "metadata": {
        "id": "HpvOjyuOY19H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.zeros(4)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1mgqbZAY1Oy",
        "outputId": "9c687f2c-d6ca-4570-9948-147eb7caadba"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[2] = 1\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwWw9zOMaVQa",
        "outputId": "baa223fb-d384-407c-a246-7e65fd6458cd"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f'El vector tiene una dimensión: {x.shape}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "xDvv0SxWK5t2",
        "outputId": "ecb544c0-d10c-4401-9c52-66d8b49c7b15"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'El vector tiene una dimensión: torch.Size([4])'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una **matriz**, por otro lado, se denota con una letra mayúscula y negritas. Por ejemplo, la matriz (o tensor) $A \\in \\mathbb{R}^{n \\times m}$. En este caso, cada columna (dimensión, feature) utilizaremos el subíndice: $x_{3}$; y cada elemento de la matriz indicará la fila y columna en la que se ubica: $x^{(2)}_{3}$, aunque también se puede utilizar alternativamente $x^{2 \\times 3}$. En Python también funciona similar, salvo por la indexación.\n",
        "\n",
        "Pongamos como ejemplo $A \\in \\mathbb{R}^{3 \\times 3}$"
      ],
      "metadata": {
        "id": "Gyzmti-oak-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.zeros((3, 2))\n",
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDQl6psiaYg9",
        "outputId": "d2479925-2f8d-4a8d-97d3-a83ed4a6705b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cambiemos el valor de $A^{(3)}_{2}$, cuyos índices programáticos son un número menor dada la indexación en cero:"
      ],
      "metadata": {
        "id": "Yh4RCQJ8dqqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primero fila, luego columna\n",
        "A[2][1] = 1\n",
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpRwFDWRcyku",
        "outputId": "03cf95f2-edac-4418-defa-7eb57ff3a387"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# También es válido:\n",
        "A[2, 1] = 2\n",
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4algiG2Oc2Ce",
        "outputId": "92b79a28-311c-43c4-b98e-2c258341587f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 2.]])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f'La matriz tiene dos dimensiones: {A.shape}', A.size(dim=0), A.size(dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1xVzawjLCfx",
        "outputId": "8f4d9499-751a-48be-914b-a9feca9859db"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('La matriz tiene dos dimensiones: torch.Size([3, 2])', 3, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sin embargo, esto puede ser confuso cuando por ejemplo hacemos operaciones a través de una dimensión (o eje, o *axis*). Por ejemplo, si sumáramos a través de la dimensión 0, esperaríamos que se sumen los números por cada fila:"
      ],
      "metadata": {
        "id": "3XkptsqYPcjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A, A.sum(0, keepdim=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hatu6nX4Nx0z",
        "outputId": "48bad589-0f54-4d03-cfc1-5a7744725b72"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 2.]]), tensor([[0., 2.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pero el resultado indica que se sumaron las columnas. Inversamente pasa lo mismo:"
      ],
      "metadata": {
        "id": "llPq-LMMPtY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A.sum(1, keepdim=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9UeMyukOKFX",
        "outputId": "ee6c97ce-75a6-4323-d22d-7845f5c90ba1"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [0.],\n",
              "        [2.]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En realidad, lo que sucede es que colapsamos el eje seleccionado y luego sumamos los valores, es decir, que la dimensión que seleccionamos para la sumatoria cambia su tamaño a 1. Esta aparente rareza tiene más sentido cuando hablamos de **tensores**, es decir, de matrices con más de dos dimensiones. Por ejemplo: "
      ],
      "metadata": {
        "id": "p4PnU4opQPap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B = torch.zeros((4,3,2))\n",
        "B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx-zTfTQLFDt",
        "outputId": "f1ad18f3-449d-4db1-b30d-150ad1608ba3"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B[1][2][1] = 1\n",
        "B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxHuTYgfMc6Z",
        "outputId": "64a79b06-c8b7-40d7-937d-8ee9ecd9f2bb"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 1.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la sumatoria a través de la dimensión 0, «colapsamos» cada bloque con los demás y los sumamos:"
      ],
      "metadata": {
        "id": "42lZEeJWRJ7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B.sum(0, keepdim=True), B.sum(0, keepdim=True).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OukCla_YQ1iI",
        "outputId": "fdeb36c5-87c3-4674-a166-5c273aa20c70"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0., 0.],\n",
              "          [0., 0.],\n",
              "          [0., 1.]]]), torch.Size([1, 3, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la sumatoria a través de la dimensión 1, colapsamos todas las filas de cada bloque y las sumamos:"
      ],
      "metadata": {
        "id": "vRgFxD7GROlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B.sum(1, keepdim=True), B.sum(1, keepdim=True).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRzLq1IIQsqn",
        "outputId": "f335c65e-4c22-46a8-a3ae-ca07294a60f1"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0., 0.]],\n",
              " \n",
              "         [[0., 1.]],\n",
              " \n",
              "         [[0., 0.]],\n",
              " \n",
              "         [[0., 0.]]]), torch.Size([4, 1, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la sumatoria a través de la dimensión 2, colapsamos todas las columnas y las sumamos:"
      ],
      "metadata": {
        "id": "y1fOO8KZRYxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B.sum(2, keepdim=True), B.sum(2, keepdim=True).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xd2z_gzQ4nY",
        "outputId": "dd5b1316-1d68-4dd2-d2bf-2b2fe66b0a68"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.],\n",
              "          [0.],\n",
              "          [0.]],\n",
              " \n",
              "         [[0.],\n",
              "          [0.],\n",
              "          [1.]],\n",
              " \n",
              "         [[0.],\n",
              "          [0.],\n",
              "          [0.]],\n",
              " \n",
              "         [[0.],\n",
              "          [0.],\n",
              "          [0.]]]), torch.Size([4, 3, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Las dimensiones de nuestro tensor B son:\n",
        "B.shape, B.size(dim=0), B.size(dim=1), B.size(dim=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj5lbm9TMAeY",
        "outputId": "06d151dc-a462-4196-bb8d-e1ac1d66ec8c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 3, 2]), 4, 3, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cálculo"
      ],
      "metadata": {
        "id": "EMh5iyGIfUI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Derivada, gradiente**:\n",
        "\n",
        "$\n",
        "f^{\\prime}(x)=\\lim _{h \\rightarrow 0} \\frac{f(x+h)-f(x)}{h}\n",
        "$\n",
        "\n",
        "Otra fórmula luce así:\n",
        "$\n",
        "\\frac{\\partial f(x, y)}{\\partial x}=\\frac{f(x+h, y)-f(x, y)}{h}\n",
        "$\n",
        "\n",
        "Esta última se lee: la derivada de $f$ con respecto a $x$."
      ],
      "metadata": {
        "id": "ZszfZxB2yeoy"
      }
    }
  ]
}