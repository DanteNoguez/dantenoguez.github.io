

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Elementos de procesamiento de lenguajes naturales, parte III &#8212; Dantis Elementorum</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-C9T8ZX64EM"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-C9T8ZX64EM');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/alfa/hacedor3';</script>
    <link rel="canonical" href="https://dantenoguez.github.io/notebooks/alfa/hacedor3.html" />
    <link rel="shortcut icon" href="../../_static/logo.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Dantis Elementorum
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Elementorum Physicæ Mathematicæ</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="programacion.html">Elementos de programación</a></li>
<li class="toctree-l1"><a class="reference internal" href="redes.html">Elementos de redes neuronales</a></li>
<li class="toctree-l1"><a class="reference internal" href="calculo.html">Elementos de cálculo infinitesimal</a></li>
<li class="toctree-l1"><a class="reference internal" href="hacedor.html">Elementos de procesamiento de lenguajes naturales, o El Hacedor</a></li>
<li class="toctree-l1"><a class="reference internal" href="hacedor2.html">Elementos de procesamiento de lenguajes naturales, parte II</a></li>
<li class="toctree-l1"><a class="reference internal" href="digitalbio.html">Elementos de biología digital</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Elementorum Politicæ Œconomicæ Iurisprudentiæ</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../beta/iurisprudentiae.html">Elementos de leyes, o Nova methodus discendæ iurisprudentiæ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beta/schumpeter.html">Notas a Schumpeter</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Elementorum Poeticæ</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../poeticae/borges.html">Elementos de literatura borgesiana, o Borges y sus precursores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../poeticae/afro.html">Elementos de música afroamericana</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exagium</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../exagium/tzompantli.html">Breve ensayo sobre el tzompantli</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exagium/diccionario.html">Diccionario fantástico de la lengua española</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exagium/escribir.html">Escribir ¿para qué?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exagium/secturnial.html">Relatos secturniales</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exagium/castillos.html">La importancia de ser olvidado</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exagium/aforismos.html">Cuaderno de notas</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exagium/ia.html">Tientos cibernéticos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exagium/coronavirus.html">Bitácora del confinamiento</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/DanteNoguez/Ensayos" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/DanteNoguez/Ensayos/issues/new?title=Issue%20on%20page%20%2Fnotebooks/alfa/hacedor3.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/notebooks/alfa/hacedor3.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Elementos de procesamiento de lenguajes naturales, parte III</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inicializacion-de-pesos-y-sesgos">Inicialización de pesos y sesgos</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#propiedades-de-la-tangente-hiperbolica">Propiedades de la tangente hiperbólica</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inicializacion-kaiming">Inicialización Kaiming</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalizacion-por-lotes-batch-normalization">Normalización por lotes (<em>batch normalization</em>)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch">PyTorch</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="elementos-de-procesamiento-de-lenguajes-naturales-parte-iii">
<h1>Elementos de procesamiento de lenguajes naturales, parte III<a class="headerlink" href="#elementos-de-procesamiento-de-lenguajes-naturales-parte-iii" title="Permalink to this heading">#</a></h1>
<p>Continuando nuestra última lección, nos adentraremos en el mundo de las «tecniquerías», procurando continuar con la optimización de nuestra red neuronal. En particular, aprenderemos sobre la normalización de lotes (<em>batch normalization</em>),</p>
<p>Para ello, utilizaremos los mismos datos y el mismo código que implementamos con base en el <em>paper</em> <a class="reference external" href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>!wget https://github.com/DanteNoguez/CalculusRatiocinator/raw/main/data/nombres.txt
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nombres</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;nombres.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
<span class="n">nombres</span><span class="p">[:</span><span class="mi">8</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;maria&#39;, &#39;rosa&#39;, &#39;jose&#39;, &#39;carmen&#39;, &#39;ana&#39;, &#39;juana&#39;, &#39;antonio&#39;, &#39;elena&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">nombres</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>21029
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">V</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">nombres</span><span class="p">)))</span>
<span class="n">paf</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="p">:</span><span class="n">f</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">f</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">V</span><span class="p">)}</span>
<span class="n">paf</span><span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">fap</span> <span class="o">=</span> <span class="p">{</span><span class="n">f</span><span class="p">:</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span><span class="p">,</span><span class="n">f</span> <span class="ow">in</span> <span class="n">paf</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fap</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{1: &#39;a&#39;, 2: &#39;b&#39;, 3: &#39;c&#39;, 4: &#39;d&#39;, 5: &#39;e&#39;, 6: &#39;f&#39;, 7: &#39;g&#39;, 8: &#39;h&#39;, 9: &#39;i&#39;, 10: &#39;j&#39;, 11: &#39;k&#39;, 12: &#39;l&#39;, 13: &#39;m&#39;, 14: &#39;n&#39;, 15: &#39;o&#39;, 16: &#39;p&#39;, 17: &#39;q&#39;, 18: &#39;r&#39;, 19: &#39;s&#39;, 20: &#39;t&#39;, 21: &#39;u&#39;, 22: &#39;v&#39;, 23: &#39;w&#39;, 24: &#39;x&#39;, 25: &#39;y&#39;, 26: &#39;z&#39;, 0: &#39;.&#39;}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nombres</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;maria&#39;, &#39;rosa&#39;, &#39;jose&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">block_size</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># longitud del contexto</span>
<span class="k">def</span> <span class="nf">construir_dataset</span><span class="p">(</span><span class="n">nombres</span><span class="p">):</span>
  <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">nombres</span><span class="p">:</span>
    <span class="c1">#print(f&#39;nombre: {n}&#39;)</span>
    <span class="n">contexto</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">block_size</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">n</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">:</span>
      <span class="n">ix</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c</span><span class="p">]</span>
      <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">contexto</span><span class="p">)</span>
      <span class="n">Y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
      <span class="c1">#print(&#39;&#39;.join(fap[i] for i in contexto), &#39;----&gt; &#39;, fap[ix])</span>
      <span class="n">contexto</span> <span class="o">=</span> <span class="n">contexto</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ix</span><span class="p">]</span>
  
  <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># contexto</span>
  <span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="c1"># objetivo</span>
  <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">construir_dataset</span><span class="p">(</span><span class="n">nombres</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>nombre: maria
... ----&gt;  m
..m ----&gt;  a
.ma ----&gt;  r
mar ----&gt;  i
ari ----&gt;  a
ria ----&gt;  .
nombre: rosa
... ----&gt;  r
..r ----&gt;  o
.ro ----&gt;  s
ros ----&gt;  a
osa ----&gt;  .
nombre: jose
... ----&gt;  j
..j ----&gt;  o
.jo ----&gt;  s
jos ----&gt;  e
ose ----&gt;  .
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[ 0,  0,  0],
         [ 0,  0, 13],
         [ 0, 13,  1],
         [13,  1, 18],
         [ 1, 18,  9],
         [18,  9,  1],
         [ 0,  0,  0],
         [ 0,  0, 18],
         [ 0, 18, 15],
         [18, 15, 19],
         [15, 19,  1],
         [ 0,  0,  0],
         [ 0,  0, 10],
         [ 0, 10, 15],
         [10, 15, 19],
         [15, 19,  5]]),
 tensor([13,  1, 18,  9,  1,  0, 18, 15, 19,  1,  0, 10, 15, 19,  5,  0]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">nombres</span><span class="p">)</span>
<span class="n">n1</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">nombres</span><span class="p">))</span>
<span class="n">n2</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.9</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">nombres</span><span class="p">))</span>

<span class="n">Xtr</span><span class="p">,</span> <span class="n">Ytr</span> <span class="o">=</span> <span class="n">construir_dataset</span><span class="p">(</span><span class="n">nombres</span><span class="p">[:</span><span class="n">n1</span><span class="p">])</span>
<span class="n">Xdev</span><span class="p">,</span> <span class="n">Ydev</span> <span class="o">=</span> <span class="n">construir_dataset</span><span class="p">(</span><span class="n">nombres</span><span class="p">[</span><span class="n">n1</span><span class="p">:</span><span class="n">n2</span><span class="p">])</span>
<span class="n">Xte</span><span class="p">,</span> <span class="n">Yte</span> <span class="o">=</span> <span class="n">construir_dataset</span><span class="p">(</span><span class="n">nombres</span><span class="p">[</span><span class="n">n2</span><span class="p">:])</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">construir_dataset</span><span class="p">(</span><span class="n">nombres</span><span class="p">)</span>

<span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Xtr</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Xdev</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Xte</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([165469, 3]),
 torch.Size([132452, 3]),
 torch.Size([16468, 3]),
 torch.Size([16549, 3]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">emb_dim</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># cada embedding tendrá 10 dimensiones</span>
<span class="n">h</span> <span class="o">=</span> <span class="mi">200</span> <span class="c1"># cada capa tendrá 200 unidades (hidden units)</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">fap</span><span class="p">)</span> <span class="c1">#tamaño del vocabulario</span>

<span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">)</span> <span class="c1"># tabla de consulta</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">emb_dim</span><span class="o">*</span><span class="n">block_size</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span> <span class="c1"># pesos ocultos</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="c1"># sesgos ocultos</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span> <span class="c1"># pesos de la segunda capa</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span> <span class="c1"># sesgos de la segunda capa</span>

<span class="n">parametros</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">b</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">))</span> <span class="c1"># número total de parámetros</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
  <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>11897
</pre></div>
</div>
</div>
</div>
<p>Como breve recordatorio, entrenemos nuestra red neuronal y veamos cómo lucía nuestra función de pérdida en nuestra lección anterior:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">max_steps</span> <span class="o">=</span> <span class="mi">10000</span> 
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">paso_i</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">perdidas_i</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
  <span class="c1">#minibatch («minilote»)</span>
  <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,))</span>

  <span class="c1"># propagación hacia delante</span>
  <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">Xtr</span><span class="p">[</span><span class="n">ix</span><span class="p">]]</span>
  <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">*</span><span class="n">block_size</span><span class="p">)</span> <span class="o">@</span> <span class="n">H</span> <span class="o">+</span> <span class="n">d</span><span class="p">)</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">U</span> <span class="o">+</span> <span class="n">b</span>
  <span class="n">perdida</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
  
  <span class="c1"># propagación hacia atrás</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">perdida</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

  <span class="c1"># actualización</span>
  <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">5000</span> <span class="k">else</span> <span class="mf">0.01</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>

  <span class="c1"># registrar estadísticas</span>
  <span class="n">paso_i</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
  <span class="n">perdidas_i</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">perdida</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">paso_i</span><span class="p">,</span> <span class="n">perdidas_i</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/6ef1a3628c1dfacc417007a9f9fca79e26bc8422112e1be9a5f10ac19c05b56c.png" src="../../_images/6ef1a3628c1dfacc417007a9f9fca79e26bc8422112e1be9a5f10ac19c05b56c.png" />
</div>
</div>
<p>Una función que nos permite obtener más fácilmente el valor actual de la función de pérdida, según queramos analizar uno u otro <em>split</em> —conjunto de datos—. Nuestra función estará precedida por un «decorador». En esencia, un decorador altera la función que precede (o que «decora»). En este caso, el decorador que utilizamos indica al programa que no debe rastrear los gradientes de nuestros parámetros. Así, nuestro programa fluye con mayor rapidez, puesto que disminuye su carga de trabajo.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span> <span class="c1"># decorador</span>
<span class="k">def</span> <span class="nf">perdida_split</span><span class="p">(</span><span class="n">split</span><span class="p">):</span>
  <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xtr</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">),</span>
         <span class="s1">&#39;val&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xdev</span><span class="p">,</span> <span class="n">Ydev</span><span class="p">),</span>
         <span class="s1">&#39;test&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xte</span><span class="p">,</span> <span class="n">Yte</span><span class="p">)}[</span><span class="n">split</span><span class="p">]</span>
  <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
  <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">*</span><span class="n">block_size</span><span class="p">)</span> <span class="o">@</span> <span class="n">H</span> <span class="o">+</span> <span class="n">d</span><span class="p">)</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">U</span> <span class="o">+</span> <span class="n">b</span>
  <span class="n">perdida</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">split</span><span class="p">,</span> <span class="n">perdida</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="n">perdida_split</span><span class="p">(</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
<span class="n">perdida_split</span><span class="p">(</span><span class="s1">&#39;val&#39;</span><span class="p">)</span>
<span class="n">perdida_split</span><span class="p">(</span><span class="s1">&#39;test&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train 2.1176223754882812
val 2.1515469551086426
test 2.1483922004699707
</pre></div>
</div>
</div>
</div>
<section id="inicializacion-de-pesos-y-sesgos">
<h2>Inicialización de pesos y sesgos<a class="headerlink" href="#inicializacion-de-pesos-y-sesgos" title="Permalink to this heading">#</a></h2>
<p>Nuestra inicialización debería ser equivalente a <span class="math notranslate nohighlight">\(\frac{1}{27}\)</span>, puesto que cualquier carácter debería ser igualmente probable que los demás antes de entrenar la red. Sin embargo, nuestra pérdida inicial es muchísimo peor. Según nuestro razonamiento, más bien deberíamos esperar una pérdida de:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mf">27.0</span><span class="p">)</span><span class="o">.</span><span class="n">log</span><span class="p">()</span> <span class="c1"># negative log likelihood</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(3.2958)
</pre></div>
</div>
</div>
</div>
<p>Para ejemplificar lo que está pasando, veamos un ejemplo a menor escala con cuatro <em>logits</em>. Cuando obtenemos valores aleatorios, nuestra distribución de probabilidades es normal —debido al diseño de <code class="docutils literal notranslate"><span class="pre">torch.randn</span></code>— y nuestros valores son cercanos a cero, de manera que una función de pérdida iniciada con estos parámetros representa buenamente nuestro razonamiento anterior:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logits_ej</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">probs_ej</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits_ej</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">perdida_ej</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs_ej</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>

<span class="n">logits_ej</span><span class="p">,</span> <span class="n">probs_ej</span><span class="p">,</span> <span class="n">perdida_ej</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([-1.1220,  0.1112, -0.9848, -0.4849]),
 tensor([0.1339, 0.4595, 0.1535, 0.2531]),
 tensor(1.8738))
</pre></div>
</div>
</div>
</div>
<p>Ahora, ejemplifiquemos lo que realmente está pasando en nuestra red.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logits_ej</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span> <span class="c1"># multiplicamos por 10 los logits</span>
<span class="n">probs_ej</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits_ej</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">perdida_ej</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs_ej</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>

<span class="n">logits_ej</span><span class="p">,</span> <span class="n">probs_ej</span><span class="p">,</span> <span class="n">perdida_ej</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([ 17.4183, -17.6220,   4.1508,  17.9295]),
 tensor([3.7490e-01, 2.2704e-16, 6.4850e-07, 6.2510e-01]),
 tensor(14.2486))
</pre></div>
</div>
</div>
</div>
<p>Dado que nuestros <em>logits</em> ahora adquieren valores más extremos, la distribución se arruina puesto que divergen con mayor magnitud entre sí. Como resultado, la pérdida inicial es gigantesca. En esta misma línea, podemos inferir que nuestra pérdida mejoraría si disminuimos nuestros pesos en la inicialización, multiplicándolos por un número sumamente pequeño. Los sesgos, de la misma manera, tendrían que ser reducidos a cero inicialmente. La finalidad de todo esto es eficientar el proceso de aprendizaje de la red, puesto que de otra forma desperdiciaría los primeros cientos o miles de iteraciones intentando corregir estas divergencias en la distribución de probabilidades asignadas a las predicciones.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">)</span> <span class="c1"># tabla de consulta</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">emb_dim</span><span class="o">*</span><span class="n">block_size</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span> <span class="c1"># pesos ocultos</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="c1"># sesgos ocultos</span>

<span class="n">U</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span> <span class="c1"># pesos de la segunda capa</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.0</span> <span class="c1"># sesgos de la segunda capa</span>

<span class="n">parametros</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">b</span><span class="p">]</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
  <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">max_steps</span> <span class="o">=</span> <span class="mi">10000</span> 
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">paso_i</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">perdidas_i</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
  <span class="c1">#minibatch («minilote»)</span>
  <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,))</span>

  <span class="c1"># propagación hacia delante</span>
  <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">Xtr</span><span class="p">[</span><span class="n">ix</span><span class="p">]]</span>
  <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">*</span><span class="n">block_size</span><span class="p">)</span> <span class="o">@</span> <span class="n">H</span> <span class="o">+</span> <span class="n">d</span><span class="p">)</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">U</span> <span class="o">+</span> <span class="n">b</span>
  <span class="n">perdida</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
  
  <span class="c1"># propagación hacia atrás</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">perdida</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

  <span class="c1"># actualización</span>
  <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">5000</span> <span class="k">else</span> <span class="mf">0.01</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>

  <span class="c1"># registrar estadísticas</span>
  <span class="n">paso_i</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
  <span class="n">perdidas_i</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">perdida</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">paso_i</span><span class="p">,</span> <span class="n">perdidas_i</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/7b61aa00cf657f18cd34337ad0f389d179efda402260cb188c50065d4a9ebf6d.png" src="../../_images/7b61aa00cf657f18cd34337ad0f389d179efda402260cb188c50065d4a9ebf6d.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">perdida_split</span><span class="p">(</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
<span class="n">perdida_split</span><span class="p">(</span><span class="s1">&#39;val&#39;</span><span class="p">)</span>
<span class="n">perdida_split</span><span class="p">(</span><span class="s1">&#39;test&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train 1.8836445808410645
val 1.9156681299209595
test 1.9303653240203857
</pre></div>
</div>
</div>
</div>
<p>Nuestro gráfico de la pérdida ahora luce diferente, y nuestra pérdida final es mejor debido a que no desperdiciamos etapas de entrenamiento.</p>
</section>
<section id="propiedades-de-la-tangente-hiperbolica">
<h2>Propiedades de la tangente hiperbólica<a class="headerlink" href="#propiedades-de-la-tangente-hiperbolica" title="Permalink to this heading">#</a></h2>
<p>En nuestra <span class="xref myst">primera lección</span> sobre redes neuronales, implementamos el cálculo del gradiente de la tangente hiperbólica con la fórmula <code class="docutils literal notranslate"><span class="pre">(1</span> <span class="pre">-</span> <span class="pre">t**2)</span></code>. Por otro lado, la tangente hiperbólica «aplasta» nuestros parámetros durante el entrenamiento, dejándolos en valores que van del 0 al 1. Si somos cuidadosos, notaremos que cuando hacemos propagación hacia atrás, nuestros parámetros que después de la tangente hiperbólica sean 1 o -1, tendrán un gradiente de 0 y, por tanto, no serán entrenados. Su «influencia» en el resultado de la red neuronal será también 0.</p>
<p>Ahora, procuraremos cuidar que nuestra red no padezca de este problema. Analicemos, por ejemplo, cómo están distribuidos nuestros parámetros después de aplicarles la tangente hiperbólica (esta es nuestra variable <code class="docutils literal notranslate"><span class="pre">a</span></code>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="mi">50</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/06bd02d078743acbf7cb82ff72606147ffcd9e2a21b256ad77a0f52b14eac5dc.png" src="../../_images/06bd02d078743acbf7cb82ff72606147ffcd9e2a21b256ad77a0f52b14eac5dc.png" />
</div>
</div>
<p>Por otra parte, estos son nuestros valores antes de pasar por <code class="docutils literal notranslate"><span class="pre">tanh</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">((</span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">*</span><span class="n">block_size</span><span class="p">)</span> <span class="o">@</span> <span class="n">H</span> <span class="o">+</span> <span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="mi">50</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/71828a6dc00e842b9958534f5bf7517c55ebb8a6755d45a7ee61d9d5891d5108.png" src="../../_images/71828a6dc00e842b9958534f5bf7517c55ebb8a6755d45a7ee61d9d5891d5108.png" />
</div>
</div>
<p>Podemos ver que nuestros valores tienen valores extremos que, en consecuencia, terminan en las «colas» (<em>tails</em>) de la tangente hiperbólica, es decir, con valores de -1 y 1. Para visualizar esto de mejor manera, pongamos que todos los valores que sean -1 y 1 sean blancos, y negros si tienen cualquier otro valor:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.99</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.image.AxesImage at 0x7f6b04ac19d0&gt;
</pre></div>
</div>
<img alt="../../_images/3aedfdba15130b7e53c3882302659111e829cbd2a53206ad1f7c2a3584d9b0a4.png" src="../../_images/3aedfdba15130b7e53c3882302659111e829cbd2a53206ad1f7c2a3584d9b0a4.png" />
</div>
</div>
<p>Hay una cantidad ingente de blancos y esto es una mala señal, puesto que implica que muchos de nuestros parámetros no están aprendiendo nada. La solución es idéntica a nuestro ejercicio anterior: si evitamos valores extremos, minimizando los valores iniciales de los parámetros y sesgos de la primera capa, entonces deberíamos obtener una mejor y más compacta distribución de valores, evitando así valores extremos y mejorando el entrenamiento. Comprobémoslo:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">)</span> <span class="c1"># tabla de consulta</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">emb_dim</span><span class="o">*</span><span class="n">block_size</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.3</span> <span class="c1"># pesos ocultos</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span> <span class="c1"># sesgos ocultos</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span> <span class="c1"># pesos de la segunda capa</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.0</span>

<span class="n">parametros</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">b</span><span class="p">]</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
  <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
  <span class="c1">#minibatch («minilote»)</span>
  <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,))</span>

  <span class="c1"># propagación hacia delante</span>
  <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">Xtr</span><span class="p">[</span><span class="n">ix</span><span class="p">]]</span>
  <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">*</span><span class="n">block_size</span><span class="p">)</span> <span class="o">@</span> <span class="n">H</span> <span class="o">+</span> <span class="n">d</span><span class="p">)</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">U</span> <span class="o">+</span> <span class="n">b</span>
  <span class="n">perdida</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
  
  <span class="c1"># propagación hacia atrás</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">perdida</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

  <span class="c1"># actualización</span>
  <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">5000</span> <span class="k">else</span> <span class="mf">0.01</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">perdida_split</span><span class="p">(</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
<span class="n">perdida_split</span><span class="p">(</span><span class="s1">&#39;val&#39;</span><span class="p">)</span>
<span class="n">perdida_split</span><span class="p">(</span><span class="s1">&#39;test&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train 1.867079734802246
val 1.8933453559875488
test 1.916183590888977
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.99</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.image.AxesImage at 0x7f6b04a74ed0&gt;
</pre></div>
</div>
<img alt="../../_images/0aa4d20f569e731b93bcebaa24a52396e5f07b70c1b9448f499f321c29568e93.png" src="../../_images/0aa4d20f569e731b93bcebaa24a52396e5f07b70c1b9448f499f321c29568e93.png" />
</div>
</div>
<p>Ahora nuestra pérdida mejoró y predomina el negro —valores distintos de -1 y 1— en nuestra red.</p>
</section>
<section id="inicializacion-kaiming">
<h2>Inicialización Kaiming<a class="headerlink" href="#inicializacion-kaiming" title="Permalink to this heading">#</a></h2>
<p>La generalización y formalización de nuestro razonamiento fue llevada a cabo por Kaiming He en un <a class="reference external" href="https://arxiv.org/abs/1502.01852"><em>paper</em> del 2015</a>. Análisis matemáticos mediante, descubrió que cuando utilizamos la tangente hiperbólica, nuestros pesos iniciales deben estar escalados por la fórmula <span class="math notranslate nohighlight">\(\frac{\frac{5}{3}}{\sqrt{\text{fan-in}}}\)</span>, donde <em>fan-in</em> —una expresión tomada de los circuitos eléctricos— es el número de <em>inputs</em> que toman nuestros pesos (en nuestro caso, <code class="docutils literal notranslate"><span class="pre">emb_dim*block_size</span></code>). Por tanto, nuestra inicialización debería estar escalada por <span class="math notranslate nohighlight">\(\frac{\frac{5}{3}}{\sqrt{\text{fan-in}}} = \frac{\frac{5}{3}}{\sqrt{\text{emb_dim x block_size}}} = \frac{\frac{5}{3}}{\sqrt{30}}\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">5</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="mi">30</span><span class="o">**</span><span class="mf">0.5</span> <span class="c1"># elevar a 0.5 es equivalente a obtener la raíz cuadrada</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.3042903097250923
</pre></div>
</div>
</div>
</div>
<p>Y obtenemos el valor que habíamos utilizado. La intuición detrás de esto es que, como decíamos, queremos minimizar los parámetros en la inicialización, y queremos también que tengan una distribución normal (gaussiana). Para conseguirlo, debemos multiplicar nuestros pesos iniciales por un número pequeño; pero para que la distribución consiga normalidad, este número pequeño debe tener ciertas cualidades —precisamente, las que la fórmula obtiene.</p>
<p>Ejemplifiquemos: supongamos que <code class="docutils literal notranslate"><span class="pre">x</span></code> es nuestro <em>input</em> y <code class="docutils literal notranslate"><span class="pre">w</span></code> nuestra matriz de pesos. Al multiplicarlos entre sí —como hacemos en una red neuronal—, nuestra distribución cambia, aumentando la desviación estándar —la extensión de los valores—:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(-0.0004) tensor(1.0086)
tensor(-0.0133) tensor(3.1457)
</pre></div>
</div>
<img alt="../../_images/75e6ad267172f2542e71c6efe449e7cfc252ba6dc08de7bdba100080e36191ec.png" src="../../_images/75e6ad267172f2542e71c6efe449e7cfc252ba6dc08de7bdba100080e36191ec.png" />
</div>
</div>
<p>Si al inicio los valores de <code class="docutils literal notranslate"><span class="pre">x</span></code> iban del -4 al 4, tras ser multiplicados por <code class="docutils literal notranslate"><span class="pre">w</span></code> ahora van del -15 al 15. Hay valores más alejados del centro de la distribución que antes, por lo que la desviación estándar es más alta. Recordemos que esto arruinaba la tangente hiperbólica. Pero si minimizamos los valores de <code class="docutils literal notranslate"><span class="pre">w</span></code>, al multiplicarlos por <code class="docutils literal notranslate"><span class="pre">x</span></code> la distribución no se expanderá. Todavía mejor: podemos minimizar los valores de <code class="docutils literal notranslate"><span class="pre">w</span></code> de tal forma que <code class="docutils literal notranslate"><span class="pre">x</span></code> mantenga exactamente la misma distribución. Para ello utilizamos las fórmulas de Kaiming He. En este ejemplo, dado que solo multiplicamos y no usamos <code class="docutils literal notranslate"><span class="pre">tanh</span></code>, la fórmula es <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{\text{fan-in}}}\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">10</span><span class="o">**</span><span class="mf">0.5</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(-0.0165) tensor(0.9966)
tensor(-0.0028) tensor(0.9878)
</pre></div>
</div>
<img alt="../../_images/e40eb02bec8f4645a450f7d7bb6443f5e72ff3de23a686e1418147c89cf923f1.png" src="../../_images/e40eb02bec8f4645a450f7d7bb6443f5e72ff3de23a686e1418147c89cf923f1.png" />
</div>
</div>
<p>Aunque los valores cambiaran, la desviación estándar permanece igual.</p>
</section>
<section id="normalizacion-por-lotes-batch-normalization">
<h2>Normalización por lotes (<em>batch normalization</em>)<a class="headerlink" href="#normalizacion-por-lotes-batch-normalization" title="Permalink to this heading">#</a></h2>
<p>Una técnica que cumple la misma función, pero que se utiliza más ampliamente por sus propiedades, es la llamada <a class="reference external" href="https://arxiv.org/abs/1502.03167">«normalización por lotes»</a>. En esencia, podemos prescindir de las fórmulas de Kaiming He y generalizar todavía más su solución. Para ello, normalizamos directamente la distribución de valores de cada matriz de pesos. El algoritmo luce así:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{rlr|}
\hline \text { Input: } &amp; \text { Values of } x \text { over a mini-batch: } \mathcal{B}=\left\{x_{1 \ldots m}\right\}  \\
&amp; \text { Parameters to be learned: } \gamma, \beta \\
\text { Output: } &amp; \left\{y_i=\mathrm{BN}_{\gamma, \beta}\left(x_i\right)\right\} &amp; \\
\mu_{\mathcal{B}} &amp; \leftarrow \frac{1}{m} \sum_{i=1}^m x_i &amp; / / \text { mini-batch mean } \\
\sigma_{\mathcal{B}}^2 &amp; \leftarrow \frac{1}{m} \sum_{i=1}^m\left(x_i-\mu_{\mathcal{B}}\right)^2 &amp; / / \text { mini-batch variance } \\
\widehat{x}_i &amp; \leftarrow \frac{x_i-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2+\epsilon}} &amp; \quad / / \text { normalize } \\
y_i &amp; \leftarrow \gamma \widehat{x}_i+\beta \equiv \operatorname{BN}_{\gamma, \beta}\left(x_i\right) &amp; / / \text { scale and shift }
\end{array}
\end{split}\]</div>
<p>Y aunque parezca complicado, la implementación programática es bastante sencilla:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">)</span> <span class="c1"># tabla de consulta</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">emb_dim</span><span class="o">*</span><span class="n">block_size</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.3</span> <span class="c1"># pesos ocultos</span>
<span class="c1">#d = torch.randn(h) * 0.01 # sesgos ocultos</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span> <span class="c1"># pesos de la segunda capa</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.0</span>

<span class="n">bngain</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span> <span class="c1"># gamma</span>
<span class="n">bnbias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span> <span class="c1"># beta</span>
<span class="n">bnmean_running</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>
<span class="n">bnstd_running</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>

<span class="n">parametros</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">bngain</span><span class="p">,</span> <span class="n">bnbias</span><span class="p">]</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
  <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
  <span class="c1">#minibatch («minilote»)</span>
  <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,))</span>

  <span class="c1"># propagación hacia delante</span>
  <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">Xtr</span><span class="p">[</span><span class="n">ix</span><span class="p">]]</span>
  <span class="n">pre_a</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">*</span><span class="n">block_size</span><span class="p">)</span> <span class="o">@</span> <span class="n">H</span> <span class="c1">#+ d</span>
  <span class="c1"># BatchNorm</span>
  <span class="c1"># -----</span>
  <span class="n">bnmean_i</span> <span class="o">=</span> <span class="n">pre_a</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">bnstd_i</span> <span class="o">=</span>  <span class="n">pre_a</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">pre_a</span> <span class="o">=</span> <span class="n">bngain</span> <span class="o">*</span> <span class="p">(</span><span class="n">pre_a</span> <span class="o">-</span> <span class="n">bnmean_i</span><span class="p">)</span> <span class="o">/</span> <span class="n">bnstd_i</span> <span class="o">+</span> <span class="n">bnbias</span>
  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">bnmean_running</span> <span class="o">=</span> <span class="mf">0.999</span> <span class="o">*</span> <span class="n">bnmean_running</span> <span class="o">+</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">bnmean_i</span>
    <span class="n">bnstd_running</span> <span class="o">=</span> <span class="mf">0.999</span> <span class="o">*</span> <span class="n">bnstd_running</span> <span class="o">+</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">bnstd_i</span> 
  <span class="c1"># -----</span>
  <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">pre_a</span><span class="p">)</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">U</span> <span class="o">+</span> <span class="n">b</span>
  <span class="n">perdida</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
  
  <span class="c1"># propagación hacia atrás</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">perdida</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

  <span class="c1"># actualización</span>
  <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">5000</span> <span class="k">else</span> <span class="mf">0.01</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span> <span class="c1"># decorador</span>
<span class="k">def</span> <span class="nf">perdida_split</span><span class="p">(</span><span class="n">split</span><span class="p">):</span>
  <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xtr</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">),</span>
         <span class="s1">&#39;val&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xdev</span><span class="p">,</span> <span class="n">Ydev</span><span class="p">),</span>
         <span class="s1">&#39;test&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xte</span><span class="p">,</span> <span class="n">Yte</span><span class="p">)}[</span><span class="n">split</span><span class="p">]</span>
  <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
  <span class="n">pre_a</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">*</span><span class="n">block_size</span><span class="p">)</span> <span class="o">@</span> <span class="n">H</span> <span class="o">+</span> <span class="n">d</span>
  <span class="n">pre_a</span> <span class="o">=</span> <span class="n">bngain</span> <span class="o">*</span> <span class="p">(</span><span class="n">pre_a</span> <span class="o">-</span> <span class="n">bnmean_running</span><span class="p">)</span> <span class="o">/</span> <span class="n">bnstd_running</span> <span class="o">+</span> <span class="n">bnbias</span>
  <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">pre_a</span><span class="p">)</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">U</span> <span class="o">+</span> <span class="n">b</span>
  <span class="n">perdida</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">split</span><span class="p">,</span> <span class="n">perdida</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="n">perdida_split</span><span class="p">(</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
<span class="n">perdida_split</span><span class="p">(</span><span class="s1">&#39;val&#39;</span><span class="p">)</span>
<span class="n">perdida_split</span><span class="p">(</span><span class="s1">&#39;test&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train 1.9545596837997437
val 1.9609586000442505
test 1.9600375890731812
</pre></div>
</div>
</div>
</div>
</section>
<section id="pytorch">
<h2>PyTorch<a class="headerlink" href="#pytorch" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Linear</span><span class="p">:</span>
  
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">))</span> <span class="o">/</span> <span class="n">fan_in</span><span class="o">**</span><span class="mf">0.5</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">fan_out</span><span class="p">)</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="kc">None</span>
  
  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>
  
  <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">]</span> <span class="o">+</span> <span class="p">([]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">BatchNorm1d</span><span class="p">:</span>
  
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="c1"># parameters (trained with backprop)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="c1"># buffers (trained with a running &#39;momentum update&#39;)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
  
  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># calculate the forward pass</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
      <span class="n">xmean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># batch mean</span>
      <span class="n">xvar</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># batch variance</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">xmean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span>
      <span class="n">xvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span>
    <span class="n">xhat</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">xmean</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">xvar</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span> <span class="c1"># normalize to unit variance</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">xhat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>
    <span class="c1"># update the buffers</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="n">xmean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="n">xvar</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>
  
  <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">]</span>

<span class="k">class</span> <span class="nc">Tanh</span><span class="p">:</span>
  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>
  <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">emb_dim</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># cada embedding tendrá 10 dimensiones</span>
<span class="n">block_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">h</span> <span class="o">=</span> <span class="mi">200</span> <span class="c1"># cada capa tendrá 200 unidades (hidden units)</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">fap</span><span class="p">)</span> <span class="c1">#tamaño del vocabulario</span>

<span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">)</span> <span class="c1"># tabla de consulta</span>
<span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">Linear</span><span class="p">(</span><span class="n">emb_dim</span> <span class="o">*</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">h</span><span class="p">),</span> <span class="n">Tanh</span><span class="p">(),</span>
    <span class="n">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">h</span><span class="p">),</span> <span class="n">Tanh</span><span class="p">(),</span>
    <span class="n">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">h</span><span class="p">),</span> <span class="n">Tanh</span><span class="p">(),</span>
    <span class="n">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">),</span>
<span class="p">]</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
  <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*=</span> <span class="mf">0.1</span>
  <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">Linear</span><span class="p">):</span>
      <span class="n">layer</span><span class="o">.</span><span class="n">weight</span> <span class="o">*=</span> <span class="mf">1.0</span>

<span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">layer</span><span class="o">.</span><span class="n">parameters</span><span class="p">()]</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">))</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
  <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>92924
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">perdida_i</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">ud</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">max_steps</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
  <span class="c1">#minibatch («minilote»)</span>
  <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,))</span>

  <span class="c1"># propagación hacia delante</span>
  <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">Xtr</span><span class="p">[</span><span class="n">ix</span><span class="p">]]</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">perdida</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
  
  <span class="c1"># propagación hacia atrás</span>
  <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
    <span class="n">layer</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">retain_grad</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">perdida</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

  <span class="c1"># actualización</span>
  <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">5000</span> <span class="k">else</span> <span class="mf">0.01</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>

  <span class="c1"># estadísticas</span>
  <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># print every once in a while</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s1">7d</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">max_steps</span><span class="si">:</span><span class="s1">7d</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">perdida</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="n">perdidas_i</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">perdida</span><span class="o">.</span><span class="n">log10</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">ud</span><span class="o">.</span><span class="n">append</span><span class="p">([((</span><span class="n">lr</span><span class="o">*</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">std</span><span class="p">())</span><span class="o">.</span><span class="n">log10</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      0/  10000: 3.3156
   1000/  10000: 3.2813
   2000/  10000: 3.2792
   3000/  10000: 3.3146
   4000/  10000: 3.3071
   5000/  10000: 3.2983
   6000/  10000: 3.2956
   7000/  10000: 3.2848
   8000/  10000: 3.2666
   9000/  10000: 3.3004
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">block_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
  <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">block_size</span>
  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">context</span><span class="p">])]</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <span class="n">H</span> <span class="o">+</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">U</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ix</span><span class="p">]</span>
    <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">break</span>

  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">fap</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">out</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>rulio.
maria.
homerigjovina.
aso.
lilandioleda.
jer.
valdinza.
sumenda.
ener.
neleudotgimio.
idelermovelina.
delma.
elzy.
balbertorancio.
casikufinechuio.
ber.
helola.
pino.
moriana.
alberijonelicio.
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks/alfa"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inicializacion-de-pesos-y-sesgos">Inicialización de pesos y sesgos</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#propiedades-de-la-tangente-hiperbolica">Propiedades de la tangente hiperbólica</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inicializacion-kaiming">Inicialización Kaiming</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalizacion-por-lotes-batch-normalization">Normalización por lotes (<em>batch normalization</em>)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch">PyTorch</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dante Noguez
</p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>