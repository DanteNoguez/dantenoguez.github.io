{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Elementos de PLN, o El Hacedor 2.0\n",
        "\n",
        "Siguiendo nuestra lección anterior, optimizaremos nuestro modelo de red neuronal para crear nombres. Ahora, lo haremos en el estilo del *paper* [Bengio et. al, 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)."
      ],
      "metadata": {
        "id": "mWGJJFnQhflL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "20vcSVe-RXia"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import requests\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = Path('data/')\n",
        "if not path.is_dir():\n",
        "  path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(path / 'nombres.txt', 'wb') as f:\n",
        "  request = requests.get('https://github.com/DanteNoguez/CalculusRatiocinator/raw/main/data/nombres.txt')\n",
        "  f.write(request.content)"
      ],
      "metadata": {
        "id": "2wiy1BkyVoK6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nombres = open('data/nombres.txt', 'r').read().splitlines()\n",
        "nombres[:8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_nJ3lXeWVII",
        "outputId": "2468c2a7-0423-4319-d40f-5274a404b399"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['maria', 'rosa', 'jose', 'carmen', 'ana', 'juana', 'antonio', 'elena']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(nombres)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okwXbLowXOQl",
        "outputId": "19dcde36-d3ee-47fc-9c5d-112c0f1a24cd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21029"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "V = sorted(set(''.join(nombres)))\n",
        "paf = {p:f+1 for f, p in enumerate(V)}\n",
        "paf['.'] = 0\n",
        "fap = {f:p for p,f in paf.items()}\n",
        "print(fap)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOuM5vdCXkVd",
        "outputId": "09e72f54-8d3f-4102-bdc3-fd0e92a5fc51"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Un modelo neuronal probabilístico de lenguaje\n",
        "\n",
        "Primero, comenzaremos dividiendo nuestros datos en «bloques». Por ejemplo, en nuestro modelo de bigramas, el bloque contenía un solo carácter, puesto que realizábamos la predicción a partir de una letra; pero podemos aumentar el «contexto» de nuestras predicciones para involucrar más letras al momento de predecir la siguiente. Veamos, por ejemplo, cómo luciría nuestro tratamiento de los datos si hiciéramos bloques de tres caracteres para predecir el siguiente:"
      ],
      "metadata": {
        "id": "n2O-rwNLp0YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def construir_dataset(nombres):\n",
        "  block_size = 3 # longitud del contexto\n",
        "  X, Y = [], []\n",
        "  for n in nombres:\n",
        "    #print(f'nombre: {n}')\n",
        "    contexto = [0] * block_size\n",
        "    for c in n + '.':\n",
        "      ix = paf[c]\n",
        "      X.append(contexto)\n",
        "      Y.append(ix)\n",
        "      #print(''.join(fap[i] for i in contexto), '----> ', fap[ix])\n",
        "      contexto = contexto[1:] + [ix]\n",
        "  \n",
        "  X = torch.tensor(X) # contexto\n",
        "  Y = torch.tensor(Y) # objetivo\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "9N7EnXZTZgco"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{margin}\n",
        "“The training set is a sequence $w_1 · · · w_T$ of words $w_t \\in V$, where the vocabulary $V$ is a large but finite set.”\n",
        "```"
      ],
      "metadata": {
        "id": "eH9gpAd8DT69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "construir_dataset(nombres[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-tv_YsjwIfh",
        "outputId": "99a8c5e9-444b-40be-b3d9-e5fae9f15ced"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nombre: maria\n",
            "... ---->  m\n",
            "..m ---->  a\n",
            ".ma ---->  r\n",
            "mar ---->  i\n",
            "ari ---->  a\n",
            "ria ---->  .\n",
            "nombre: rosa\n",
            "... ---->  r\n",
            "..r ---->  o\n",
            ".ro ---->  s\n",
            "ros ---->  a\n",
            "osa ---->  .\n",
            "nombre: jose\n",
            "... ---->  j\n",
            "..j ---->  o\n",
            ".jo ---->  s\n",
            "jos ---->  e\n",
            "ose ---->  .\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0,  0,  0],\n",
              "         [ 0,  0, 13],\n",
              "         [ 0, 13,  1],\n",
              "         [13,  1, 18],\n",
              "         [ 1, 18,  9],\n",
              "         [18,  9,  1],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0, 18],\n",
              "         [ 0, 18, 15],\n",
              "         [18, 15, 19],\n",
              "         [15, 19,  1],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0, 10],\n",
              "         [ 0, 10, 15],\n",
              "         [10, 15, 19],\n",
              "         [15, 19,  5]]),\n",
              " tensor([13,  1, 18,  9,  1,  0, 18, 15, 19,  1,  0, 10, 15, 19,  5,  0]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nuestra intuición detrás de esta aproximación es que el lenguaje funciona mejor con contexto: así como el sentido de un concepto se entiende mejor en contexto, también los caracteres se pueden predecir más razonablemente dado un contexto más amplio. \n",
        "\n",
        "Similar a como habíamos hecho anteriormente, construimos una matriz `X` para contener el contexto como entrada y un vector `Y` que contiene el objetivo (es decir, carácter) que debe seguir a cada respectivo contexto. Como se puede apreciar, solamente estamos construyendo `X` e `Y` con sus respectivos índices del vocabulario.\n",
        "\n",
        "Dado que solo tomamos 3 nombres como ejemplo, nuestros datos únicamente contienen 16 contextos o *inputs* y 16 objetivos o *outputs*:"
      ],
      "metadata": {
        "id": "dO_nn97fssxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = construir_dataset(nombres[:3])\n",
        "X.shape, Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnQbjfrAvkdi",
        "outputId": "9f2cfe9a-2c4d-4a2a-df9f-c5f6d682c123"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([16, 3]), torch.Size([16]))"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora estamos listos para hacer el *embedding*. Mientras que en el *paper* los datos se incrustan en una tabla de consulta de 30 dimensiones (o *features*) para un vocabulario de 17,000 palabras, nosotros —que únicamente tenemos un vocabulario de 27 caracteres— podemos aproximarnos a la incrustación con algo más pequeño, como una incrustación de dos dimensiones."
      ],
      "metadata": {
        "id": "0mwDZ-E5wpGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "C = torch.randn((27, 2)) # tabla de consulta\n",
        "C"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGlTifYhzqv7",
        "outputId": "b7a76b4d-3ca7-46b9-d54f-2eb8aa5a9ae8"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.7079,  0.2444],\n",
              "        [-0.9648,  0.9906],\n",
              "        [ 0.3772,  0.6053],\n",
              "        [ 0.5600, -0.9066],\n",
              "        [ 0.0494,  1.8393],\n",
              "        [-0.2774,  1.6655],\n",
              "        [-1.8486, -0.8595],\n",
              "        [-2.8719, -0.1518],\n",
              "        [ 0.1987,  0.3194],\n",
              "        [-1.0431, -0.1715],\n",
              "        [-0.4272,  2.0461],\n",
              "        [ 0.8884, -0.0656],\n",
              "        [-1.6604,  0.4760],\n",
              "        [ 0.1157,  1.4921],\n",
              "        [-1.7283,  0.3410],\n",
              "        [-0.9365,  0.2463],\n",
              "        [ 2.4093, -0.1046],\n",
              "        [ 0.0064,  1.6897],\n",
              "        [-0.2445, -0.8068],\n",
              "        [-0.5709,  0.2377],\n",
              "        [ 0.0886,  0.0178],\n",
              "        [ 0.8750,  0.5877],\n",
              "        [ 0.6039, -0.5445],\n",
              "        [ 0.1814, -0.7042],\n",
              "        [ 1.2580,  0.7008],\n",
              "        [ 0.1558,  0.9199],\n",
              "        [ 0.8629,  0.9445]])"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como vemos, cada *token* o elemento del volcabulario se incrustará en dos dimensiones, es decir, tendrá dos números asociados. Ahora, tomaremos un atajo que nos permitirá ser más eficientes con la codificación y la primera capa de la red neuronal. Anteriormente, habíamos hecho un *one-hot encoding* para luego pasarlo por una capa `W`; pero, bien visto, estos dos pasos pueden ser omitidos porque consiguen el mismo resultado que la incrustación en nuestra tabla `C`. \n",
        "\n",
        "Primero: la codificación *one-hot*, si fuéramos a multiplicarla por `C`, anularía todos los valores de `C` al multiplicarlos por 0 y conservaría una fila correspondiente a la de la multiplicación por 1. Ergo, podemos omitir la multiplicación y hacer una indexación para asociar directamente cada carácter con cada fila que un vector *one-hot* multiplicaría por 1. Dicho esto, podemos concebir a `C` como un equivalente de la capa `W`, puesto que consiste de valores aleatorios que asignan un número a cada carácter y luego pueden optimizarse con propagación hacia atrás.\n",
        "\n",
        "Dicho esto, la indexación (*embedding*) será bastante simple:\n",
        "\n",
        "```{margin}\n",
        "“A mapping $C$ from any element $i$ of $V$ to a real vector $C(i) \\in \\mathbb{R}^m$. It represents the distributed feature vectors associated with each word in the vocabulary. In practice, C is represented by a $\\left|V\\right| \\times m$ matrix of free parameters”.\n",
        "```"
      ],
      "metadata": {
        "id": "KXQ0eItb0cAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb = C[X] # embedding\n",
        "\n",
        "f'Segunda fila de C: {C[1].numpy()} | Tercer valor del tercer bloque incrustado (es decir, letra a): {emb[2][2].numpy()}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "AWRbIvaQAJbc",
        "outputId": "adb2f425-e0ea-4fb0-878d-e9e5c77c4b56"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Segunda fila de C: [-0.96479154  0.99060136] | Tercer valor del tercer bloque incrustado (es decir, letra a): [-0.96479154  0.99060136]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Dimensiones del embedding: ', emb.shape) \n",
        "print('Tres bloques del embedding, correspondientes a «..m», «.ma» y «mar»:', emb[1:4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wVT6iiNIale",
        "outputId": "b41d0bd0-58d2-49e0-a608-a672db6fa705"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensiones del embedding:  torch.Size([16, 3, 2])\n",
            "Tres bloques del embedding, correspondientes a «..m», «.ma» y «mar»: tensor([[[ 1.7079,  0.2444],\n",
            "         [ 1.7079,  0.2444],\n",
            "         [ 0.1157,  1.4921]],\n",
            "\n",
            "        [[ 1.7079,  0.2444],\n",
            "         [ 0.1157,  1.4921],\n",
            "         [-0.9648,  0.9906]],\n",
            "\n",
            "        [[ 0.1157,  1.4921],\n",
            "         [-0.9648,  0.9906],\n",
            "         [-0.2445, -0.8068]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, echemos un vistazo a la arquitectura que deseamos lograr:\n",
        "\n",
        "```{figure} ../../img/bengio2003.png\n",
        "---\n",
        "width: 70%\n",
        "name: bengio2003\n",
        "---\n",
        "Arquitectura neuronal $f\\left(i, w_{t-1}, \\cdots, w_{t-n+1}\\right)=g\\left(i, C\\left(w_{t-1}\\right), \\cdots, C\\left(w_{t-n+1}\\right)\\right)$ donde $g$ es la red neuronal y $C(i)$ es el $i$-ésimo vector de cada palabra. En nuestro caso, utilizamos bloques de tres letras (`contexto`, vector `X`) en lugar de palabras ($w$).\n",
        "```"
      ],
      "metadata": {
        "id": "Mvh5GMkLIWsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como vemos, tenemos casi terminado el inicio y únicamente nos falta concatenar entre sí los bloques del *embedding*, puesto que juntos atravesarán la misma capa de neuronas.\n",
        "```{margin}\n",
        "“\\[...] $x$ is the word features layer activation vector, which is the concatenation of the input word features from the matrix $C$: $x = C\\left(w_{t-1}\\right), \\left(w_{t-2}\\right) \\cdots, C\\left(w_{t-n+1}\\right)$.”\n",
        "```\n",
        "Para conseguirlo, podemos utilizar distintos métodos con PyTorch:"
      ],
      "metadata": {
        "id": "Ln__Tj3MQz1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metodo1 = torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1)\n",
        "print(f\"\"\"Ahora, en lugar de estar contenidos en bloques de tres filas: \n",
        "{emb[0]}\n",
        "Estarían contenidos en bloques de una fila (seis columnas):\n",
        "{metodo1[0]}\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llevzXqoY8kG",
        "outputId": "9fed5a70-9bfa-477c-cab8-6ac25d623b3d"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ahora, en lugar de estar contenidos en bloques de tres filas: \n",
            "tensor([[1.7079, 0.2444],\n",
            "        [1.7079, 0.2444],\n",
            "        [1.7079, 0.2444]])\n",
            "Estarían contenidos en bloques de una fila (seis columnas):\n",
            "tensor([1.7079, 0.2444, 1.7079, 0.2444, 1.7079, 0.2444])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metodo2 = torch.cat(torch.unbind(emb, 1), 1)\n",
        "f'Aunque también es equivalente: {metodo2[0]}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "NoRTuW6YaZox",
        "outputId": "80993c8d-79be-42b7-8a21-6cd056eca12f"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Aunque también es equivalente: tensor([1.7079, 0.2444, 1.7079, 0.2444, 1.7079, 0.2444])'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pero el método más eficiente[^1] y simple es `view`. Como primer argumento, colocaremos `-1` para que de esta forma PyTorch infiera el tamaño de la dimensión 0 que debería tener el tensor (sería, pues, equivalente a colocar `emb[0].shape`), y como segundo argumento `6` porque queremos que el tensor tenga las 6 columnas correspondientes a un bloque de tres *tokens*:"
      ],
      "metadata": {
        "id": "BBMagWewbQ2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb.view(-1, 6)[0], emb.view(-1, 6).shape # esta es la variable x del paper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBCWB06samzE",
        "outputId": "35081dc0-0b7e-4ee7-9af7-26587ce0221a"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1.7079, 0.2444, 1.7079, 0.2444, 1.7079, 0.2444]), torch.Size([16, 6]))"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora ya tenemos lo suficiente para definir más variables:\n",
        "\n",
        "> “Let $h$ be the number of hidden units[^2], and $m$ the number of features associated with each word. When no direct connections from word features to outputs are desired, the matrix $W$ is set to 0 . The free parameters of the model are the output biases $b$ (with $|V|$ elements), the hidden layer biases $d$ (with $h$ elements), the hidden-to-output weights $U$ (a $|V| \\times h$ matrix), the word features to output weights $W$ (a $|V| \\times(n-1) m$ matrix), the hidden layer weights $H$ (a $h \\times(n-1) m$ matrix), and the word features $C$(a $|V| \\times m$ matrix $)$: $\\theta=(b, d, W, U, H, C)$”."
      ],
      "metadata": {
        "id": "aQkp7vTli1RI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f'Número de features (m), es decir, número de componentes de cada bloque: {emb.view(-1, 6).size(dim=1)} | Elementos de |V|: {len(V)+1}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "wGjpIQCJTQHT",
        "outputId": "9879b32e-8349-406b-bf2d-3cfcb4228dfd"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Número de features (m), es decir, número de componentes de cada bloque: 6 | Elementos de |V|: 27'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Él número de parámetros ($h$) depende del problema a tratar: generalmente, a mayor cantidad de datos, es mejor mayor cantidad de parámetros. En general, cuestiones técnicas como esta dependen de la evaluación experimental que hagamos de nuestro modelo: tras pruebas con diferentes números de parámetros, podemos elegir la que más efectiva y eficiente sea. En el *paper*, por ejemplo, probaron con 50 y 100 *hidden units*. Dado que por el momento solo estamos ejemplificando con tres nombres, $h = 50$ unidades serán suficientes.\n",
        "\n",
        "Por otra parte, fijar un valor de 0 a $W$ es igual que no utilizarla, de manera que omitiremos su definición porque no la necesitamos."
      ],
      "metadata": {
        "id": "YvTldl3y1cnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h = 50\n",
        "d = torch.randn((h))\n",
        "H = torch.randn((6, h))\n",
        "\n",
        "a = torch.tanh(emb.view(-1, 6) @ H + d)\n",
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn986Q_bRgr2",
        "outputId": "e77d3163-0ad9-4245-d8d7-2892897290cb"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, la capa oculta previa al *output*, es decir, la *hidden-to-output layer* se compone de $U$ y $b$, mientras que el resultado de esta capa son —el lector lo recordará— lo que llamamos *logits*, es decir, el resultado de la última capa de la red neuronal. Estos *logits* serán convertidos en  probabilidades y, con ello, tendremos el *output* de toda la red. "
      ],
      "metadata": {
        "id": "CiZ4E0jsVs0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "U = torch.randn((h, 27))\n",
        "b = torch.randn(27)\n",
        "\n",
        "logits = a @ U + b\n",
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ctI-j3zV2K0",
        "outputId": "6d185c10-cf79-42c7-9371-5fd4501aa3d9"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, para convertir esto en probabilidades, haremos lo mismo que en la lección pasada:"
      ],
      "metadata": {
        "id": "vf6uqdGxf07U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = logits.exp()\n",
        "prob = counts / counts.sum(1, keepdims=True)\n",
        "prob[0].sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UZCsFTOf4vc",
        "outputId": "16661668-ecbe-4f9d-cdbb-5a95a17af845"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para crear nuestra función de pérdida, necesitamos seleccionar nuestros objetivos (con base en el índice que nos dio `Y`):"
      ],
      "metadata": {
        "id": "FOU_daFOf5_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prob[torch.arange(16), Y]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XesO8cnLgFsB",
        "outputId": "623ca2f4-fe33-4c57-8b77-c1785eeaf6b9"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.5711e-06, 2.9872e-04, 5.4979e-05, 3.6278e-07, 2.1988e-07, 1.4920e-06,\n",
              "        4.6494e-04, 6.1111e-05, 9.9566e-01, 4.6007e-03, 3.9694e-07, 2.1725e-07,\n",
              "        1.4721e-05, 7.4690e-03, 1.1107e-10, 8.2172e-06])"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplifiquemos: si nuestro *input* es «mar» (`emb[3]`), nuestra probabilidad debe ser alta para que el número del *embedding* que represente la letra «i» de «María» sea un *output*:"
      ],
      "metadata": {
        "id": "vAogwu906LYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\"\"En un inicio, cada letra de «mar» correspondía a su índice en el vocabulario: {X[3]}\n",
        "Cuando pasamos estos índices a una matriz para que fueran representados por dos números, obtuvimos: {emb[3]}\n",
        "Al mismo tiempo, la letra «i» fue guardada como objetivo en Y, siendo su índice: {Y[3]}\n",
        "De manera que la probabilidad de nuestra red neuronal debe ser alta para el número que representa la letra «i» en el embedding: {-prob[3, Y[3]]}\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzP5D6cn7C6-",
        "outputId": "c23cf358-ac47-465f-cd90-5f74d1b2b9c7"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "En un inicio, cada letra de «mar» correspondía a su índice en el vocabulario: tensor([13,  1, 18])\n",
            "Cuando pasamos estos índices a una matriz para que fueran representados por dos números, obtuvimos: tensor([[ 0.1157,  1.4921],\n",
            "        [-0.9648,  0.9906],\n",
            "        [-0.2445, -0.8068]])\n",
            "Al mismo tiempo, la letra «i» fue guardada como objetivo en Y, siendo su índice: 9\n",
            "De manera que la probabilidad de nuestra red neuronal debe ser alta para el número que representa la letra «i» en el embedding: -3.6277964454711764e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado que no hemos entrenado la red, la probabilidad anterior es muy baja. Para poder entrenarla, formularemos la función de pérdida mediante el promedio del logaritmo natural de las probabilidades que la red neuronal asigna a cada objetivo, y finalmente hacemos de este un número positivo al multiplicarlo por $-1$, igual que en la lección pasada:"
      ],
      "metadata": {
        "id": "8_8s5YhMgrDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perdida = -prob[torch.arange(16), Y].log().mean()\n",
        "perdida"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-V-YLiMg9Qi",
        "outputId": "1645999d-0acc-4d87-e4b6-0c83b5bf70d3"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(11.1475)"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando PyTorch, podemos simplificar todo este proceso mediante el uso de `cross_entropy`:"
      ],
      "metadata": {
        "id": "MMDFftuZiZuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "F.cross_entropy(logits, Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHcV_mJPigMl",
        "outputId": "e030d5ae-8f62-4fc7-d7c4-dc878eb28fd9"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(11.1475)"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, agrupamos los parámetros —$\\theta$— en una variable:"
      ],
      "metadata": {
        "id": "YccvWoeSutKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parametros = [b, d, U, H, C]"
      ],
      "metadata": {
        "id": "SozrJQzlu6Le"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indicamos a PyTorch que requeriremos gradientes para el entrenamiento de nuestros parámetros:"
      ],
      "metadata": {
        "id": "5sMEdaOk9Syz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for p in parametros:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "zr_TdESFvm1p"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, podemos entrenar la red:"
      ],
      "metadata": {
        "id": "jdrFZM0m9a7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "  # paso hacia delante\n",
        "  emb = C[X]\n",
        "  h = torch.tanh(emb.view(-1, 6) @ H + d)\n",
        "  logits = h @ U + b\n",
        "  perdida = F.cross_entropy(logits, Y)\n",
        "  \n",
        "  # propagación hacia atrás\n",
        "  for p in parametros:\n",
        "    p.grad = None\n",
        "  perdida.backward()\n",
        "  \n",
        "  for p in parametros:\n",
        "    p.data += -0.1 * p.grad\n",
        "\n",
        "print(perdida.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07Gi5EJpweFt",
        "outputId": "f2fb1bfc-72ef-4ab5-dc7b-8d09a98003a7"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.22844049334526062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bien, hemos conseguido un resultado decente en nuestro ejemplo. Ahora es momento de entrenar nuestra red con todos los nombres a la vez. \n",
        "\n",
        "Aunque primero, hacen falta algunos ajustes. En primer lugar, evitaremos que nuestro modelo «memorice» cada *output* correspondiente a cada *input* (a esta memorización se le llama *overfitting*), puesto que queremos nombres nuevos y originales y no una regurgitación de los que estamos utilizando. Para ello, se han desarrollado un número de técnicas en *deep learning*; pero de momento, utilizaremos una de las más elementales, y consiste en separar nuestros datos en tres partes: entrenamiento, validación y prueba. Entrenaremos nuestro modelo con una cantidad razonable de nombres (el 80 % de ellos), definimos nuestros parámetros —en rigor, hiperparámetros[^3]— con los datos de validación (10 % de nuestros nombres) y verificamos que el modelo sepa generalizar su aprendizaje con los datos destinados a las pruebas (último 10 % de nombres)."
      ],
      "metadata": {
        "id": "SrSZQ3lkEQLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(nombres)\n",
        "n1 = int(0.8*len(nombres))\n",
        "n2 = int(0.9*len(nombres))\n",
        "\n",
        "Xtr, Ytr = construir_dataset(nombres[:n1])\n",
        "Xdev, Ydev = construir_dataset(nombres[n1:n2])\n",
        "Xte, Yte = construir_dataset(nombres[n2:])\n",
        "X, Y = construir_dataset(nombres)\n",
        "\n",
        "X.shape, Xtr.shape, Xdev.shape, Xte.shape"
      ],
      "metadata": {
        "id": "N0zzO1IwdW5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "335a4e7c-7f8e-44af-86c8-ce231d0b170d"
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([165469, 3]),\n",
              " torch.Size([132468, 3]),\n",
              " torch.Size([16441, 3]),\n",
              " torch.Size([16560, 3]))"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora definimos nuestros hiperparámetros, aunque esta vez utilizaremos $h = 100$:"
      ],
      "metadata": {
        "id": "HVvkgagZHBlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "C = torch.randn(27, 6)\n",
        "emb = C[Xtr]\n",
        "h = 100\n",
        "H = torch.randn((6, h))\n",
        "d = torch.randn(h)\n",
        "\n",
        "a = torch.tanh(emb.view(-1, 6) @ H + d)\n",
        "\n",
        "U = torch.randn(h, 27)\n",
        "b = torch.randn(27)\n",
        "\n",
        "logits = a @ U + b\n",
        "\n",
        "parametros = [C, H, d, U, b]"
      ],
      "metadata": {
        "id": "I0uqOeClM0bH"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nuestro número total de parámetros resultantes es:"
      ],
      "metadata": {
        "id": "-7YQCEE9KIGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.numel() for p in parametros)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpijkcRINOUM",
        "outputId": "42bbcd96-3797-465b-e309-30843fb36148"
      },
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3589"
            ]
          },
          "metadata": {},
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por otro lado, podemos acelerar el entrenamiento de la red neuronal de la siguiente forma: en cada repetición del *loop* de aprendizaje, podemos seleccionar un «lote» (*batch*) o segmento de los datos para únicamente llevar a cabo el proceso de aprendizaje en ese mismo lote, de manera que el modelo no se entrene tomando siempre en consideración todos los datos a la vez, sino datos —en este caso, nombres— aleatorios en cada iteración. Aunque esto implique sacrificar en cierta medida el desempeño de la red neuronal, sin embargo ese sacrificio se compensa con la rapidez que podemos generar a cambio.\n",
        "\n",
        "Para crear estos lotes, podemos utilizar el siguiente código, el cual generará índices correspondientes a 32 nombres aleatorios de entre aquellos que pertenezcan a nuestros datos:"
      ],
      "metadata": {
        "id": "OyC-Cw1ILP6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ix = torch.randint(0, X.shape[0], (32,))\n",
        "f'Tres ejemplos: {ix[:3].numpy()}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "8o2LeL14VhKw",
        "outputId": "ff848129-c365-4d32-b1f4-e3c89a3a4176"
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tres ejemplos: [56193 42924 19114]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, nos falta tratar los detalles técnicos detrás de la *learning rate* («tasa de aprendizaje») del modelo. Recordemos que, cuando modificamos los parámetros en la dirección del gradiente, generalmente atenuamos al gradiente multiplicándolo por un número pequeño para así no excedernos en el ajuste, consiguiendo una pérdida cercana a 0 sin sobrepasarla.\n",
        "\n",
        "Para determinar una *learning rate* razonable, tenemos que conseguir —empíricamente— los «límites» de la misma:"
      ],
      "metadata": {
        "id": "lIJn4uOiKLIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for p in parametros:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "srB_AEmtMTQB"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIt4s5qvdAft",
        "outputId": "bbebdac8-43ac-42e8-dc3f-b2391d3497d7"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "165469"
            ]
          },
          "metadata": {},
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(10):\n",
        "  #minibatch («minilote»)\n",
        "  ix = torch.randint(0, X.shape[0], (32,))\n",
        "\n",
        "  # propagación hacia delante\n",
        "  emb = C[X[ix]]\n",
        "  a = torch.tanh(emb.view(-1, 6) @ H + d)\n",
        "  logits = a @ U + b\n",
        "  perdida = F.cross_entropy(logits, Y[ix])\n",
        "  print(perdida.item())\n",
        "  \n",
        "  # propagación hacia atrás\n",
        "  for p in parametros:\n",
        "    p.grad = None\n",
        "  perdida.backward()\n",
        "\n",
        "  # actualización\n",
        "  lr = -1 # intentaremos obtener el límite superior\n",
        "  for p in parametros:\n",
        "    p.data += lr * p.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "ur8J9v32ZGRO",
        "outputId": "fc704f41-5930-4da8-ee9e-8e218cd3ce4c"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-240-0682273b3c9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mU\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mperdida\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperdida\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3013\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3014\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (96) to match target batch_size (32)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lre = torch.linspace(-3, 0, 1000)\n",
        "lrs = 10**lre"
      ],
      "metadata": {
        "id": "BBz2E7b9R019"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lri, lossi)"
      ],
      "metadata": {
        "id": "547WVWX3SQwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10**-0.5"
      ],
      "metadata": {
        "id": "yjinSayVUnzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lri = []\n",
        "lossi = []\n",
        "stepi = []"
      ],
      "metadata": {
        "id": "cALHoO36fxQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "  #minibatch\n",
        "  ix = torch.randint(0, Xtr.shape[0], (32,))\n",
        "\n",
        "  #forward pass\n",
        "  emb = C[Xtr][ix]\n",
        "  h = torch.tanh(emb.view(-1, 30) @ W1 + b1)\n",
        "  logits = h @ W2 + b2\n",
        "  loss = F.cross_entropy(logits, Ytr[ix])\n",
        "  \n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "  #update\n",
        "  #lr = lrs[i]\n",
        "  lr = -0.031\n",
        "  for p in parameters:\n",
        "    p.data += lr * p.grad\n",
        "\n",
        "  #track stats\n",
        "  #lri.append(lre[i])\n",
        "  stepi.append(i)\n",
        "  lossi.append(loss.log10().item())\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "FZvwJ9N_KNN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(stepi, lossi)"
      ],
      "metadata": {
        "id": "GW2pHmdef7qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb = C[Xdev]\n",
        "h = torch.tanh(emb.view(-1, 30) @ W1 + b1)\n",
        "logits = h @ W2 + b2\n",
        "loss = F.cross_entropy(logits, Ydev)\n",
        "loss"
      ],
      "metadata": {
        "id": "VJcK54eTPpTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 3\n",
        "for _ in range(20):\n",
        "  out = []\n",
        "  context = [0] * block_size\n",
        "  while True:\n",
        "    emb = C[torch.tensor([context])]\n",
        "    h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
        "    logits = h @ W2 + b2\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    ix = torch.multinomial(probs, num_samples=1).item()\n",
        "    context = context[1:] + [ix]\n",
        "    out.append(ix)\n",
        "    if ix == 0:\n",
        "      break\n",
        "\n",
        "  print(''.join(itos[i] for i in out))"
      ],
      "metadata": {
        "id": "vS8h0tWHQLCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[^1]: El método `view` es eficiente porque [no requiere de nuevo espacio](http://blog.ezyang.com/2019/05/pytorch-internals/) en la memoria de nuestra computadora, sino que utiliza el mismo tensor para reacomodarlo de manera distinta. Conforme nuestros programas se vuelvan más complejos, debemos procurar eficientar al máximo nuestros recursos computacionales. Para un acercamiento más general al tema, véase el [artículo de Horace He](https://horace.io/brrr_intro.html).\n",
        "\n",
        "[^2]: En *deep learning*, el término *hidden* («oculto») se utiliza para referirse a componentes de una red neuronal que no son «visibles» ni como entradas ni como salidas de la red. Se trata de entradas o salidas que se procesan internamente por la red neuronal antes de arrojar un resultado final. La *hidden unit*, en ese sentido, se refiere a cada unidad o componente de una capa oculta (*hidden layer*) en la red. Podemos entenderla como sinónimo de «nodo» o «neurona».\n",
        "\n",
        "[^3]: Se denominan «parámetros» a los números o coeficientes que la red neuronal aprende y determina durante el entrenamiento (por ejemplo, los números de los pesos y los sesgos). Por otra parte, los «hiperparámetros» son aquellos que nosotros definimos manualmente según la naturaleza de cada problema, y que optimizamos mediante pruebas con distintos valores (por ejemplo, nuestras unidades ocultas $h$)."
      ],
      "metadata": {
        "id": "rbHyRZ9gbY3P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GOWtZ6n5GGa-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}