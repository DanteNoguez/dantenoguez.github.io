
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Elementos de procesamiento de lenguajes naturales, o El Hacedor &#8212; Dantis Elementorum</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../../_static/logo.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Elementos de procesamiento de lenguajes naturales, parte II" href="hacedor2.html" />
    <link rel="prev" title="Elementos de cálculo infinitesimal" href="calculo.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Dantis Elementorum</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Intro
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Elementorum Physicæ Mathematicæ
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="programacion.html">
   Elementos de programación
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="redes.html">
   Elementos de redes neuronales
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="calculo.html">
   Elementos de cálculo infinitesimal
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Elementos de procesamiento de lenguajes naturales, o El Hacedor
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hacedor2.html">
   Elementos de procesamiento de lenguajes naturales, parte II
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Elementorum Politicæ Œconomicæ Iurisprudentiæ
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../beta/iurisprudentiae.html">
   Elementos de leyes, o Nova methodus discendæ iurisprudentiæ
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../beta/schumpeter.html">
   Notas a Schumpeter
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Elementorum Poeticæ
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../poeticae/borges.html">
   Elementos de literatura borgesiana, o Borges y sus precursores
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../poeticae/afro.html">
   Elementos de música afroamericana
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Exagium
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../exagium/tzompantli.html">
   Breve ensayo sobre el tzompantli
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exagium/castillos.html">
   La importancia de ser olvidado
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exagium/diccionario.html">
   Diccionario fantástico de la lengua española
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exagium/escribir.html">
   Escribir ¿para qué?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exagium/coronavirus.html">
   Bitácora del confinamiento
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exagium/sue%C3%B1os.html">
   Relatos secturniales
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/DanteNoguez/Ensayos/main?urlpath=tree/ensayos/notebooks/alfa/hacedor.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/DanteNoguez/Ensayos"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/DanteNoguez/Ensayos/issues/new?title=Issue%20on%20page%20%2Fnotebooks/alfa/hacedor.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/notebooks/alfa/hacedor.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bigrama">
   Bigrama
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#red-neuronal">
   Red neuronal
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hacia-la-formulacion-de-un-modelo">
   Hacia la formulación de un modelo
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bibliografia-recomendada">
   Bibliografía recomendada
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Elementos de procesamiento de lenguajes naturales, o El Hacedor</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bigrama">
   Bigrama
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#red-neuronal">
   Red neuronal
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hacia-la-formulacion-de-un-modelo">
   Hacia la formulación de un modelo
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bibliografia-recomendada">
   Bibliografía recomendada
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="elementos-de-procesamiento-de-lenguajes-naturales-o-el-hacedor">
<h1>Elementos de procesamiento de lenguajes naturales, o El Hacedor<a class="headerlink" href="#elementos-de-procesamiento-de-lenguajes-naturales-o-el-hacedor" title="Permalink to this headline">#</a></h1>
<p>Este texto está inspirado en <a class="reference external" href="https://github.com/karpathy/makemore">makemore</a> de Andrej Karpathy. A continuación, crearemos un modelo de lenguaje basado en bigramas. Utilizando 21,209 nombres argentinos como base, nuestro modelo aprenderá patrones estadísticos para idear nuevos nombres en español.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>El conjunto de datos original se puede encontrar <a class="reference external" href="https://www.kaggle.com/datasets/akielbowicz/nombres-de-personas-fsicas-de-argentina">aquí</a>. El código que utilicé para limpiar los datos está comentado en la primera celda; el archivo generado con dicho código se encuentra <a class="reference external" href="https://github.com/DanteNoguez/CalculusRatiocinator/blob/main/data/nombres.txt">aquí</a>.</p>
</aside>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="c1">#nombres = pd.read_csv(&#39;historico-nombres.csv&#39;).iloc[0:200000]</span>
<span class="c1">#regex = &quot;[^a-z]&quot;</span>
<span class="c1">#nombres = nombres[&#39;nombre&#39;].str.lower()</span>
<span class="c1">#filtro = nombres.str.contains(&quot;[^a-z]&quot;)</span>
<span class="c1">#nombres = nombres[~filtro].astype(&#39;str&#39;)</span>

<span class="c1">#nombres.to_csv(r&#39;nombres.txt&#39;, header=None, index=None, mode=&#39;a&#39;)</span>
<span class="c1">#nombres.head(10)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>!wget https://github.com/DanteNoguez/CalculusRatiocinator/raw/main/data/nombres.txt
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">palabras</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;nombres.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">palabras</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;maria&#39;,
 &#39;rosa&#39;,
 &#39;jose&#39;,
 &#39;carmen&#39;,
 &#39;ana&#39;,
 &#39;juana&#39;,
 &#39;antonio&#39;,
 &#39;elena&#39;,
 &#39;teresa&#39;,
 &#39;angela&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">palabras</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>21029
</pre></div>
</div>
</div>
</div>
<section id="bigrama">
<h2>Bigrama<a class="headerlink" href="#bigrama" title="Permalink to this headline">#</a></h2>
<p>Primero, formaremos bigramas (pares) de caracteres por cada nombre que hay en nuestro conjunto de datos. Al final e inicio de cada nombre, agregaremos un <em>token</em> <code class="docutils literal notranslate"><span class="pre">.</span></code> para indicar el inicio y fin de dicho nombre:</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Cada <em>token</em> es una unidad indivisible de texto, aunque el diseño o especificación de <em>tokens</em> en un modelo es una decisión personal (técnica, para ser más precisos). Por ejemplo, podemos crear un conjunto de datos enfocado en palabras, de manera que «mesa» sea un <em>token</em>; pero también podemos considerar a cada letra del alfabeto —junto con el <code class="docutils literal notranslate"><span class="pre">.</span></code>— como un <em>token</em>, como estamos haciendo en nuestro caso.</p>
</aside>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">palabras</span><span class="p">[:</span><span class="mi">3</span><span class="p">]:</span> <span class="c1"># vemos los primeros tres nombres</span>
  <span class="n">cs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> 
  <span class="k">for</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cs</span><span class="p">,</span> <span class="n">cs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span> <span class="c1"># iteramos sobre cada caracter para crear bigramas</span>
    <span class="n">bigrama</span> <span class="o">=</span> <span class="p">(</span><span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">)</span>
    <span class="n">b</span><span class="p">[</span><span class="n">bigrama</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">bigrama</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1"># hacemos un conteo de bigramas</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>. m
m a
a r
r i
i a
a .
. r
r o
o s
s a
a .
. j
j o
o s
s e
e .
</pre></div>
</div>
</div>
</div>
<p>El conteo de bigramas luce así:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">b</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{(&#39;.&#39;, &#39;m&#39;): 1,
 (&#39;m&#39;, &#39;a&#39;): 1,
 (&#39;a&#39;, &#39;r&#39;): 1,
 (&#39;r&#39;, &#39;i&#39;): 1,
 (&#39;i&#39;, &#39;a&#39;): 1,
 (&#39;a&#39;, &#39;.&#39;): 2,
 (&#39;.&#39;, &#39;r&#39;): 1,
 (&#39;r&#39;, &#39;o&#39;): 1,
 (&#39;o&#39;, &#39;s&#39;): 2,
 (&#39;s&#39;, &#39;a&#39;): 1,
 (&#39;.&#39;, &#39;j&#39;): 1,
 (&#39;j&#39;, &#39;o&#39;): 1,
 (&#39;s&#39;, &#39;e&#39;): 1,
 (&#39;e&#39;, &#39;.&#39;): 1}
</pre></div>
</div>
</div>
</div>
<p>Ahora, crearemos una lista de caracteres únicos (nuestro vocabulario) para luego asignarles un índice en un diccionario de Python. A este proceso de mapear o relacionar cada letra de nuestro vocabulario con un número se le denomina «incrustación» (<em>embedding</em>), mientras que el diccionario de Python resultante es una «tabla de consulta» (<em>lookup table</em>), debido a que en ella podemos buscar la letra que corresponde a un número y viceversa.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">caracs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">palabras</span><span class="p">))))</span> <span class="c1"># lista de caracteres únicos (tokens)</span>

<span class="n">paf</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="p">:</span><span class="n">f</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">f</span><span class="p">,</span><span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">caracs</span><span class="p">)}</span> <span class="c1"># mapeamos letras a números de principio a fin</span>
<span class="n">paf</span><span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># agregamos nuestro token «.»</span>
<span class="n">fap</span> <span class="o">=</span> <span class="p">{</span><span class="n">f</span><span class="p">:</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span><span class="p">,</span><span class="n">f</span> <span class="ow">in</span> <span class="n">paf</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span> <span class="c1"># invertimos el orden para que sea apropiado</span>
<span class="n">fap</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{1: &#39;a&#39;,
 2: &#39;b&#39;,
 3: &#39;c&#39;,
 4: &#39;d&#39;,
 5: &#39;e&#39;,
 6: &#39;f&#39;,
 7: &#39;g&#39;,
 8: &#39;h&#39;,
 9: &#39;i&#39;,
 10: &#39;j&#39;,
 11: &#39;k&#39;,
 12: &#39;l&#39;,
 13: &#39;m&#39;,
 14: &#39;n&#39;,
 15: &#39;o&#39;,
 16: &#39;p&#39;,
 17: &#39;q&#39;,
 18: &#39;r&#39;,
 19: &#39;s&#39;,
 20: &#39;t&#39;,
 21: &#39;u&#39;,
 22: &#39;v&#39;,
 23: &#39;w&#39;,
 24: &#39;x&#39;,
 25: &#39;y&#39;,
 26: &#39;z&#39;,
 0: &#39;.&#39;}
</pre></div>
</div>
</div>
</div>
<p>Ahora, construiremos una matriz —vía PyTorch— con el conteo de todos los bigramas de nuestro conjunto de datos. Con esta matriz, podremos familiarizarnos más visualmente con lo que hemos estado preparando. Las dimensiones de la matriz serán 27x27 porque tenemos 27 elementos en nuestro vocabulario y queremos emparejarlos (hacer bigramas) con cada uno de los otros elementos del mismo:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">N</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">27</span><span class="p">,</span><span class="mi">27</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">palabras</span><span class="p">:</span>
  <span class="n">cs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cs</span><span class="p">,</span> <span class="n">cs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c2</span><span class="p">]</span>
    <span class="n">N</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span> <span class="n">ix2</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">27</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">27</span><span class="p">):</span>
    <span class="n">cts</span> <span class="o">=</span> <span class="n">fap</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">fap</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">cts</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">N</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/hacedor_16_0.png" src="../../_images/hacedor_16_0.png" />
</div>
</div>
<p>Hemos contado la ocurrencia de cada bigrama en el documento de nombres. Ahora, podemos utilizar este conteo como una distribución de probabilidades acerca de cuál letra debe ser consecutiva con otra. Ejemplifiquemos con una fila:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([   0., 2611.,  909., 1465., 1038., 2205.,  941.,  924.,  668.,  726.,
         522.,   88., 1230., 1228.,  913.,  584.,  774.,   41., 1014., 1248.,
         578.,  154.,  548.,  169.,    0.,  218.,  233.])
</pre></div>
</div>
</div>
</div>
<p>Obtendremos las probabilidades de cada valor al dividir cada uno por la sumatoria de los demás. Con este truco, todos los valores sumados entre sí nos darán 1:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">N</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">p</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.0000, 0.1242, 0.0432, 0.0697, 0.0494, 0.1049, 0.0447, 0.0439, 0.0318,
        0.0345, 0.0248, 0.0042, 0.0585, 0.0584, 0.0434, 0.0278, 0.0368, 0.0019,
        0.0482, 0.0593, 0.0275, 0.0073, 0.0261, 0.0080, 0.0000, 0.0104, 0.0111])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">p</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1.0000)
</pre></div>
</div>
</div>
</div>
<p>Ahora utilizaremos <code class="docutils literal notranslate"><span class="pre">torch.multinomial</span></code> para generar números enteros con base en las probabilidades de la distribución que creamos. Primero veamos un ejemplo:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="c1">#creamos tres valores aleatorios</span>
<span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="c1"># ahora, creamos una distribución de probabilidades con base en ellos</span>
<span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.7762, 0.0321, 0.0691])
tensor([0.8846, 0.0366, 0.0787])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># ahora tomamos muestras de números enteros con base en la distribución</span>
<span class="c1"># Notemos que los números generados reflejan la distribución de probabilidades anteriores</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0, 0, 0, 0, 2, 0, 0, 0, 0, 0])
</pre></div>
</div>
</div>
</div>
<p>Podemos ejemplificar lo mismo con la primera fila de nuestra matriz:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">N</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([19,  5, 14,  4, 13])
</pre></div>
</div>
</div>
</div>
<p>Pero el resultado obtenido es el índice. Utilicemos nuestra tabla de consulta para obtener la letra correspondiente:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">ejemplo</span> <span class="o">=</span> <span class="n">fap</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
<span class="n">ejemplo</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;g&#39;
</pre></div>
</div>
</div>
</div>
<p>Ahora haremos lo mismo con todos los bigramas:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">P</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="c1"># agregamos 1 al conteo para que el logaritmo no tenga problemas eventualmente (smoothing)</span>
<span class="n">P</span> <span class="o">/=</span> <span class="n">P</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># el 1 indica que la sumatoria se hace en la dimensión 1 (i. e., las columnas colapsan para sumarse)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">P</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">P</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor(1.0000), torch.Size([27, 27]))
</pre></div>
</div>
</div>
</div>
<p>Las probabilidades de nuestra primera fila lucen así:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">P</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([4.7492e-05, 1.2405e-01, 4.3218e-02, 6.9624e-02, 4.9345e-02, 1.0477e-01,
        4.4738e-02, 4.3930e-02, 3.1772e-02, 3.4527e-02, 2.4839e-02, 4.2268e-03,
        5.8463e-02, 5.8368e-02, 4.3408e-02, 2.7783e-02, 3.6807e-02, 1.9947e-03,
        4.8205e-02, 5.9318e-02, 2.7498e-02, 7.3613e-03, 2.6073e-02, 8.0737e-03,
        4.7492e-05, 1.0401e-02, 1.1113e-02])
</pre></div>
</div>
</div>
</div>
<p>Ahora que ya tenemos una probabilidad asignada a cada bigrama, podemos comenzar a predecir el carácter que debe acompañar a su precedente con base en nuestra matriz de probabilidades. Experimentemos con cinco palabras:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
  <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">ix</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fap</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">break</span>
    
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>macito.
esaranartens.
milioredia.
hinitobal.
rinoreria.
</pre></div>
</div>
</div>
</div>
<p>Aunque quizá no elijamos ninguno de estos nombres para uso personal, podemos ver que el modelo funciona y ha generado palabras que de alguna forma reflejan la estructura del español.</p>
<p>También podemos observar las probabilidades asignadas a cada bigrama:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">palabras</span><span class="p">[:</span><span class="mi">3</span><span class="p">]:</span>
  <span class="n">cs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cs</span><span class="p">,</span> <span class="n">cs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c2</span><span class="p">]</span> 
    <span class="n">prob</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span> <span class="n">ix2</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">c1</span><span class="si">}{</span><span class="n">c2</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">prob</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.m: 0.0584
ma: 0.3492
ar: 0.0863
ri: 0.2349
ia: 0.1907
a.: 0.4568
.r: 0.0482
ro: 0.1212
os: 0.0515
sa: 0.1840
a.: 0.4568
.j: 0.0248
jo: 0.2045
os: 0.0515
se: 0.1236
e.: 0.0674
</pre></div>
</div>
</div>
</div>
<p>Dado que altas probabilidades en nuestros bigramas indican buen «aprendizaje», en el sentido de que nuestro modelo no es completamente aleatorio, sino que concede importancia a bigramas apropiadamente, entonces podemos medir la «precisión» o capacidad de nuestro modelo mediante la función de verosimilitud (<em>likelihood</em>), que es el resultado de multiplicar todas las probabilidades entre sí. Si el número es alto, eso indicaría que nuestro modelo funciona bien; si es bajo, eso indicaría que no tiene suficiente información para predecir caracteres.</p>
<p>Por conveniencia, esta estimación utiliza el logaritmo natural de las probabilidades: sumar los logaritmos de las probabilidades es equivalente a multiplicar las probabilidades (es decir, podemos emplear cualquiera de las dos formas para estimar la verosimilitud). Esto es particularmente útil porque nuestras probabilidades están dadas en números decimales, de manera que multiplicarlas entre sí nos daría un número pequeño y poco intuitivo.</p>
<p>El logaritmo natural de una serie de números presenta como valor máximo al 0, pero como valor mínimo al infinito:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log
  &quot;&quot;&quot;Entry point for launching an IPython kernel.
</pre></div>
</div>
<img alt="../../_images/hacedor_40_1.png" src="../../_images/hacedor_40_1.png" />
</div>
</div>
<p>Pero, dado que quisiéramos números positivos para hacerlo más intuitivo, podemos volver positivo este número al multiplicarlo por -1:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logprob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
<span class="n">nlog</span> <span class="o">=</span> <span class="o">-</span><span class="n">logprob</span>
<span class="sa">f</span><span class="s1">&#39;Logaritmo natural de la probabilidad: </span><span class="si">{</span><span class="n">logprob</span><span class="si">}</span><span class="s1"> | Logaritmo natural negativo: </span><span class="si">{</span><span class="n">nlog</span><span class="si">}</span><span class="s1">&#39;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Logaritmo natural de la probabilidad: -2.6970839500427246 | Logaritmo natural negativo: 2.6970839500427246&#39;
</pre></div>
</div>
</div>
</div>
<p>Y el logaritmo negativo de la verosimilitud (<em>negative log likelihood</em>) es la suma de todos los logaritmos negativos. Nuestra función de pérdida<a class="footnote-reference brackets" href="#id4" id="id1">1</a> entonces podría ser el logaritmo negativo de la verosimilitud (<code class="docutils literal notranslate"><span class="pre">nll</span></code>), normalizada para obtener el promedio. Mientras esta función de pérdida sea menor, nuestro modelo será mejor:</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Matemáticamente, podemos formular esto como <span class="math notranslate nohighlight">\(-\log\left(p(X\mid\boldsymbol{\theta})\right) = -\log(p(x_1\mid\boldsymbol{\theta})) - \log(p(x_2\mid\boldsymbol{\theta})) \cdots - \log(p(x_n\mid\boldsymbol{\theta})) = -\sum_i \log(p(x_i \mid \theta)).\)</span> Para saber más, véase <a class="reference external" href="https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html">este capítulo</a>.</p>
</aside>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">log_likelihood</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">n</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">palabras</span><span class="p">:</span>
  <span class="n">cs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cs</span><span class="p">,</span> <span class="n">cs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c2</span><span class="p">]</span>
    <span class="n">prob</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span><span class="n">ix2</span><span class="p">]</span>
    <span class="n">logprob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
    <span class="n">log_likelihood</span> <span class="o">+=</span> <span class="n">logprob</span>
    <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">nll</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_likelihood</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Logaritmo negativo de verosimilitud: </span><span class="si">{</span><span class="n">nll</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Logaritmo negativo de verosimilitud promedio: </span><span class="si">{</span><span class="n">nll</span><span class="o">/</span><span class="n">n</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Logaritmo negativo de verosimilitud: 375152.34375
Logaritmo negativo de verosimilitud promedio: 2.2672061920166016
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">log_likelihood</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">n</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="k">for</span> <span class="n">bi</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;dante&#39;</span><span class="p">]:</span>
  <span class="n">cs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">bi</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cs</span><span class="p">,</span> <span class="n">cs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c2</span><span class="p">]</span>
    <span class="n">prob</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span><span class="n">ix2</span><span class="p">]</span>
    <span class="n">logprob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
    <span class="n">log_likelihood</span> <span class="o">+=</span> <span class="n">logprob</span>
    <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">c1</span><span class="si">}{</span><span class="n">c2</span><span class="si">}</span><span class="s1"> | prob: </span><span class="si">{</span><span class="n">prob</span><span class="si">:</span><span class="s1">.7f</span><span class="si">}</span><span class="s1"> | logaritmo de la verosimilitud: </span><span class="si">{</span><span class="n">log_likelihood</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">nll</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_likelihood</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;logaritmo negativo de la verosimilitud: </span><span class="si">{</span><span class="n">nll</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;promedio del logaritmo negativo: </span><span class="si">{</span><span class="n">nll</span><span class="o">/</span><span class="n">n</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.d | prob: 0.0493446 | logaritmo de la verosimilitud: -3.0089
da | prob: 0.3054336 | logaritmo de la verosimilitud: -4.1949
an | prob: 0.1229762 | logaritmo de la verosimilitud: -6.2907
nt | prob: 0.0515025 | logaritmo de la verosimilitud: -9.2568
te | prob: 0.1527994 | logaritmo de la verosimilitud: -11.1355
e. | prob: 0.0674018 | logaritmo de la verosimilitud: -13.8326
logaritmo negativo de la verosimilitud: 13.832551956176758
promedio del logaritmo negativo: 2.3054254055023193
</pre></div>
</div>
</div>
</div>
</section>
<section id="red-neuronal">
<h2>Red neuronal<a class="headerlink" href="#red-neuronal" title="Permalink to this headline">#</a></h2>
<p>Ahora que tenemos una función de pérdida, podemos adaptar nuestro modelo a una red neuronal y optimizarlo. Crearemos bigramas de la misma manera, pero ahora crearemos un vector <span class="math notranslate nohighlight">\(x\)</span> con el primer elemento del bigrama y otro <span class="math notranslate nohighlight">\(y\)</span> con el segundo. Las <span class="math notranslate nohighlight">\(x\)</span> serán entonces nuestras entradas y las <span class="math notranslate nohighlight">\(y\)</span> nuestros objetivos:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Juntaremos los bigramas para el set de entrenamiento (inputs x, objetivos y)</span>
<span class="c1"># Primero un ejemplo:</span>

<span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">palabras</span><span class="p">[:</span><span class="mi">1</span><span class="p">]:</span>
  <span class="n">cs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cs</span><span class="p">,</span> <span class="n">cs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c2</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">)</span>
    <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix1</span><span class="p">)</span>
    <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix2</span><span class="p">)</span>

<span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
<span class="n">xs</span><span class="p">,</span> <span class="n">ys</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>. m
m a
a r
r i
i a
a .
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([ 0, 13,  1, 18,  9,  1]), tensor([13,  1, 18,  9,  1,  0]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Ahora todas las palabras</span>

<span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">palabras</span><span class="p">:</span>
  <span class="n">cs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cs</span><span class="p">,</span> <span class="n">cs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">ix1</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c1</span><span class="p">]</span>
    <span class="n">ix2</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c2</span><span class="p">]</span>
    <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix1</span><span class="p">)</span>
    <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix2</span><span class="p">)</span>

<span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span> <span class="c1"># Pasamos cada bigrama a tensores x (inputs) e y (predicción deseada)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
<span class="n">xs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 0, 13,  1,  ..., 12, 12,  1])
</pre></div>
</div>
</div>
</div>
<p>Para pasar esta información a una red neuronal, primero la codificaremos (haremos un <em>encoding</em>) en vectores vía <em>one-hot encoding</em>, ya que este formato es más conveniente para una red neuronal. Esto significa que nuestros vectores tendrán 27 elementos, y todos serán de valor 0 salvo aquel que ocupe el lugar del carácter correspondiente, el cual será 1.</p>
<p>Visualicemos, por ejemplo, el vector correspondiente a la letra «a», que se encuentra en la posición 1 de nuestro vocabulario (el punto <code class="docutils literal notranslate"><span class="pre">.</span></code> ocupa la posición 0):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="c1"># Primero veamos un ejemplo:</span>
<span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">6</span><span class="p">],</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">27</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">xenc</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])
</pre></div>
</div>
</div>
</div>
<p>Podemos crear una visualización más gráfica de 6 vectores codificados. Como digo, la posición del 1 en cada vector indica el índice de la letra a la que corresponde:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">xenc</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.image.AxesImage at 0x7f7b02839610&gt;
</pre></div>
</div>
<img alt="../../_images/hacedor_52_1.png" src="../../_images/hacedor_52_1.png" />
</div>
</div>
<p>Ahora haremos lo mismo con todos los datos:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">27</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">xenc</span><span class="p">,</span> <span class="n">xenc</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[1., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 1., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 1., 0.,  ..., 0., 0., 0.]]), torch.Size([165469, 27]))
</pre></div>
</div>
</div>
</div>
<p>Ahora crearemos una capa de neuronas (<em>i. e.,</em> una matriz de pesos, o sea, una <em>linear layer</em>)<a class="footnote-reference brackets" href="#id5" id="id2">2</a>, asignando pesos aleatorios a nuestro modelo para que se multipliquen con las entradas y se optimicen mediante la propagación hacia atrás. Más adelante explicaremos cómo eligiremos las dimensiones de la matriz de pesos.</p>
<p>Primero, procuremos entender cómo funcionará la multiplicación de nuestros vectores con la matriz de pesos. Ejemplifiquemos con los primeros tres vectores (o sea, los primeros tres caracteres) de nuestra incrustación:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">27</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">xenc</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([3, 27]), torch.Size([27, 4]))
</pre></div>
</div>
</div>
</div>
<p>Nuestra matriz de pesos luce así:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">w</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 1.6804, -0.3333,  0.1523,  1.0912],
        [ 0.0711, -1.5924,  0.6650, -1.9040],
        [ 0.2606,  0.2247, -0.6540, -0.6477],
        [-0.4081, -2.2263, -0.6014, -1.3560],
        [ 1.3712,  0.4519,  1.1165,  0.4909],
        [ 0.5392, -0.9536, -0.5489,  0.5621],
        [-0.1191,  1.0517, -0.5388,  0.0509],
        [-0.2328,  0.5691, -0.1776, -0.8785],
        [-0.9944,  0.1690,  0.4808, -0.9270],
        [-1.9163, -0.4442, -0.8332,  0.2094],
        [-1.5595,  0.3131,  0.3176, -1.0424],
        [-0.8103,  0.0612, -0.3940, -0.6969],
        [ 1.6637,  0.1493,  0.2939,  0.4968],
        [ 0.8579, -0.4684, -0.5580, -0.5566],
        [-2.3027, -0.3928,  0.9376,  0.2877],
        [ 0.1478, -0.5560,  0.2106,  1.4563],
        [ 0.8955,  0.3372, -2.2538,  1.4221],
        [ 1.2976, -0.4296, -0.0524, -1.1490],
        [ 1.8105,  1.5439,  1.2894,  1.5108],
        [-0.9403, -1.0278, -1.1975, -1.4744],
        [-1.0490, -0.5257,  0.0466,  0.1303],
        [ 0.9318,  0.4428,  0.6996, -1.0788],
        [-1.5708, -0.7023, -1.2703, -0.9609],
        [ 0.3845,  1.9697, -0.0413,  0.8551],
        [-0.2153,  1.3822,  0.0026, -0.8545],
        [ 0.6023, -2.3813, -1.3161, -1.5124],
        [ 0.3529,  0.2896, -0.4342,  1.6535]])
</pre></div>
</div>
</div>
</div>
<p>Nuestra matriz de vectores:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xenc</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.]])
</pre></div>
</div>
</div>
</div>
<p>Si multiplicamos ambas, obtenemos:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ejemplo</span> <span class="o">=</span> <span class="n">xenc</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span> <span class="o">@</span> <span class="n">w</span>
<span class="n">ejemplo</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 1.6804, -0.3333,  0.1523,  1.0912],
        [ 0.8579, -0.4684, -0.5580, -0.5566],
        [ 0.0711, -1.5924,  0.6650, -1.9040]])
</pre></div>
</div>
</div>
</div>
<p>El resultado de la multiplicación es una matriz con dimensiones 3x4. Para entender cómo se generó esta matriz, podemos tomar el primer vector de <code class="docutils literal notranslate"><span class="pre">xenc</span></code> y multiplicar cada uno de sus elementos por la primera columna de <code class="docutils literal notranslate"><span class="pre">w</span></code>. El primer vector luce así:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xenc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])
</pre></div>
</div>
</div>
</div>
<p>De manera que al multiplicarlo por la primera columna, elemento por elemento (es decir, realizando una multiplicación Hadamard, denotada comúnmente por el signo <span class="math notranslate nohighlight">\(\odot\)</span>), obtenemos:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xenc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">w</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([1.6804, 0.0000, 0.0000, -0.0000, 0.0000, 0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, 0.0000, 0.0000, -0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, -0.0000, -0.0000, 0.0000, -0.0000, 0.0000, -0.0000, 0.0000, 0.0000])
</pre></div>
</div>
</div>
</div>
<p>Y la sumatoria de este vector claramente resulta:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">xenc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">w</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1.6804)
</pre></div>
</div>
</div>
</div>
<p>Que podemos observar en el primer valor de nuestra multiplicación de <code class="docutils literal notranslate"><span class="pre">xenc</span></code> con <code class="docutils literal notranslate"><span class="pre">w</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ejemplo</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 1.6804, -0.3333,  0.1523,  1.0912],
        [ 0.8579, -0.4684, -0.5580, -0.5566],
        [ 0.0711, -1.5924,  0.6650, -1.9040]])
</pre></div>
</div>
</div>
</div>
<p>Exactamente lo mismo, aunque de manera más eficiente, sucede cuando multiplicamos ambas matrices. En síntesis: al multiplicar nuestra matriz <code class="docutils literal notranslate"><span class="pre">w</span></code> por la matriz <code class="docutils literal notranslate"><span class="pre">xenc</span></code>, cada columna de pesos evalúa cada vector de <code class="docutils literal notranslate"><span class="pre">xenc</span></code>. Es decir, obtenemos una matriz de dimensiones 3x4 donde cada fila corresponde a cada vector (<em>i. e.</em>, cada carácter), pero esta fila tiene 4 valores correspondientes a la evaluación del vector por cada una de las columnas de la matriz <code class="docutils literal notranslate"><span class="pre">w</span></code>.</p>
</section>
<section id="hacia-la-formulacion-de-un-modelo">
<h2>Hacia la formulación de un modelo<a class="headerlink" href="#hacia-la-formulacion-de-un-modelo" title="Permalink to this headline">#</a></h2>
<p>Antes de programar nuestra red neuronal, detengámonos a entender lo que estamos haciendo: en primer lugar, podemos conceptualizar a una red neuronal como una función: nosotros esperamos que nos proporcione un resultado con base en las entradas que le suministremos:</p>
<img src='https://miro.medium.com/max/640/1*sPg-0hha7o3iNPjY4n-vow.jpeg' width=400 class='center'>
<p>Lo característico de esta función es que podemos entrenarla para que se configure a sí misma, es decir, la función encontrará (o «aprenderá») los parámetros necesarios para transformar las entradas que le proporcionemos en las salidas que queremos. En este caso particular, queremos que «transforme» una letra de entrada en otra de salida, y que realice este proceso hasta conseguir un nombre. La transformación que procuraremos a continuación será lineal y se realizará mediante la matriz de pesos <code class="docutils literal notranslate"><span class="pre">w</span></code>.</p>
<p>Antes, recapitulemos: hemos separado cada bigrama de nuestro conjunto de datos en tensores <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> e <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>. El tensor <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> contiene las entradas, es decir, el primer carácter de cada bigrama que creamos por cada nombre que tenemos. El tensor <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> contiene el segundo carácter de cada uno de los bigramas. Utilizaremos <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> para entrenar al modelo e indicarle cuál carácter debe suceder a cualquier carácter dado de <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Por ejemplo, si mi carácter de entrada a la red neuronal es «a», mi modelo podrá aprender que existen altas probabilidades de que esté acompañada por la letra «n»; después, partirá de «n» para generar el siguiente carácter y así sucesivamente hasta generar un nombre.</p>
<p>Pero para poder introducir nuestras letras en una red neuronal, debemos transformarlas en números con los que pueda operar. Para ello, codificamos nuestro tensor <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> vía vectores <em>one-hot</em> que, concatenados, constituyen la matriz <code class="docutils literal notranslate"><span class="pre">xenc</span></code>. A esta codificación —que equivale a <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> pero bajo una representación numérica basada en el índice de nuestra tabla de consulta— la multiplicamos por <code class="docutils literal notranslate"><span class="pre">w</span></code>, una capa lineal que evaluará nuestras entradas <code class="docutils literal notranslate"><span class="pre">xenc</span></code> por cada una de las neuronas que tenga.</p>
<p>Entonces, el resultado de esta multiplicación de matrices nos da un conteo de todos los caracteres de <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> —codificados en vectores—, evaluados por cada neurona de la capa lineal <code class="docutils literal notranslate"><span class="pre">w</span></code>. En ese sentido, este resultado es equivalente a la matriz <code class="docutils literal notranslate"><span class="pre">N</span></code> que graficamos anteriormente, aunque con un grado de complejidad mayor debido a las transformaciones numéricas que hemos realizado.</p>
<p>En realidad, lo que queremos hacer a continuación es entrenar nuestra red neuronal para que, con base en cada vector de entrada (de la matriz <code class="docutils literal notranslate"><span class="pre">xenc</span></code>), ese mismo vector sea transformado (mediante la multiplicación por los pesos) en probabilidades correspondientes a cada <em>token</em> que debería acompañarlo.</p>
<p>Manos a la obra: primero, crearemos una matriz <code class="docutils literal notranslate"><span class="pre">W</span></code> de dimensiones 27x27: necesitamos 27 filas para nuestros 27 <em>tokens</em> del vocabulario, y necesitamos 27 evaluaciones (columnas) para cada <em>token</em>. Estas evaluaciones tendrán que especificarnos las probabilidades asignadas a cada <em>token</em> de acompañar al <em>token</em> inicial evaluado.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">27</span><span class="p">,</span> <span class="mi">27</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># Creamos weights aleatorios</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span> <span class="c1">#multiplicamos valores de xenc por W para obtener log-counts</span>
<span class="n">logits</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([165469, 27])
</pre></div>
</div>
</div>
</div>
<p>Por lo pronto, nuestros valores son aleatorios y no han sido entrenados. Ahora, dado que nuestros <code class="docutils literal notranslate"><span class="pre">logits</span></code><a class="footnote-reference brackets" href="#id6" id="id3">3</a> tienen valores pequeños, negativos y positivos, queremos transformarlos para que puedan reflejar mejor la naturaleza de un «conteo» y nos faciliten su conversión en probabilidades. Para ello, únicamente necesitamos exponenciarlos, puesto que los números negativos terminarán en un rango del 0 al 1, y los positivos se convertirán en números mayores a 1. Visualicemos la función exponencial y después apliquémosla a nuestra matriz:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/hacedor_75_0.png" src="../../_images/hacedor_75_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">counts</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="c1"># exponenciamos para obtener valores mayores a 0, equivalentes a matriz N</span>
</pre></div>
</div>
</div>
</div>
<p>Ahora, convertiremos nuestros conteos en probabilidades, dividiéndolos entre la sumatoria de todos los elementos de su fila correspondiente:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># normalizar los counts para obtener probabilidades</span>
<span class="n">probs</span> <span class="c1"># estos últimos dos pasos son equivalentes a la función softmax</span>
<span class="n">probs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([165469, 27])
</pre></div>
</div>
</div>
</div>
<p>Ahora, nuestra primera fila contiene probabilidades que lucen así:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.0096, 0.0704, 0.0243, 0.0560, 0.0687, 0.0090, 0.0270, 0.0457, 0.0506,
        0.0017, 0.0289, 0.0205, 0.0080, 0.0471, 0.0213, 0.0887, 0.0838, 0.0318,
        0.0142, 0.0524, 0.0047, 0.0464, 0.0985, 0.0197, 0.0227, 0.0201, 0.0283],
       grad_fn=&lt;SelectBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Antes de continuar, podemos visualizar el arreglo de información que tenemos. Tomemos como base nuestro primer nombre («María», que ha quedado ajustado a «.maria.»):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nlls</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">ys</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-----------&#39;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Bigrama ejemplo </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">fap</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="si">}{</span><span class="n">fap</span><span class="p">[</span><span class="n">y</span><span class="p">]</span><span class="si">}</span><span class="s1">, índices </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Input: </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Probabilidades de cada output calculadas por la red neuronal: </span><span class="si">{</span><span class="n">probs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Output correcto: </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">probs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Probabilidad asignada por la red neuronal al carácter correcto: </span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="n">logp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Logaritmo de la verosimilitud&#39;</span><span class="p">,</span> <span class="n">logp</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
  <span class="n">nll</span> <span class="o">=</span> <span class="o">-</span><span class="n">logp</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Logaritmo negativo de la verosimilitud:&#39;</span><span class="p">,</span> <span class="n">nll</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
  <span class="n">nlls</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">nll</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;----------&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Promedio de la nll, i. e. pérdida total = </span><span class="si">{</span><span class="n">nlls</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-----------
Bigrama ejemplo 1: .m, índices 0,13
Input: 0
Probabilidades de cada output calculadas por la red neuronal: tensor([0.0096, 0.0704, 0.0243, 0.0560, 0.0687, 0.0090, 0.0270, 0.0457, 0.0506,
        0.0017, 0.0289, 0.0205, 0.0080, 0.0471, 0.0213, 0.0887, 0.0838, 0.0318,
        0.0142, 0.0524, 0.0047, 0.0464, 0.0985, 0.0197, 0.0227, 0.0201, 0.0283],
       grad_fn=&lt;SelectBackward0&gt;)
Output correcto: 13
Probabilidad asignada por la red neuronal al carácter correcto: 0.04705430194735527
Logaritmo de la verosimilitud -3.056452989578247
Logaritmo negativo de la verosimilitud: 3.056452989578247
-----------
Bigrama ejemplo 2: ma, índices 13,1
Input: 13
Probabilidades de cada output calculadas por la red neuronal: tensor([0.0171, 0.0113, 0.0380, 0.0324, 0.0241, 0.0180, 0.0223, 0.0269, 0.0354,
        0.0124, 0.0092, 0.0323, 0.0372, 0.0657, 0.1751, 0.0092, 0.0080, 0.1024,
        0.0069, 0.0092, 0.0266, 0.0315, 0.0494, 0.0146, 0.1493, 0.0156, 0.0198],
       grad_fn=&lt;SelectBackward0&gt;)
Output correcto: 1
Probabilidad asignada por la red neuronal al carácter correcto: 0.011345318518579006
Logaritmo de la verosimilitud -4.478950023651123
Logaritmo negativo de la verosimilitud: 4.478950023651123
-----------
Bigrama ejemplo 3: ar, índices 1,18
Input: 1
Probabilidades de cada output calculadas por la red neuronal: tensor([0.0121, 0.0177, 0.0048, 0.1065, 0.0027, 0.0480, 0.0031, 0.0411, 0.0335,
        0.0278, 0.1691, 0.0037, 0.0235, 0.0201, 0.0221, 0.0541, 0.0329, 0.0447,
        0.0453, 0.0087, 0.0131, 0.0272, 0.0486, 0.0452, 0.0956, 0.0360, 0.0130],
       grad_fn=&lt;SelectBackward0&gt;)
Output correcto: 18
Probabilidad asignada por la red neuronal al carácter correcto: 0.04529079794883728
Logaritmo de la verosimilitud -3.094651460647583
Logaritmo negativo de la verosimilitud: 3.094651460647583
-----------
Bigrama ejemplo 4: ri, índices 18,9
Input: 18
Probabilidades de cada output calculadas por la red neuronal: tensor([0.1148, 0.0210, 0.0277, 0.0906, 0.0275, 0.0093, 0.0087, 0.0190, 0.0039,
        0.0451, 0.0309, 0.0589, 0.1442, 0.0375, 0.0079, 0.0652, 0.0272, 0.0082,
        0.0084, 0.0355, 0.0327, 0.0257, 0.0086, 0.0225, 0.0264, 0.0722, 0.0203],
       grad_fn=&lt;SelectBackward0&gt;)
Output correcto: 9
Probabilidad asignada por la red neuronal al carácter correcto: 0.04506437107920647
Logaritmo de la verosimilitud -3.099663257598877
Logaritmo negativo de la verosimilitud: 3.099663257598877
-----------
Bigrama ejemplo 5: ia, índices 9,1
Input: 9
Probabilidades de cada output calculadas por la red neuronal: tensor([0.0452, 0.0301, 0.1283, 0.0392, 0.0107, 0.0309, 0.1220, 0.0093, 0.1139,
        0.0111, 0.0087, 0.0042, 0.0337, 0.0945, 0.0093, 0.0085, 0.0163, 0.0475,
        0.0031, 0.0154, 0.0541, 0.0090, 0.0990, 0.0133, 0.0252, 0.0098, 0.0078],
       grad_fn=&lt;SelectBackward0&gt;)
Output correcto: 1
Probabilidad asignada por la red neuronal al carácter correcto: 0.030115216970443726
Logaritmo de la verosimilitud -3.5027246475219727
Logaritmo negativo de la verosimilitud: 3.5027246475219727
-----------
Bigrama ejemplo 6: a., índices 1,0
Input: 1
Probabilidades de cada output calculadas por la red neuronal: tensor([0.0121, 0.0177, 0.0048, 0.1065, 0.0027, 0.0480, 0.0031, 0.0411, 0.0335,
        0.0278, 0.1691, 0.0037, 0.0235, 0.0201, 0.0221, 0.0541, 0.0329, 0.0447,
        0.0453, 0.0087, 0.0131, 0.0272, 0.0486, 0.0452, 0.0956, 0.0360, 0.0130],
       grad_fn=&lt;SelectBackward0&gt;)
Output correcto: 0
Probabilidad asignada por la red neuronal al carácter correcto: 0.0121311629191041
Logaritmo de la verosimilitud -4.411977767944336
Logaritmo negativo de la verosimilitud: 4.411977767944336
----------
Promedio de la nll, i. e. pérdida total = 3.607403516769409
</pre></div>
</div>
</div>
</div>
<p>Bien, tenemos los 6 bigramas del nombre, el índice de cada input y output de cada bigrama, etcétera. Ahora, queremos ajustar nuestro modelo para que, con base en la pérdida de cada bigrama —medida por el logaritmo negativo de la verosimilitud, igual que anteriormente—, optimicemos los pesos de la matriz <code class="docutils literal notranslate"><span class="pre">W</span></code> de tal forma que, al multiplicarla por cada vector input (<code class="docutils literal notranslate"><span class="pre">xenc</span></code>), nos devuelva otro vector con probabilidades asignadas a cada carácter que puede suceder el carácter en cuestión, pero con probabilidad alta asignada al carácter que debe acompañarlo.</p>
<p>Tomemos en cuenta que todas las operaciones que hemos realizado hasta ahora son diferenciables (se pueden derivar). Ahora, para poder programar una función de pérdida que entrene todos nuestros términos, ejemplifiquemos también con nuestro primer nombre:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xs</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 0, 13,  1, 18,  9,  1])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ys</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([13,  1, 18,  9,  1,  0])
</pre></div>
</div>
</div>
</div>
<p>Obtenemos el índice de cada bigrama en nuestros tensores <code class="docutils literal notranslate"><span class="pre">x</span></code> e <code class="docutils literal notranslate"><span class="pre">y</span></code> y, con base en ellos, rastreamos la probabilidad asignada a <code class="docutils literal notranslate"><span class="pre">y</span></code> dado <code class="docutils literal notranslate"><span class="pre">x</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">13</span><span class="p">],</span> <span class="n">probs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">probs</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">18</span><span class="p">],</span> <span class="n">probs</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">9</span><span class="p">],</span> <span class="n">probs</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">probs</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor(0.0471, grad_fn=&lt;SelectBackward0&gt;),
 tensor(0.0113, grad_fn=&lt;SelectBackward0&gt;),
 tensor(0.0453, grad_fn=&lt;SelectBackward0&gt;),
 tensor(0.0451, grad_fn=&lt;SelectBackward0&gt;),
 tensor(0.0301, grad_fn=&lt;SelectBackward0&gt;),
 tensor(0.0121, grad_fn=&lt;SelectBackward0&gt;))
</pre></div>
</div>
</div>
</div>
<p>Que sería equivalente a hacer algo como:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">),</span> <span class="n">ys</span><span class="p">[:</span><span class="mi">6</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.0471, 0.0113, 0.0453, 0.0451, 0.0301, 0.0121],
       grad_fn=&lt;IndexBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Y para obtener el promedio general del logaritmo negativo de la verosimilitud, únicamente agregamos:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">),</span> <span class="n">ys</span><span class="p">[:</span><span class="mi">6</span><span class="p">]]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(3.6074, grad_fn=&lt;NegBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Finalmente, agregaremos un componente de regularización —el cual explicaremos en otra ocasión, aunque de momento se puede visualizar <a class="reference external" href="https://youtu.be/EehRcPo1M-Q">este video</a>— a nuestra pérdida. Ahora ya podemos entrenar nuestro modelo:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">num</span> <span class="o">=</span> <span class="n">xs</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># FORWARD PASS</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
  <span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">27</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="c1"># one-hot encoding</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span> <span class="c1">#multiplicamos valores de x por w para obtener logits</span>
  <span class="n">counts</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="c1"># exponenciamos para obtener valores mayores a 0, equivalentes a matriz N</span>
  <span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># normalizar los conteos para obtener probabilidades</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num</span><span class="p">),</span> <span class="n">ys</span><span class="p">]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.01</span><span class="o">*</span><span class="p">(</span><span class="n">W</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="c1"># creamos función de pérdida (este último término es la regularización) </span>
  
  <span class="c1"># BACKWARD PASS</span>
  <span class="n">W</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># equivalente a reiniciar los gradientes a 0</span>
  <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># propagación hacia atrás</span>
  <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
  <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;step: [</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">&gt;02d</span><span class="si">}</span><span class="s2">]   loss=</span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

  <span class="c1"># UPDATE</span>
  <span class="n">W</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="mi">50</span> <span class="o">*</span> <span class="n">W</span><span class="o">.</span><span class="n">grad</span> <span class="c1"># actualizamos los valores de W con base en sus gradientes</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>step: [00]   loss=3.806062
step: [05]   loss=2.672217
step: [10]   loss=2.497344
step: [15]   loss=2.428483
step: [20]   loss=2.393344
step: [25]   loss=2.371606
step: [30]   loss=2.356778
step: [35]   loss=2.346093
step: [40]   loss=2.338058
step: [45]   loss=2.331840
</pre></div>
</div>
</div>
</div>
<p>Podemos visualizar nuestra pérdida a lo largo del entrenamiento:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span> 
<span class="n">ax</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Pérdida&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Step&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/hacedor_95_0.png" src="../../_images/hacedor_95_0.png" />
</div>
</div>
<p>Nuestra distribución de probabilidades para cada carácter ahora luce así (por detalles técnicos, debemos leer «.» en lugar de «`»):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">chr</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">96</span><span class="p">,</span> <span class="mi">123</span><span class="p">))),</span> <span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> 
<span class="n">ax</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/hacedor_97_0.png" src="../../_images/hacedor_97_0.png" />
</div>
</div>
<p>Una vez entrenado el modelo, podemos obtener muestras con base en él:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
  <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">ix</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">ix</span><span class="p">]),</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">27</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fap</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">break</span>

  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>a.
o.
r.
osama.
dan.
iolidenarteceliugialdeminalbqs.
cin.
limo.
enima.
cincejumiliva.
</pre></div>
</div>
</div>
</div>
<p>Finalmente, comparemos las matrices entrenadas de ambos métodos:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">W_exp</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
<span class="n">P_nn</span> <span class="o">=</span> <span class="n">W_exp</span> <span class="o">/</span> <span class="n">W_exp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">P_nn</span><span class="o">.</span><span class="n">shape</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;plasma&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Método de conteo&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">P_nn</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;plasma&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Red neuronal&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/hacedor_101_0.png" src="../../_images/hacedor_101_0.png" />
</div>
</div>
<p>Aunque nuestro modelo de red neuronal no haya superado al de simple conteo y probabilidad, puesto que en realidad los implementamos de tal manera que son prácticamente iguales, lo cierto es que esta estructura de red neuronal contiene ya los rudimentos esenciales para superar con creces al modelo anterior. En realidad, una implementación más compleja y óptima de nuestra red neuronal solo consistirá en cambiar la manera en que lidiamos con los datos (nuestro vocabulario, <em>tokens</em>, tabla de consulta, <code class="docutils literal notranslate"><span class="pre">xenc</span></code>, por ejemplo) y con las capas de neuronas (nuestra <code class="docutils literal notranslate"><span class="pre">W</span></code>, por ejemplo). Todo lo demás permanecerá igual. En la próxima lección profundizaremos en esto.</p>
</section>
<section id="bibliografia-recomendada">
<h2>Bibliografía recomendada<a class="headerlink" href="#bibliografia-recomendada" title="Permalink to this headline">#</a></h2>
<p><strong>zhang2021dive</strong> Zhang, Aston; Lipton, Zachary C.; Li, Mu y Smola, Alexander J. <em>Dive into Deep Learning</em> (cap. 15), 2021. <a class="reference external" href="https://d2l.ai/chapter_natural-language-processing-pretraining/index.html">URL</a>.</p>
<p><strong>voita2020nlpCourse</strong> Voita, Elena. <em>NLP Course For You</em>, 2020. <a class="reference external" href="https://lena-voita.github.io/nlp_course.html">URL</a>.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id4"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>La función de pérdida siempre dependerá de la naturaleza del problema. Dado que este es un problema de clasificación relacionado con probabilidades, hemos elegido una función apta. Durante la implementación de <code class="docutils literal notranslate"><span class="pre">pequegrad</span></code>, habíamos empleado la regresión lineal.</p>
</dd>
<dt class="label" id="id5"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Comúnmente se denomina capa lineal o <em>lineal layer</em> a la serie de pesos y/o sesgos que creamos para multiplicarlos y sumarlos por nuestras entradas. Como hemos visto, tanto la función multiplicación como la función suma siempre resultan una línea recta en el plano cartesiano, y de ahí el adjetivo «lineal». Estamos transformando linealmente nuestras entradas, puesto que la graficación de la multiplicación y la suma con los pesos y sesgos es una línea.</p>
</dd>
<dt class="label" id="id6"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Hemos denominado «logit» al resultado de nuestra multiplicación de matrices, sin embargo, en el ámbito del <em>deep learning</em>, el término «logit» puede llegar a ser <a class="reference external" href="https://stackoverflow.com/a/50511692/19440446">bastante ambiguo</a>. En nuestro caso, denominamos así a nuestra matriz porque es la última (y única) capa de la red neuronal, y está representando un conteo en bruto de la ocurrencia de cada <span class="math notranslate nohighlight">\(x\)</span> que luego utilizaremos para convertir en probabilidades. Aunque nuestra explicación también sea insatisfactoria, debemos conformarnos con ella por el momento. La ambigüedad del término es tal que su misma composición es extraña: no está prefijado con base en «logaritmo», sino en «logístico», pues es una abreviación de «unidad logística» (<em>logistic unit</em>); pero nunca ha estado clara la razón detrás del término «logístico» en matemáticas y, para más inri, el uso de «logit» en <em>deep learning</em> no siempre tiene un fundamento matemático riguroso. En fin, no nos perdamos entre las ramas y volvamos a lo nuestro.</p>
</dd>
</dl>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks/alfa"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="calculo.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Elementos de cálculo infinitesimal</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="hacedor2.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Elementos de procesamiento de lenguajes naturales, parte II</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Dante Noguez<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>