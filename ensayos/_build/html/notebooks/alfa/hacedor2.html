
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Elementos de procesamiento de lenguajes naturales, parte II &#8212; Dantis Elementorum</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../../_static/logo.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Elementos de leyes, o Nova methodus discendæ iurisprudentiæ" href="../beta/iurisprudentiae.html" />
    <link rel="prev" title="Elementos de procesamiento de lenguajes naturales, o El Hacedor" href="hacedor.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Dantis Elementorum</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Intro
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Elementorum Physicæ Mathematicæ
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="programacion.html">
   Elementos de programación
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="redes.html">
   Elementos de redes neuronales
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="calculo.html">
   Elementos de cálculo infinitesimal
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hacedor.html">
   Elementos de procesamiento de lenguajes naturales, o El Hacedor
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Elementos de procesamiento de lenguajes naturales, parte II
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Elementorum Politicæ Œconomicæ Iurisprudentiæ
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../beta/iurisprudentiae.html">
   Elementos de leyes, o Nova methodus discendæ iurisprudentiæ
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Elementorum Poeticæ
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../poeticae/borges.html">
   Elementos de literatura borgesiana, o Borges y sus precursores
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../poeticae/afro.html">
   Elementos de música afroamericana
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Exagium
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../exagium/tzompantli.html">
   Breve ensayo sobre el tzompantli
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exagium/castillos.html">
   La importancia de ser olvidado
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/DanteNoguez/Ensayos/main?urlpath=tree/ensayos/notebooks/alfa/hacedor2.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/DanteNoguez/Ensayos"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/DanteNoguez/Ensayos/issues/new?title=Issue%20on%20page%20%2Fnotebooks/alfa/hacedor2.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/notebooks/alfa/hacedor2.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#un-modelo-neuronal-probabilistico-de-lenguaje">
   Un modelo neuronal probabilístico de lenguaje
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizacion-de-modelos-neuronales-conjuntos-de-datos-sobreajuste-tasa-de-aprendizaje-y-lotes">
   Optimización de modelos neuronales: conjuntos de datos, sobreajuste, tasa de aprendizaje y lotes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ensayo-de-un-modelo-con-hiperparametros-optimos">
   Ensayo de un modelo con hiperparámetros óptimos
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Elementos de procesamiento de lenguajes naturales, parte II</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#un-modelo-neuronal-probabilistico-de-lenguaje">
   Un modelo neuronal probabilístico de lenguaje
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizacion-de-modelos-neuronales-conjuntos-de-datos-sobreajuste-tasa-de-aprendizaje-y-lotes">
   Optimización de modelos neuronales: conjuntos de datos, sobreajuste, tasa de aprendizaje y lotes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ensayo-de-un-modelo-con-hiperparametros-optimos">
   Ensayo de un modelo con hiperparámetros óptimos
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="elementos-de-procesamiento-de-lenguajes-naturales-parte-ii">
<h1>Elementos de procesamiento de lenguajes naturales, parte II<a class="headerlink" href="#elementos-de-procesamiento-de-lenguajes-naturales-parte-ii" title="Permalink to this headline">#</a></h1>
<p>Siguiendo nuestra lección anterior, optimizaremos nuestro modelo de red neuronal para crear nombres. Ahora, lo haremos en el estilo del <em>paper</em> <a class="reference external" href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">Bengio et. al, 2003</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s1">&#39;data/&#39;</span><span class="p">)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">path</span><span class="o">.</span><span class="n">is_dir</span><span class="p">():</span>
  <span class="n">path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s1">&#39;nombres.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
  <span class="n">request</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;https://github.com/DanteNoguez/CalculusRatiocinator/raw/main/data/nombres.txt&#39;</span><span class="p">)</span>
  <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nombres</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/nombres.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
<span class="n">nombres</span><span class="p">[:</span><span class="mi">8</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;maria&#39;, &#39;rosa&#39;, &#39;jose&#39;, &#39;carmen&#39;, &#39;ana&#39;, &#39;juana&#39;, &#39;antonio&#39;, &#39;elena&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">nombres</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>21029
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">V</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">nombres</span><span class="p">)))</span>
<span class="n">paf</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="p">:</span><span class="n">f</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">f</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">V</span><span class="p">)}</span>
<span class="n">paf</span><span class="p">[</span><span class="s1">&#39;.&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">fap</span> <span class="o">=</span> <span class="p">{</span><span class="n">f</span><span class="p">:</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span><span class="p">,</span><span class="n">f</span> <span class="ow">in</span> <span class="n">paf</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fap</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{1: &#39;a&#39;, 2: &#39;b&#39;, 3: &#39;c&#39;, 4: &#39;d&#39;, 5: &#39;e&#39;, 6: &#39;f&#39;, 7: &#39;g&#39;, 8: &#39;h&#39;, 9: &#39;i&#39;, 10: &#39;j&#39;, 11: &#39;k&#39;, 12: &#39;l&#39;, 13: &#39;m&#39;, 14: &#39;n&#39;, 15: &#39;o&#39;, 16: &#39;p&#39;, 17: &#39;q&#39;, 18: &#39;r&#39;, 19: &#39;s&#39;, 20: &#39;t&#39;, 21: &#39;u&#39;, 22: &#39;v&#39;, 23: &#39;w&#39;, 24: &#39;x&#39;, 25: &#39;y&#39;, 26: &#39;z&#39;, 0: &#39;.&#39;}
</pre></div>
</div>
</div>
</div>
<section id="un-modelo-neuronal-probabilistico-de-lenguaje">
<h2>Un modelo neuronal probabilístico de lenguaje<a class="headerlink" href="#un-modelo-neuronal-probabilistico-de-lenguaje" title="Permalink to this headline">#</a></h2>
<p>Primero, comenzaremos dividiendo nuestros datos en «bloques». Por ejemplo, en nuestro modelo de bigramas, el bloque contenía un solo carácter, puesto que realizábamos la predicción a partir de una letra; pero podemos aumentar el «contexto» de nuestras predicciones para involucrar más letras al momento de predecir la siguiente. Veamos, por ejemplo, cómo luciría nuestro tratamiento de los datos si hiciéramos bloques de tres caracteres para predecir el siguiente:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">construir_dataset</span><span class="p">(</span><span class="n">nombres</span><span class="p">):</span>
  <span class="n">block_size</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># longitud del contexto</span>
  <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">nombres</span><span class="p">:</span>
    <span class="c1">#print(f&#39;nombre: {n}&#39;)</span>
    <span class="n">contexto</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">block_size</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">n</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">:</span>
      <span class="n">ix</span> <span class="o">=</span> <span class="n">paf</span><span class="p">[</span><span class="n">c</span><span class="p">]</span>
      <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">contexto</span><span class="p">)</span>
      <span class="n">Y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
      <span class="c1">#print(&#39;&#39;.join(fap[i] for i in contexto), &#39;----&gt; &#39;, fap[ix])</span>
      <span class="n">contexto</span> <span class="o">=</span> <span class="n">contexto</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ix</span><span class="p">]</span>
  
  <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># contexto</span>
  <span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="c1"># objetivo</span>
  <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span>
</pre></div>
</div>
</div>
</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>“The training set is a sequence <span class="math notranslate nohighlight">\(w_1 · · · w_T\)</span> of words <span class="math notranslate nohighlight">\(w_t \in V\)</span>, where the vocabulary <span class="math notranslate nohighlight">\(V\)</span> is a large but finite set.”</p>
</aside>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">construir_dataset</span><span class="p">(</span><span class="n">nombres</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>nombre: maria
... ----&gt;  m
..m ----&gt;  a
.ma ----&gt;  r
mar ----&gt;  i
ari ----&gt;  a
ria ----&gt;  .
nombre: rosa
... ----&gt;  r
..r ----&gt;  o
.ro ----&gt;  s
ros ----&gt;  a
osa ----&gt;  .
nombre: jose
... ----&gt;  j
..j ----&gt;  o
.jo ----&gt;  s
jos ----&gt;  e
ose ----&gt;  .
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[ 0,  0,  0],
         [ 0,  0, 13],
         [ 0, 13,  1],
         [13,  1, 18],
         [ 1, 18,  9],
         [18,  9,  1],
         [ 0,  0,  0],
         [ 0,  0, 18],
         [ 0, 18, 15],
         [18, 15, 19],
         [15, 19,  1],
         [ 0,  0,  0],
         [ 0,  0, 10],
         [ 0, 10, 15],
         [10, 15, 19],
         [15, 19,  5]]),
 tensor([13,  1, 18,  9,  1,  0, 18, 15, 19,  1,  0, 10, 15, 19,  5,  0]))
</pre></div>
</div>
</div>
</div>
<p>Nuestra intuición detrás de esta aproximación es que el lenguaje funciona mejor con contexto: así como el sentido de un concepto se entiende mejor en contexto, también los caracteres se pueden predecir más razonablemente dado un contexto más amplio.</p>
<p>Similar a como habíamos hecho anteriormente, construimos una matriz <code class="docutils literal notranslate"><span class="pre">X</span></code> para contener el contexto como entrada y un vector <code class="docutils literal notranslate"><span class="pre">Y</span></code> que contiene el objetivo (es decir, carácter) que debe seguir a cada respectivo contexto. Como se puede apreciar, solamente estamos construyendo <code class="docutils literal notranslate"><span class="pre">X</span></code> e <code class="docutils literal notranslate"><span class="pre">Y</span></code> con sus respectivos índices del vocabulario.</p>
<p>Dado que solo tomamos 3 nombres como ejemplo, nuestros datos únicamente contienen 16 contextos o <em>inputs</em> y 16 objetivos o <em>outputs</em>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">construir_dataset</span><span class="p">(</span><span class="n">nombres</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>
<span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([16, 3]), torch.Size([16]))
</pre></div>
</div>
</div>
</div>
<p>Ahora estamos listos para hacer el <em>embedding</em>. Mientras que en el <em>paper</em> los datos se incrustan en una tabla de consulta de 30 dimensiones (o <em>features</em>) para un vocabulario de 17,000 palabras, nosotros —que únicamente tenemos un vocabulario de 27 caracteres— podemos aproximarnos a la incrustación con algo más pequeño, como una incrustación de dos dimensiones (<span class="math notranslate nohighlight">\(m = 2\)</span>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">27</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="c1"># tabla de consulta</span>
<span class="n">C</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 1.1950, -1.6664],
        [ 1.0195, -0.4103],
        [-0.2143,  1.6073],
        [ 1.3569,  1.3905],
        [ 0.7943, -0.9118],
        [ 0.4180,  0.3789],
        [-1.1457,  0.4822],
        [-0.5390, -1.8550],
        [ 0.1015,  1.0336],
        [ 0.1822,  1.7465],
        [ 1.0465,  0.5108],
        [-0.3649,  1.3349],
        [-1.6279, -1.2823],
        [ 0.3673, -1.0746],
        [ 1.4023, -0.8277],
        [ 1.2148, -0.7885],
        [ 0.3532, -0.8317],
        [ 0.4932,  0.6096],
        [-1.4151,  0.7786],
        [ 3.8467, -1.3610],
        [ 1.9704, -0.0231],
        [-0.5216, -0.7434],
        [ 0.9543, -0.3574],
        [ 1.0646, -0.6760],
        [-0.2655, -0.1184],
        [ 2.4975,  0.1847],
        [-2.5699,  0.6519]])
</pre></div>
</div>
</div>
</div>
<p>Como vemos, cada <em>token</em> o elemento del volcabulario se incrustará en dos dimensiones, es decir, tendrá dos números asociados. Ahora, tomaremos un atajo que nos permitirá ser más eficientes con la codificación y la primera capa de la red neuronal. Anteriormente, habíamos hecho un <em>one-hot encoding</em> para luego pasarlo por una capa <code class="docutils literal notranslate"><span class="pre">W</span></code>; pero, bien visto, estos dos pasos pueden ser omitidos porque consiguen el mismo resultado que la incrustación en nuestra tabla <code class="docutils literal notranslate"><span class="pre">C</span></code>.</p>
<p>Primero: la codificación <em>one-hot</em>, si fuéramos a multiplicarla por <code class="docutils literal notranslate"><span class="pre">C</span></code>, anularía todos los valores de <code class="docutils literal notranslate"><span class="pre">C</span></code> al multiplicarlos por 0 y conservaría una fila correspondiente a la de la multiplicación por 1. Ergo, podemos omitir la multiplicación y hacer una indexación para asociar directamente cada carácter con cada fila que un vector <em>one-hot</em> multiplicaría por 1. Dicho esto, podemos concebir a <code class="docutils literal notranslate"><span class="pre">C</span></code> como un equivalente de la capa <code class="docutils literal notranslate"><span class="pre">W</span></code>, puesto que consiste de valores aleatorios que asignan un número a cada carácter y luego pueden optimizarse con propagación hacia atrás.</p>
<p>Dicho esto, la indexación (<em>embedding</em>) será bastante simple:</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>“A mapping <span class="math notranslate nohighlight">\(C\)</span> from any element <span class="math notranslate nohighlight">\(i\)</span> of <span class="math notranslate nohighlight">\(V\)</span> to a real vector <span class="math notranslate nohighlight">\(C(i) \in \mathbb{R}^m\)</span>. It represents the distributed feature vectors associated with each word in the vocabulary. In practice, C is represented by a <span class="math notranslate nohighlight">\(\left|V\right| \times m\)</span> matrix of free parameters”.</p>
</aside>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">X</span><span class="p">]</span> <span class="c1"># embedding</span>

<span class="sa">f</span><span class="s1">&#39;Segunda fila de C: </span><span class="si">{</span><span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">}</span><span class="s1"> | Tercer valor del tercer bloque incrustado (es decir, letra a): </span><span class="si">{</span><span class="n">emb</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Segunda fila de C: [ 1.0194591  -0.41030166] | Tercer valor del tercer bloque incrustado (es decir, letra a): [ 1.0194591  -0.41030166]&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Dimensiones del embedding: &#39;</span><span class="p">,</span> <span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Tres bloques del embedding, correspondientes a «..m», «.ma» y «mar»:&#39;</span><span class="p">,</span> <span class="n">emb</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dimensiones del embedding:  torch.Size([16, 3, 2])
Tres bloques del embedding, correspondientes a «..m», «.ma» y «mar»: tensor([[[ 1.1950, -1.6664],
         [ 1.1950, -1.6664],
         [ 0.3673, -1.0746]],

        [[ 1.1950, -1.6664],
         [ 0.3673, -1.0746],
         [ 1.0195, -0.4103]],

        [[ 0.3673, -1.0746],
         [ 1.0195, -0.4103],
         [-1.4151,  0.7786]]])
</pre></div>
</div>
</div>
</div>
<p>Ahora, echemos un vistazo a la arquitectura que deseamos lograr:</p>
<figure class="align-default" id="bengio2003">
<a class="reference internal image-reference" href="../../_images/bengio2003.png"><img alt="../../_images/bengio2003.png" src="../../_images/bengio2003.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">Arquitectura neuronal <span class="math notranslate nohighlight">\(f\left(i, w_{t-1}, \cdots, w_{t-n+1}\right)=g\left(i, C\left(w_{t-1}\right), \cdots, C\left(w_{t-n+1}\right)\right)\)</span> donde <span class="math notranslate nohighlight">\(g\)</span> es la red neuronal y <span class="math notranslate nohighlight">\(C(i)\)</span> es el <span class="math notranslate nohighlight">\(i\)</span>-ésimo vector de cada palabra. En nuestro caso, utilizamos bloques de tres letras (<code class="docutils literal notranslate"><span class="pre">contexto</span></code>, vector <code class="docutils literal notranslate"><span class="pre">X</span></code>) en lugar de palabras (<span class="math notranslate nohighlight">\(w\)</span>).</span><a class="headerlink" href="#bengio2003" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Como vemos, tenemos casi terminado el inicio y únicamente nos falta concatenar entre sí los bloques del <em>embedding</em>, puesto que juntos atravesarán la misma capa de neuronas.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>“[…] <span class="math notranslate nohighlight">\(x\)</span> is the word features layer activation vector, which is the concatenation of the input word features from the matrix <span class="math notranslate nohighlight">\(C\)</span>: <span class="math notranslate nohighlight">\(x = C\left(w_{t-1}\right), \left(w_{t-2}\right) \cdots, C\left(w_{t-n+1}\right)\)</span>.”</p>
</aside>
<p>Para conseguirlo, podemos utilizar distintos métodos con PyTorch:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">metodo1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">emb</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">emb</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">emb</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">:]],</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;Ahora, en lugar de estar contenidos en bloques de tres filas: </span>
<span class="si">{</span><span class="n">emb</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"></span>
<span class="s2">Estarían contenidos en bloques de una fila (seis columnas):</span>
<span class="si">{</span><span class="n">metodo1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Ahora, en lugar de estar contenidos en bloques de tres filas: 
tensor([[ 1.1950, -1.6664],
        [ 1.1950, -1.6664],
        [ 1.1950, -1.6664]])
Estarían contenidos en bloques de una fila (seis columnas):
tensor([ 1.1950, -1.6664,  1.1950, -1.6664,  1.1950, -1.6664])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">metodo2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">emb</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="sa">f</span><span class="s1">&#39;Aunque también es equivalente: </span><span class="si">{</span><span class="n">metodo2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Aunque también es equivalente: tensor([ 1.1950, -1.6664,  1.1950, -1.6664,  1.1950, -1.6664])&#39;
</pre></div>
</div>
</div>
</div>
<p>Pero el método más eficiente<a class="footnote-reference brackets" href="#id4" id="id1">1</a> y simple es <code class="docutils literal notranslate"><span class="pre">view</span></code>. Como primer argumento, colocaremos <code class="docutils literal notranslate"><span class="pre">-1</span></code> para que de esta forma PyTorch infiera el tamaño de la dimensión 0 que debería tener el tensor (sería, pues, equivalente a colocar <code class="docutils literal notranslate"><span class="pre">emb[0].shape</span></code>), y como segundo argumento <code class="docutils literal notranslate"><span class="pre">6</span></code> porque queremos que el tensor tenga las 6 columnas correspondientes a un bloque de tres <em>tokens</em>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># esta es la variable x del paper</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([ 1.1950, -1.6664,  1.1950, -1.6664,  1.1950, -1.6664]),
 torch.Size([16, 6]))
</pre></div>
</div>
</div>
</div>
<p>Ahora ya tenemos lo suficiente para definir más variables:</p>
<blockquote>
<div><p>“Let <span class="math notranslate nohighlight">\(h\)</span> be the number of hidden units<a class="footnote-reference brackets" href="#id5" id="id2">2</a>, and <span class="math notranslate nohighlight">\(m\)</span> the number of features associated with each word. When no direct connections from word features to outputs are desired, the matrix <span class="math notranslate nohighlight">\(W\)</span> is set to 0 . The free parameters of the model are the output biases <span class="math notranslate nohighlight">\(b\)</span> (with <span class="math notranslate nohighlight">\(|V|\)</span> elements), the hidden layer biases <span class="math notranslate nohighlight">\(d\)</span> (with <span class="math notranslate nohighlight">\(h\)</span> elements), the hidden-to-output weights <span class="math notranslate nohighlight">\(U\)</span> (a <span class="math notranslate nohighlight">\(|V| \times h\)</span> matrix), the word features to output weights <span class="math notranslate nohighlight">\(W\)</span> (a <span class="math notranslate nohighlight">\(|V| \times(n-1) m\)</span> matrix), the hidden layer weights <span class="math notranslate nohighlight">\(H\)</span> (a <span class="math notranslate nohighlight">\(h \times(n-1) m\)</span> matrix), and the word features <span class="math notranslate nohighlight">\(C\)</span>(a <span class="math notranslate nohighlight">\(|V| \times m\)</span> matrix <span class="math notranslate nohighlight">\()\)</span>: <span class="math notranslate nohighlight">\(\theta=(b, d, W, U, H, C)\)</span>”.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sa">f</span><span class="s1">&#39;Número de features (m), es decir, número de componentes de cada bloque: </span><span class="si">{</span><span class="n">C</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s1"> | Número de elementos por bloque «(n-1)m»: </span><span class="si">{</span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s1"> | Elementos de |V|: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">V</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Número de features (m), es decir, número de componentes de cada bloque: 2 | Número de elementos por bloque «(n-1)m»: 6 | Elementos de |V|: 27&#39;
</pre></div>
</div>
</div>
</div>
<p>El número de parámetros (<span class="math notranslate nohighlight">\(h\)</span>) depende del problema a tratar: generalmente, a mayor cantidad de datos, es mejor mayor cantidad de parámetros. En general, cuestiones técnicas como esta dependen de la evaluación experimental que hagamos de nuestro modelo: tras pruebas con diferentes números de parámetros, podemos elegir la que más efectiva y eficiente sea. En el <em>paper</em>, por ejemplo, probaron con 50 y 100 <em>hidden units</em>. Dado que por el momento solo estamos ejemplificando con tres nombres, <span class="math notranslate nohighlight">\(h = 50\)</span> unidades serán suficientes.</p>
<p>Por otra parte, fijar un valor de 0 a <span class="math notranslate nohighlight">\(W\)</span> es igual que no utilizarla, de manera que omitiremos su definición porque no la necesitamos.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">h</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">h</span><span class="p">))</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span> <span class="o">@</span> <span class="n">H</span> <span class="o">+</span> <span class="n">d</span><span class="p">)</span>
<span class="n">a</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([16, 50])
</pre></div>
</div>
</div>
</div>
<p>Ahora, la capa oculta previa al <em>output</em>, es decir, la <em>hidden-to-output layer</em> se compone de <span class="math notranslate nohighlight">\(U\)</span> y <span class="math notranslate nohighlight">\(b\)</span>, mientras que el resultado de esta capa son —el lector lo recordará— lo que llamamos <em>logits</em>, es decir, el resultado de la última capa de la red neuronal. Estos <em>logits</em> serán convertidos en  probabilidades y, con ello, tendremos el <em>output</em> de toda la red.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">U</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">h</span><span class="p">,</span> <span class="mi">27</span><span class="p">))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">27</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">U</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">logits</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([16, 27])
</pre></div>
</div>
</div>
</div>
<p>Ahora, para convertir esto en probabilidades, haremos lo mismo que en la lección pasada:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">counts</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
<span class="n">prob</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">prob</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1.0000)
</pre></div>
</div>
</div>
</div>
<p>Para crear nuestra función de pérdida, necesitamos seleccionar nuestros objetivos (con base en el índice que nos dio <code class="docutils literal notranslate"><span class="pre">Y</span></code>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">prob</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span> <span class="n">Y</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([2.7201e-07, 1.4083e-02, 5.6987e-07, 3.5629e-07, 9.4318e-10, 4.6116e-08,
        2.3890e-10, 9.0728e-04, 2.6708e-06, 2.7046e-05, 1.8723e-08, 2.6864e-07,
        7.6290e-06, 8.3047e-06, 1.6912e-09, 1.4596e-08])
</pre></div>
</div>
</div>
</div>
<p>Ejemplifiquemos: si nuestro <em>input</em> es «mar» (<code class="docutils literal notranslate"><span class="pre">emb[3]</span></code>), nuestra probabilidad debe ser alta para que el número del <em>embedding</em> que represente la letra «i» de «María» sea un <em>output</em>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;En un inicio, cada letra de «mar» correspondía a su índice en el vocabulario: </span><span class="si">{</span><span class="n">X</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="si">}</span><span class="s2"></span>
<span class="s2">Cuando pasamos estos índices a una matriz para que fueran representados por dos números, obtuvimos: </span><span class="si">{</span><span class="n">emb</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="si">}</span><span class="s2"></span>
<span class="s2">Al mismo tiempo, la letra «i» fue guardada como objetivo en Y, siendo su índice: </span><span class="si">{</span><span class="n">Y</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="si">}</span><span class="s2"></span>
<span class="s2">De manera que la probabilidad de nuestra red neuronal debe ser alta para el número que representa la letra «i» en el embedding: </span><span class="si">{</span><span class="o">-</span><span class="n">prob</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="n">Y</span><span class="p">[</span><span class="mi">3</span><span class="p">]]</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>En un inicio, cada letra de «mar» correspondía a su índice en el vocabulario: tensor([13,  1, 18])
Cuando pasamos estos índices a una matriz para que fueran representados por dos números, obtuvimos: tensor([[ 0.3673, -1.0746],
        [ 1.0195, -0.4103],
        [-1.4151,  0.7786]])
Al mismo tiempo, la letra «i» fue guardada como objetivo en Y, siendo su índice: 9
De manera que la probabilidad de nuestra red neuronal debe ser alta para el número que representa la letra «i» en el embedding: -3.5628877981253027e-07
</pre></div>
</div>
</div>
</div>
<p>Dado que no hemos entrenado la red, la probabilidad anterior es muy baja. Para poder entrenarla, formularemos la función de pérdida mediante el promedio del logaritmo natural de las probabilidades que la red neuronal asigna a cada objetivo, y finalmente hacemos de este un número positivo al multiplicarlo por <span class="math notranslate nohighlight">\(-1\)</span>, igual que en la lección pasada:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">perdida</span> <span class="o">=</span> <span class="o">-</span><span class="n">prob</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span> <span class="n">Y</span><span class="p">]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">perdida</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(14.5898)
</pre></div>
</div>
</div>
</div>
<p>Usando PyTorch, podemos simplificar todo este proceso mediante el uso de <code class="docutils literal notranslate"><span class="pre">cross_entropy</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(14.5898)
</pre></div>
</div>
</div>
</div>
<p>Finalmente, agrupamos los parámetros —<span class="math notranslate nohighlight">\(\theta\)</span>— en una variable:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">parametros</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">C</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Indicamos a PyTorch que requeriremos gradientes para el entrenamiento de nuestros parámetros:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
  <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
</div>
<p>Finalmente, podemos entrenar la red:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
  <span class="c1"># paso hacia delante</span>
  <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">X</span><span class="p">]</span>
  <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span> <span class="o">@</span> <span class="n">H</span> <span class="o">+</span> <span class="n">d</span><span class="p">)</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">U</span> <span class="o">+</span> <span class="n">b</span>
  <span class="n">perdida</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
  
  <span class="c1"># propagación hacia atrás</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">perdida</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
  
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>

<span class="nb">print</span><span class="p">(</span><span class="n">perdida</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.25268855690956116
</pre></div>
</div>
</div>
</div>
<p>Bien, hemos conseguido un resultado decente en nuestro ejemplo. Ahora es momento de entrenar nuestra red con todos los nombres a la vez.</p>
</section>
<section id="optimizacion-de-modelos-neuronales-conjuntos-de-datos-sobreajuste-tasa-de-aprendizaje-y-lotes">
<h2>Optimización de modelos neuronales: conjuntos de datos, sobreajuste, tasa de aprendizaje y lotes<a class="headerlink" href="#optimizacion-de-modelos-neuronales-conjuntos-de-datos-sobreajuste-tasa-de-aprendizaje-y-lotes" title="Permalink to this headline">#</a></h2>
<p>Primero hacen falta algunos ajustes. Por una parte, evitaremos que nuestro modelo «memorice» o «sobreajuste» cada <em>output</em> correspondiente a cada <em>input</em> (a este sobreajuste se le llama <em>overfitting</em>), puesto que queremos nombres nuevos y originales y no una regurgitación de los que estamos utilizando. Para ello, se han desarrollado un número de técnicas en <em>deep learning</em>; pero de momento, utilizaremos una de las más elementales, y consiste en separar nuestros datos en tres partes: entrenamiento, validación y prueba. Entrenaremos nuestro modelo con una cantidad razonable de nombres (el 80 % de ellos), definimos nuestros parámetros —en rigor, hiperparámetros<a class="footnote-reference brackets" href="#id6" id="id3">3</a>— con los datos de validación (10 % de nuestros nombres) y verificamos que el modelo sepa generalizar su aprendizaje con los datos destinados a las pruebas (último 10 % de nombres).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">nombres</span><span class="p">)</span>
<span class="n">n1</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">nombres</span><span class="p">))</span>
<span class="n">n2</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.9</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">nombres</span><span class="p">))</span>

<span class="n">Xtr</span><span class="p">,</span> <span class="n">Ytr</span> <span class="o">=</span> <span class="n">construir_dataset</span><span class="p">(</span><span class="n">nombres</span><span class="p">[:</span><span class="n">n1</span><span class="p">])</span>
<span class="n">Xdev</span><span class="p">,</span> <span class="n">Ydev</span> <span class="o">=</span> <span class="n">construir_dataset</span><span class="p">(</span><span class="n">nombres</span><span class="p">[</span><span class="n">n1</span><span class="p">:</span><span class="n">n2</span><span class="p">])</span>
<span class="n">Xte</span><span class="p">,</span> <span class="n">Yte</span> <span class="o">=</span> <span class="n">construir_dataset</span><span class="p">(</span><span class="n">nombres</span><span class="p">[</span><span class="n">n2</span><span class="p">:])</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">construir_dataset</span><span class="p">(</span><span class="n">nombres</span><span class="p">)</span>

<span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Xtr</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Xdev</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Xte</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([165469, 3]),
 torch.Size([132418, 3]),
 torch.Size([16559, 3]),
 torch.Size([16492, 3]))
</pre></div>
</div>
</div>
</div>
<p>Ahora definimos nuestros hiperparámetros, aunque esta vez utilizaremos <span class="math notranslate nohighlight">\(h = 100\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">27</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">Xtr</span><span class="p">]</span>
<span class="n">h</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span> <span class="o">@</span> <span class="n">H</span> <span class="o">+</span> <span class="n">d</span><span class="p">)</span>

<span class="n">U</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="mi">27</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">27</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">U</span> <span class="o">+</span> <span class="n">b</span>

<span class="n">parametros</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">b</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Nuestro número total de parámetros resultantes es:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3481
</pre></div>
</div>
</div>
</div>
<p>Por otro lado, podemos acelerar el entrenamiento de la red neuronal de la siguiente forma: en cada repetición del <em>loop</em> de aprendizaje, podemos seleccionar un «lote» (<em>batch</em>) o segmento de los datos para únicamente llevar a cabo el proceso de aprendizaje en ese mismo lote, de manera que el modelo no se entrene tomando siempre en consideración todos los datos a la vez, sino datos —en este caso, nombres— aleatorios en cada iteración. Aunque esto implique sacrificar en cierta medida el desempeño de la red neuronal, sin embargo ese sacrificio se compensa con la rapidez que podemos generar a cambio.</p>
<p>Para crear estos lotes, podemos utilizar el siguiente código, el cual generará índices correspondientes a 32 nombres aleatorios de entre aquellos que pertenezcan a nuestros datos:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="mi">32</span><span class="p">,))</span>
<span class="sa">f</span><span class="s1">&#39;Tres ejemplos: </span><span class="si">{</span><span class="n">ix</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Tres ejemplos: [123401  87687  70020]&#39;
</pre></div>
</div>
</div>
</div>
<p>Finalmente, nos falta tratar los detalles técnicos detrás de la <em>learning rate</em> («tasa de aprendizaje») del modelo. Recordemos que, cuando modificamos los parámetros en la dirección del gradiente, generalmente atenuamos al gradiente multiplicándolo por un número pequeño para así no excedernos en el ajuste, consiguiendo una pérdida cercana a 0 sin sobrepasarla.</p>
<p>Para determinar una <em>learning rate</em> razonable, tenemos que averiguar empíricamente los «límites» de la misma:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
  <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
  <span class="c1">#minibatch («minilote»)</span>
  <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xdev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="mi">32</span><span class="p">,))</span>

  <span class="c1"># propagación hacia delante</span>
  <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">Xdev</span><span class="p">[</span><span class="n">ix</span><span class="p">]]</span>
  <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span> <span class="o">@</span> <span class="n">H</span> <span class="o">+</span> <span class="n">d</span><span class="p">)</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">U</span> <span class="o">+</span> <span class="n">b</span>
  <span class="n">perdida</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Ydev</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">perdida</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
  
  <span class="c1"># propagación hacia atrás</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">perdida</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

  <span class="c1"># actualización</span>
  <span class="n">lr</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="c1"># intentaremos obtener el límite superior</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>22.080612182617188
18.37497329711914
15.564812660217285
15.621807098388672
17.045852661132812
18.507583618164062
21.449209213256836
21.56830596923828
24.576093673706055
15.103194236755371
</pre></div>
</div>
</div>
</div>
<p>Como vemos, la pérdida disminuye con una tasa de aprendizaje de -1; pero al mismo tiempo indica que esta tasa es muy alta porque no es nada estable y se balancea hacia arriba y abajo. Este puede ser nuestro límite superior, busquemos el inferior:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
  <span class="c1">#minibatch («minilote»)</span>
  <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xdev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="mi">32</span><span class="p">,))</span>

  <span class="c1"># propagación hacia delante</span>
  <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">Xdev</span><span class="p">[</span><span class="n">ix</span><span class="p">]]</span>
  <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span> <span class="o">@</span> <span class="n">H</span> <span class="o">+</span> <span class="n">d</span><span class="p">)</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">U</span> <span class="o">+</span> <span class="n">b</span>
  <span class="n">perdida</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Ydev</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">perdida</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
  
  <span class="c1"># propagación hacia atrás</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">perdida</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

  <span class="c1"># actualización</span>
  <span class="n">lr</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.001</span> <span class="c1"># intentaremos obtener el límite inferior</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>16.730724334716797
19.113916397094727
16.265771865844727
16.343080520629883
19.638389587402344
17.050613403320312
16.68904685974121
23.818950653076172
16.777978897094727
20.498703002929688
</pre></div>
</div>
</div>
</div>
<p>De igual forma, esta pérdida es subóptima porque es demasiado lenta para eficientar el aprendizaje, de manera que este puede ser nuestro límite inferior. Ahora, haremos una prueba de entrenamiento con tasas de aprendizaje que se encuentren dentro de estos límites; pero, para que nuestra tasa de aprendizaje no cambie lineal sino exponencialmente, podemos utilizar nuestros números como potencias de 10. Es decir, en teoría, nuestras tasas de aprendizaje serían mil números del 0.001 al 1:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">numpy</span><span class="p">());</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/hacedor2_59_0.png" src="../../_images/hacedor2_59_0.png" />
</div>
</div>
<p>Pero <span class="math notranslate nohighlight">\(10^{-3} = 0.001\)</span> y <span class="math notranslate nohighlight">\(10{^0} = 1\)</span>, de manera que utilizar este truco nos ayuda a obtener un cambio exponencial en nuestras tasas de aprendizaje. Ahora, nuestros 1000 números serán exponentes de 10 y lucirán así:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lre</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">lrs</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="n">lre</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs</span><span class="o">.</span><span class="n">numpy</span><span class="p">());</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/hacedor2_61_0.png" src="../../_images/hacedor2_61_0.png" />
</div>
</div>
<p>Ahora, para elegir el número correcto en este rango, podemos registrar nuestras pérdidas correspondientes a cada tasa durante el entrenamiento:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lr_i</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">perdidas_i</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
  <span class="c1">#minibatch («minilote»)</span>
  <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xdev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="mi">32</span><span class="p">,))</span>

  <span class="c1"># propagación hacia delante</span>
  <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">Xdev</span><span class="p">[</span><span class="n">ix</span><span class="p">]]</span>
  <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span> <span class="o">@</span> <span class="n">H</span> <span class="o">+</span> <span class="n">d</span><span class="p">)</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">U</span> <span class="o">+</span> <span class="n">b</span>
  <span class="n">perdida</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Ydev</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
  <span class="c1">#print(perdida.item())</span>
  
  <span class="c1"># propagación hacia atrás</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">perdida</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

  <span class="c1"># actualización</span>
  <span class="n">lr</span> <span class="o">=</span> <span class="n">lrs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="c1"># intentaremos obtener el límite inferior</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>

  <span class="c1"># registrar estadísticas</span>
  <span class="n">lr_i</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lre</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
  <span class="n">perdidas_i</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">perdida</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lr_i</span><span class="p">,</span> <span class="n">perdidas_i</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/hacedor2_64_0.png" src="../../_images/hacedor2_64_0.png" />
</div>
</div>
<p>En el gráfico podemos apreciar que nuestra tasa de aprendizaje óptima —es decir, la que mejor minimiza la pérdida— se encuentra alrededor de <span class="math notranslate nohighlight">\(10^{-1}\)</span> (es decir, <span class="math notranslate nohighlight">\(0.1\)</span>), de manera que podemos utilizar esta tasa con mayor confianza para entrenar todos nuestros parámetros con los datos de <code class="docutils literal notranslate"><span class="pre">Xtr</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">paso_i</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">perdidas_i</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
  <span class="c1">#minibatch («minilote»)</span>
  <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="mi">32</span><span class="p">,))</span>

  <span class="c1"># propagación hacia delante</span>
  <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">Xtr</span><span class="p">[</span><span class="n">ix</span><span class="p">]]</span>
  <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span> <span class="o">@</span> <span class="n">H</span> <span class="o">+</span> <span class="n">d</span><span class="p">)</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">U</span> <span class="o">+</span> <span class="n">b</span>
  <span class="n">perdida</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
  
  <span class="c1"># propagación hacia atrás</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">perdida</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

  <span class="c1"># actualización</span>
  <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="c1"># nueva tasa</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>

  <span class="c1"># registrar estadísticas</span>
  <span class="n">paso_i</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
  <span class="n">perdidas_i</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">perdida</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">paso_i</span><span class="p">,</span> <span class="n">perdidas_i</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/hacedor2_67_0.png" src="../../_images/hacedor2_67_0.png" />
</div>
</div>
<p>Debido a que entrenamos nuestros parámetros en lotes, la función de pérdida tiene algo de ruido cuando se acerca a su valor mínimo. Comprobemos ahora la pérdida que tenemos en <code class="docutils literal notranslate"><span class="pre">Xdev</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">Xdev</span><span class="p">]</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span> <span class="o">@</span> <span class="n">H</span> <span class="o">+</span> <span class="n">d</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">U</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">perdida</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Ydev</span><span class="p">)</span>
<span class="n">perdida</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(2.4327, grad_fn=&lt;NllLossBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Finalmente, podemos visualizar cómo luce nuestro <em>embedding</em> ahora que ha sido entrenado:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">C</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">C</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">C</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">fap</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s1">&#39;minor&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/hacedor2_71_0.png" src="../../_images/hacedor2_71_0.png" />
</div>
</div>
<p>Como vemos, las vocales —junto con la «y», aunque sin la «i»— aparecen agrupadas entre sí: esto quiere decir que nuestro modelo las interpreta como letras similares. Lo mismo aplica para las consonantes.</p>
<p>Hasta el momento, no hemos conseguido mejorar nuestro modelo anterior. En la práctica, la optimización de modelos se hace con un número de pruebas que contengan hiperparámetros distintos. Podemos, por ejemplo, aumentar nuestros parámetros, nuestro <em>embedding</em>, nuestras capas, probar nuevas tasas de aprendizaje, entrenar el modelo por más tiempo, etcétera. Intentemos hacer un poco de todo para finalizar esta lección:</p>
</section>
<section id="ensayo-de-un-modelo-con-hiperparametros-optimos">
<h2>Ensayo de un modelo con hiperparámetros óptimos<a class="headerlink" href="#ensayo-de-un-modelo-con-hiperparametros-optimos" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">nombres</span><span class="p">)</span>
<span class="n">n1</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">nombres</span><span class="p">))</span>
<span class="n">n2</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.9</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">nombres</span><span class="p">))</span>

<span class="n">Xtr</span><span class="p">,</span> <span class="n">Ytr</span> <span class="o">=</span> <span class="n">construir_dataset</span><span class="p">(</span><span class="n">nombres</span><span class="p">[:</span><span class="n">n1</span><span class="p">])</span>
<span class="n">Xdev</span><span class="p">,</span> <span class="n">Ydev</span> <span class="o">=</span> <span class="n">construir_dataset</span><span class="p">(</span><span class="n">nombres</span><span class="p">[</span><span class="n">n1</span><span class="p">:</span><span class="n">n2</span><span class="p">])</span>
<span class="n">Xte</span><span class="p">,</span> <span class="n">Yte</span> <span class="o">=</span> <span class="n">construir_dataset</span><span class="p">(</span><span class="n">nombres</span><span class="p">[</span><span class="n">n2</span><span class="p">:])</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">construir_dataset</span><span class="p">(</span><span class="n">nombres</span><span class="p">)</span>

<span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">27</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="c1"># un embedding de 10 dimensiones</span>
<span class="n">h</span> <span class="o">=</span> <span class="mi">200</span> <span class="c1"># aumentamos las unidades ocultas a 200</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">30</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span> <span class="c1"># recordemos: 10 * 3 = 30</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="mi">27</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">27</span><span class="p">)</span>

<span class="n">parametros</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">b</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
  <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">15000</span><span class="p">):</span>
  <span class="c1">#minibatch («minilote»)</span>
  <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="mi">32</span><span class="p">,))</span>

  <span class="c1"># propagación hacia delante</span>
  <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">Xtr</span><span class="p">[</span><span class="n">ix</span><span class="p">]]</span>
  <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span> <span class="o">@</span> <span class="n">H</span> <span class="o">+</span> <span class="n">d</span><span class="p">)</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">U</span> <span class="o">+</span> <span class="n">b</span>
  <span class="n">perdida</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
  
  <span class="c1"># propagación hacia atrás</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">perdida</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

  <span class="c1"># actualización</span>
  <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">14000</span> <span class="k">else</span> <span class="mf">0.01</span> <span class="c1"># nuestra tasa de aprendizaje disminuirá hacia el final del entrenamiento</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parametros</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>

<span class="nb">print</span><span class="p">(</span><span class="n">perdida</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.7781141996383667
</pre></div>
</div>
</div>
</div>
<p>Finalmente hemos superado al modelo anterior con fuerza bruta. Aunque podríamos seguir ajustándolo, me parece que esto será suficiente para obtener mejores muestras de nombres. Revisemos la pérdida de nuestros <code class="docutils literal notranslate"><span class="pre">Xdev</span></code> y <code class="docutils literal notranslate"><span class="pre">Xte</span></code> para cerciorarnos de que nuestro modelo haya generalizado sus aprendizajes, en vez de memorizarlos:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">Xdev</span><span class="p">]</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span> <span class="o">@</span> <span class="n">H</span> <span class="o">+</span> <span class="n">d</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">U</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">perdida</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Ydev</span><span class="p">)</span>
<span class="n">perdida</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1.9739, grad_fn=&lt;NllLossBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">Xte</span><span class="p">]</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span> <span class="o">@</span> <span class="n">H</span> <span class="o">+</span> <span class="n">d</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">U</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">perdida</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yte</span><span class="p">)</span>
<span class="n">perdida</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1.9782, grad_fn=&lt;NllLossBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Tenemos también pérdidas razonables para ambos conjuntos de datos. Conforme la pérdida del entrenamiento difiera más de las otras dos (<code class="docutils literal notranslate"><span class="pre">Xdev</span></code> y <code class="docutils literal notranslate"><span class="pre">Xte</span></code>), esto indicaría que nuestros parámetros están memorizando los datos, no generalizándolos. En ese sentido, debemos ser cuidados de no sobreentrenar, ni tampoco sobredimensionar el modelo, puesto que si nos excedemos, terminará memorizando todos nuestros datos.</p>
<p>Para finalizar, obtengamos algunas muestras de nombres generados por nuestro modelo:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">block_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
  <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">block_size</span>
  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">context</span><span class="p">])]</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <span class="n">H</span> <span class="o">+</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">U</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ix</span><span class="p">]</span>
    <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">break</span>

  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">fap</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">out</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>cedelzorena.
carixto.
leiscasiria.
jovino.
renta.
marizio.
alfidela.
alsa.
paulogia.
taw.
migdina.
duboa.
secahmidio.
sihorgel.
floreta.
remenicia.
penoyda.
filiano.
frinda.
geria.
</pre></div>
</div>
</div>
</div>
<p>Indudablemente, estos nombres son más respetables.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id4"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>El método <code class="docutils literal notranslate"><span class="pre">view</span></code> es eficiente porque <a class="reference external" href="http://blog.ezyang.com/2019/05/pytorch-internals/">no requiere de nuevo espacio</a> en la memoria de nuestra computadora, sino que utiliza el mismo tensor para reacomodarlo de manera distinta. Conforme nuestros programas se vuelvan más complejos, debemos procurar eficientar al máximo nuestros recursos computacionales. Para un acercamiento más general al tema, véase el <a class="reference external" href="https://horace.io/brrr_intro.html">artículo de Horace He</a>.</p>
</dd>
<dt class="label" id="id5"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>En <em>deep learning</em>, el término <em>hidden</em> («oculto») se utiliza para referirse a componentes de una red neuronal que no son «visibles» ni como entradas ni como salidas de la red. Se trata de entradas o salidas que se procesan internamente por la red neuronal antes de arrojar un resultado final. La <em>hidden unit</em>, en ese sentido, se refiere a cada unidad o componente de una capa oculta (<em>hidden layer</em>) en la red. Podemos entenderla como sinónimo de «nodo» o «neurona».</p>
</dd>
<dt class="label" id="id6"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Se denominan «parámetros» a los números o coeficientes que la red neuronal aprende y determina durante el entrenamiento (por ejemplo, los números de los pesos y los sesgos). Por otra parte, los «hiperparámetros» son aquellos que nosotros definimos manualmente según la naturaleza de cada problema, y que optimizamos mediante pruebas con distintos valores (por ejemplo, nuestras unidades ocultas <span class="math notranslate nohighlight">\(h\)</span>).</p>
</dd>
</dl>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks/alfa"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="hacedor.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Elementos de procesamiento de lenguajes naturales, o El Hacedor</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../beta/iurisprudentiae.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Elementos de leyes, o Nova methodus discendæ iurisprudentiæ</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Dante Noguez<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>