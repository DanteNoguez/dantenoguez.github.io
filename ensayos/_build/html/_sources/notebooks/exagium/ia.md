# Tientos cibernéticos

<br>
<p align="right"> 
I’m going to concede that super-intelligence will be real
<br>
and I need to devote my remaining life to it.
<br>
<i>
Phil Wang
</i>
</p>
<br>

<p align="center"> <b>
§
</b>
</p>

La inteligencia artificial reemplazará a quienes temen ser reemplazados por ella.

<p align="center"> <b>
§
</b>
</p>

Lo más extraño de la inteligencia artificial es que hay quienes, tras dedicar años a la formulación de modelos gigantescos que puedan imitar el habla humana, se horrorizan al comprobar que han creado modelos gigantescos que imitan el habla humana.

<p align="center"> <b>
§
</b>
</p>

La creencia de que la inteligencia artificial representa un riesgo existencial para nosotros solo puede echar raíz por dos vías: o por una colosal ingenuidad política y una ignorancia supina sobre moral y matemáticas, o por una velada filosofía política de corte maquiavélico.

<p align="center"> <b>
§
</b>
</p>

Las arquitecturas de aprendizaje profundo que subyacen la inteligencia artificial actual están catalizadas por algoritmos compuestos de expresiones matemáticas bastante simples, como la multiplicación de matrices, la diferenciación y las funciones no lineales. Esto por sí mismo no es suficiente para crear una amenaza existencial para la humanidad.

<p align="center"> <b>
§
</b>
</p>

Hace cuatro años, cuando [escribí](https://www.academia.edu/43673994/La_Cuarta_Revolucio_n_Industrial_y_la_planeacio_n_poli_tica_en_Me_xico) sobre desempleo tecnológico, la inmensa mayoría de investigadores creían que las labores artísticas[^1] no se podían automatizar porque involucraban «imaginación», «creatividad» y otras cualidades «eminentemente humanas». Hoy, esas mismas personas son las que quieren censurar aquellas inteligencias artificiales que son capaces de generar arte. De fondo, el mismo falso razonamiento prevalece: creer que el arte se crea *ex nihilo* con varitas mágicas como el genio y la imaginación, y no mediante el aprendizaje de técnicas concretas y la inspiración tomada de cientos de otras obras.

<p align="center"> <b>
§
</b>
</p>

Me imagino que es imposible utilizar el lenguaje, e incluso pensar claramente, para aquellos que se quejan del término «inteligencia artificial». Han de creer que el cálculo infinitesimal se hace con piedritas, que los geómetras van al campo a trabajar con la tierra, y que los filósofos pierden tiempo hablando de gnoseología cuando deberían estar predicando su amor por la sabiduría.

<p align="center"> <b>
§
</b>
</p>

La inteligencia humana está determinada por la evolución de la especie y, por ello, especializada en las pocas parcelas de la realidad necesarias para su supervivencia. Lo mismo podemos decir de la inteligencia de cualquier otro animal. Utilizar al humano como vara de medir la inteligencia implica un sesgo evolutivo arbitrario hacia sus realidades cotidianas. Ese sesgo implica también una limitación de nuestra definición de inteligencia, así como de los posibles desarrollos de inteligencias artificiales.

<p align="center"> <b>
§
</b>
</p>

El poco conocimiento científico que tenemos sobre la inteligencia y la conciencia no es suficiente para desestimar la idea de que las máquinas puedan ser inteligentes o conscientes. De cualquier forma, los estándares humanos son una metáfora pobre para lo que la inteligencia artificial es, y una limitación para lo que podría ser.

<p align="right"> <i>
20 de diciembre, 2022
</i>
</p>

<br>

<p align="center"> <b>
Apéndice
</b>
</p>

*Aquila non capit muscas*, pero si ya perdí mi tiempo viendo esto, que al menos valga como advertencia para alguien más: ahí van apuntes críticos a la conferencia de [Carlos Madrid sobre inteligencia artificial](https://youtu.be/12_c7-eL7Cc).

$1$. Su repaso histórico, aunque abiertamente no pretenda ser detallado, es bastante impreciso. La investigación no se «apagó por dosis de realidad» en los 70. En realidad, desde los 60 ya existía el enfoque de redes neuronales profundas y probabilísticas (*cfr*. Rosenblatt, Steinbuch). Hacia finales de los 60 y principios de los 70 ya se había logrado el «aprendizaje» en distintos modelos. En los 70, Seppo Linnainmaa pergeñó el algoritmo de retropropagación, mientras que Amari y Werbos lo utilizaron para conectar las redes neuronales con el descenso de gradiente estocástico.

De la misma forma, la historia continuó las siguientes décadas y hasta nuestros días de una manera distinta a como se plantea. En general, son bastante groseros los saltos históricos que se hacen. Al llegar a Deep Blue como único hito específico destacable, diciendo que lo probabilístico inicia en los 90 con redes bayesianas, es todo erróneo: no es cierto que empiece en los 90 (sino 30 años antes), ni es cierto que inicie con redes bayesianas, ni es cierto que la arquitecura bayesiana sea la que hoy se utiliza para los modelos *state of the art*.

$2$. Alrededor del minuto 46:20 se dice que los ordenadores están condenados a «pensar lógica y axiomáticamente» según ciertas reglas. Me gustaría pedir a Carlos que precise en qué sentido y de qué forma el output de una arquitectura *transformer* es axiomático.

Al poco tiempo, además de aludir a un «pensamiento semiformal» —no sé si alguien sea capaz de explicar con rigor ese término—, agregó algo así como que los humanos se diferencian de las máquinas porque también piensan con «símbolos geométricos y topológicos». ¿Podría explicar exactamente a qué se refiere con eso y por qué eso se diferencia de «pensar lógica o algebraicamente», o por qué eso es inteligente, o por qué eso nos diferencia de las máquinas? ¿Podría además justificar por qué —según él— una máquina no es capaz de representaciones geométricas pero sí algebraicas?  Y ya que estamos, ¿de qué naturaleza son entonces las representaciones multidimensionales que aprende un *transformer* durante el entrenamiento, y en qué sentido no son geométricas las transformaciones de vectores y matrices que llevan a cabo? ¿Qué partes de la geometría son imposibles de formular en términos algebraicos y, de entre ellas, cuáles restringen el desarrollo de la inteligencia?

$3$. En cuanto al argumento de la habitación china (~47:25), aunque tenga su fulcro de verdad, es una caricatura de los modelos. Los modelos modernos no tienen «reglas» a seguir a rajatabla, ni tampoco tienen un «manual» fuera del que no funcionen. Me parece que Carlos está pensando en algoritmos estándar y no en las arquitecturas que se han desarrollado desde hace décadas. Precisamente algo interesante de estos modelos es su naturaleza no determinista. Tienen propiedades «emergentes», no funcionan con base en operaciones previamente definidas, y son capaces de ofrecer respuestas coherentes y verdaderas ante situaciones o problemas completamente nuevos (nunca antes «vistos» por el modelo).

En cualquier caso, en el ejemplo de la habitación, ¿puede Carlos —o cualquier otro humano viviente o pasado— explicar exactamente qué procesos mentales y operatorios tendría que llevar a cabo la persona dentro de la habitación para que podamos admitir que entiende, comprende y razona? ¿Puede darnos la lista exacta de requisitos necesarios para que se pueda decir que un determinado ente razona o no razona? ¿Cuántos Nobel calcula que necesitaría para poder responder a estas preguntas?

Para ser más claro: no sabemos definir qué procesos psicológicos y neurológicos tienen lugar cuando «entendemos», entonces aludir a esos procesos es impropio, sobre todo para argumentar que las máquinas no los satisfacen. No tenemos un estándar riguroso para definir si los satisfacen o no, y no lo tendremos hasta que no entendamos mejor cómo funciona la inteligencia humana. Mientras tanto, los criterios de distinción son sumamente cuestionables, especialmente cuando nos adentramos en los detalles (como fue el caso de la ponencia).

$4$. Minuto ~51:00. Dice Carlos, como en forma de crítica, que si le quitas los datos a un modelo deja de funcionar. Me imagino que Carlos dice esto porque piensa que el humano es capaz de pensar *ex nihilo*.

Poco tiempo después, dice que ChatGPT (y el traductor de Google) devuelven frases frecuentes porque hacen «combinaciones lineales entre vectores». Los modelos son especialmente útiles, entre otras cosas, porque utilizan funciones no lineales, entonces no sé a qué se refiere con esa frase. Los modelos también son especialmente únicos porque no devuelven «frases frecuentes», sino combinaciones muchas veces novedosas que son posibles por los componentes estocásticos que los subyacen (de nuevo: por eso no nos quedamos en Bayes).

$5$. Minuto ~53:30. «A DALLE le tuvieron que decir que cada imagen es un gato». O bien Carlos piensa que a los humanos nunca nos tuvieron que señalar qué es un gato y lo sabemos a priori, o bien Carlos espera que el lector tenga la inmensa generosidad de traducir esto por «DALLE no tiene la misma capacidad de generalización de conocimiento que un humano».

$6$. Minuto ~55:30. La caracterización de cómo «interpreta» un modelo a una imagen es bastante extraña y confusa. Dice que el «programa» solo «ve píxeles, no ve orejas, ni ojos». ¿Qué significa exactamente «ver orejas»? ¿Podría explicar Carlos qué procesos lleva a cabo el humano para «ver orejas», desde los fotorreceptores en la retina, pasando por las señales eléctricas que atraviesan el nervio óptico, hasta el procesamiento que lleva a cabo la corteza visual primaria? ¿Piensa que el cerebro no procesa la información que percibe y, por ello, «ve» a la oreja «en crudo» y la representa tal cual llegó a los ojos? En particular, ¿qué piensa de la investigación que se ha llevado a cabo desde 1959, cuando Hubel y Wiesel descubrieron que distintos grupos de neuronas en los gatos responden a formas, sombras e inclinaciones distintas, ergo, el procesamiento visual biológico no se da holísticamente, «por orejas y ojos», sino por estructuras, patrones y características más simples? Vamos, que —guardando las distancias— tanto la visión humana como la del *deep learning*, además de requerir un procesamiento previo de la información, se da por «features», de manera que el argumento de «ver orejas» me parece extraño, flojo y confuso. Y también me cuesta entender por qué cree que no se pueden diferenciar objetos en deep learning, cuando precisamente esa es una de las aplicaciones que tiene.

Se queja también de que el «programa extrae patrones estadísticos de bits». Según él, ¿cómo entonces tendrían que aprender los modelos a representar objetos para «poder ver» como humanos? Si desde 1959 no se ha demostrado que los animales también ven por patrones, ¿entonces cómo aprenden y cómo ven? ¿Por qué es censurable que la percepción se guíe por patrones? Pero más importante, ¿es siquiera deseable que las máquinas sean exactamente iguales a nosotros? ¿Por qué es tan esencial para la inteligencia que procesen visualmente los fenómenos como nosotros?

$7$. Si la inteligencia es corpórea, apotética y operatoria, ¿por qué una simulación o un robot, que perfectamente pueden satisfacer todo eso, no son inteligentes?

$8$. Minuto ~1:01:00. Se queja Carlos de que el traductor de Google necesita cierto contexto para traducir «jack» en lugar de «cat». Me imagino que él puede en este preciso momento traducir «banco» al inglés, sin necesidad de ningún contexto adicional. ¿Podría decirme su respuesta? Y, si es tan amable, explicar también detalladamente por qué los humanos no necesitan contextualizar las palabras para poder traducirlas a otro lenguaje, o siquiera para poder comunicarse.

$9$. Minuto ~1:04:42. «Conforme el programa aprende más en una tendencia, se vuelve reacio a aprender en otra dirección». ¿A qué se refiere exactamente con eso? ¿Al «overfitting»? ¿A que el valor de cada «weight» del modelo es fijo después del entrenamiento? Si es cualquiera de las dos cosas, entonces hay técnicas bastante elementales para resolverlo. Además, ¿en qué sentido hace esta crítica? ¿Los humanos no están también claramente sesgados en función de su experiencia y conocimiento?

Poco tiempo después plantea, como en una suerte de crítica, que las máquinas tienen el problema de inducción y no lidian bien con los cisnes negros. ¿Los humanos ya tienen eso resuelto y por eso son inteligentes?

$10$. Minuto ~1:16:41. ¿«DALLE no es inteligente porque usa píxeles en vez de pinceles»? ¿Ese es el «sustrato material» que impide que se le llame inteligente a una máquina? ¿Qué pasa si se integra a DALLE con un robot capaz de pintar con pincel sus outputs? ¿Dónde queda la objeción «corpórea»? ¿[Estos robots](https://youtu.be/-e1_QhJ1EhQ) que manipulan y lanzan objetos, y pisan el mismo suelo que nosotros, por qué no son inteligentes según ese mismo criterio?

$11$. De nuevo, me parece extraño que se reduzcan los modelos al teorema de Bayes. Los componentes probabilísticos más comúnmente utilizados (los procesos de Markov, la entropía cruzada, la función *softmax*, la «negative log likelihood», etc.) son claramente distintos del teorema y de las redes de Bayes. Además, todos ellos están conjugados con muchos otros elementos del cálculo diferencial y vectorial, del álgebra lineal, de la optimización matemática, etc., de manera que venderlos como simple probabilidad es bastante engañoso. Un modelo estrictamente probabilístico no es capaz de alcanzar el mismo nivel de sofisticación de ChatGPT.

En suma, Carlos no definió qué entiende por inteligencia (¿y quién puede decir que es capaz de hacerlo?), pero sí negó por razones ambiguas, imprecisas, erróneas, contradictorias y oscuras que las máquinas no son inteligentes. Percibí el afán de decir que la «inteligencia artificial» no es inteligente ni artificial, que es lo mismo que enfadarse porque el «cálculo infinitesimal» no trata de piedritas (calculus) infinitamente pequeñas, o decir que la filosofía es una chapuza porque no se dedica a predicar el amor (philos) a la sabiduría (sophia). No veo a dónde se quiere llegar con ese nivel de análisis terminológico superficial. Y, al final de todo, no se dijo nada sobre los temas verdaderamente interesantes que surgen con estos modelos (como las propiedades emergentes y la originalidad de la que hablaba, o las similitudes y diferencias entre los procesos biológicos y los computacionales, o la pregunta sobre cómo integramos en ellos la comprensión si ni siquiera nosotros sabemos cómo funciona o qué es). Carlos ha dedicado dos horas a tropezarse con razonamientos defectuosos para finalmente decir lo mismo que podría haber dicho cualquiera que haya usado ChatGPT (y entendido a medias cualquier artículo de divulgación sobre el tema): que los modelos no tienen intencionalidad ni razonamiento como los humanos y que, bueno, pues eso, funcionan con esas cosas de Bayes y de probabilidad, algo así, tú me entiendes. No creo que haga falta tropezarse con una historia imprecisa, ni con una caracterización errónea, ni con razonamientos en falso, ni con citas de académicos de conventillo, ni con ciencias todavía inconclusas, ni con alusiones a un término tan ambiguo como «inteligencia», para decir que las máquinas no razonan como nosotros. Al final, para usar su propio giro, no hace falta resolver lo oscuro con lo más oscuro.

*PD: Estoy bastante seguro de que Carlos Madrid vio mis comentarios en más de una ocasión, pero nunca respondió ni reaccionó en lo absoluto. Que el lector juzgue por qué.*

<p align="right"> <i>
14 de marzo, 2023
</i>
</p>

---
**Bibliografía musical**

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/1Sc2RHxyKHMeyCChQTWTnp?utm_source=generator&theme=0" width="100%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

<iframe width="100%" height="166" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/1358901880&color=%2304a9eb&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true"></iframe><div style="font-size: 10px; color: #cccccc;line-break: anywhere;word-break: normal;overflow: hidden;white-space: nowrap;text-overflow: ellipsis; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif;font-weight: 100;"><a href="https://soundcloud.com/robert-wilborn" title="Jamie XX ID&#x27;s" target="_blank" style="color: #cccccc; text-decoration: none;">Jamie XX ID&#x27;s</a> · <a href="https://soundcloud.com/robert-wilborn/jamie-xx-pale-blue-dancer-id" title="Jamie XX - Pale Blue Dancer (ID)" target="_blank" style="color: #cccccc; text-decoration: none;">Jamie XX - Pale Blue Dancer (ID)</a></div>

[^1]: Establecían una penosa dicotomía entre trabajos «manuales» y «creativos» para determinar si podían ser automatizados o no. Con base en eso, aconsejaban unos planes de estudio ridículos en los que enseñarían a la gente a ser creativa e imaginativa para diferenciarse de las máquinas. Además, dado que ser chófer o manejar un taxi les parecía facilísimo, decían que esa era una labor manual que se automatizaría prontamente. Al final, resulta que manejar es infinitamente más complejo que pintar arte y escribir poesía.